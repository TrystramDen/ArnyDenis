%version of 03-27-19

\chapter{SETS, BOOLEAN ALGEBRAS, AND LOGICS}
\label{ch:sets-BA-logic}

\section{Sets}
\label{sec:sets}

This chapter studies three of the most basic concepts that underlie
mathematics.

{\it Sets} \index{set} are probably the most basic object of
mathematical discourse---sets exist to have {\it elements},
\index{set!element} or {\it members}, \index{set!member} the entities
that {\em belong to} \index{set!the belong-to relation} the set.
Despite the conceptual simplicity of the notion {\it set}, that notion
is surprisingly difficult to specify formally; philosophers have been
debating the nature of the notion for millennia.  Yet, the intuitive
grasp of the concept that {\em everyone} develops just in the course
of living is surprisingly adequate for almost all intellectual
endeavors---so we join the ranks of the multitude of instructors who
say that the concept set is so woven into our lives as humans that,
for all relevant purposes, we all know, and understand, what sets are.
As soon as we begin to build upon the notion {\it set} in order to
develop the rudiments of science and/or mathematics, we must impose
some structure upon the sets of interest and assemble a repertoire of
operations to manipulate them.  At this point, we can no longer
proceed based just on life experience: we must proceed based on
understanding, not just instinct.  This is the point at which our
joint journey into the wondrous world of mathematics begins.

Once we begin to devise a repertoire of operations on sets, we begin
to observe the emergence of patterns, which soon become the ``laws''
that govern how operations interact with each other
\begin{itemize}
\item
as one example: Does the result of applying both operations
$\circ_1$ and $\circ_2$ depend on the {\em order} of their
application?
\end{itemize}
and how sets change based on the operations they have undergone.
\begin{itemize}
\item
as one example: What is the relation between the the following sets?
\[ \big(S_1 \circ (S_2 \circ S_3)\big) \ \
\mbox{ and } \ \ \big((S_1 \circ S_2) \circ S_3\big)
\]
\end{itemize}
**HERE
  .  and two of its
closest kindred


\subsection{Fundamental Set-Related Concepts}
\label{sec:set-concepts}

{\em The reader knows what a set is and recognizes that some sets are
  finite, while others are infinite}.  Speaking informally---a formal
treatment will follow in later chapters---here are a few illustrative
finite sets:
\begin{itemize}
\item
the set of words in this book

I do not know how big this set is, but I imagine that you as a reader
have a better idea than I as an author.
\item
the set of characters in any JAVA program

Note that while we are sure that this set is finite, we are not so
confident about the number of seconds the program will run!
\item
the set consisting of {\em you}

Paraphrasing the iconic television figure Mister Rogers, ``You are
unique.''  This set has just one element.

\item
the set of unicorns in New York City

I will not argue with you about this, but I believe that this is the
{\em empty set} $\emptyset$, which has zero members. 
\end{itemize}
Some familiar infinite sets are:
\begin{itemize}
\item
the set of {\em nonnegative integers}
\item
the set of {\em positive integers}
\item
the set of {\em all integers}
\item
the set of nonnegative {\em rational numbers}---which are quotients of
integers
\item
the set of nonnegative {\em real numbers}---which can be viewed
computationally as the set of numbers that admit infinite decimal
expansions,
\item
the set of nonnegative {\em complex numbers}---which can be viewed as
ordered pairs of real numbers,
\item
the set of {\em all} finite-length binary strings.

A {\it binary string}\index{Binary string} is a sequence of $0$s and
$1$s.  When discussing computer-related matters, one often calls each
$0$ and $1$ that occurs in a binary string a {\it bit}\index{Bit:
  binary digit} (for {\it binary digit}).  The term ``bit'' leads to
the term {\it bit string} as a synonym of {\it binary string}.
\end{itemize}
Despite this assumption, we begin the chapter by reviewing some basic
concepts concerning sets and operations thereon.

As noted early, sets were created to contain members/elements.  We
denote the fact that element $t$ {\it belongs to},\index{set!the
  belongs to relation} or, {\it is an element of} set $T$ by the
notation $t \in T$\index{set!membership: $\in$}.  A {\em
  subset}\index{set!subset} of a set $T$ is a set $S$ each of whose
members belongs to $T$.  The subset relation occurs in two forms, The
{\em strong} form of the relation, denoted $S \subset
T$,\index{set!strong subset relation} says that every element of $S$
is an element of $T$, but {\em not} conversely; i.e., $T$ contains
(one or more) elements that $S$ does not.  The {\em weak} form of the
relation, denoted $S \subseteq T$,\index{set!weak subset relation}
is defined as follows:
\[
[S \subseteq T] \ \ \mbox{ means: } \ \
\Big[ \mbox{\em either } \ \ [S = T]
\ \ \mbox{\em or } \ \ [S \subset T] \Big].
\]

For any finite set $S$, we denote by $|S|$ the {\it cardinality}
\index{set!cardinality}\index{cardinality!finite set} of $S$, which is
the number of elements in $S$.  Finite sets having three special
cardinalities are singled out with special names.  The limiting case
of finite sets is the unique {\em empty set}, which we denote by
$\emptyset$; thus, $\emptyset$ is characterized by the equation
$|\emptyset| = 0$.  (The empty set is often a limiting case of
set-defined entities.)  If $|S| = 1$, then we call $S$ a {\em
  singleton};\index{set!singleton set} and if $|S| = 2$, then we call
$S$ a {\em doubleton}.\index{set!doubleton}

It is often useful to have a convenient term and notation for {\em the
  set of all subsets of a set $S$}.  This bigger set---it contains
$2^{|S|}$ elements when $S$ is finite---is denoted by $\p(S)$ and is
called the {\em power set}\index{power set:set of all subsets} of
$S$.\footnote{The name ``power set'' arises from the relative
  cardinalities of $S$ and ${\cal P}(S)$ for finite $S$.}  Note
carefully the two set-relations that we are talking about here:

{\em A set $T$ that is a {\em subset} of set $S$ is an {\em element}
  of the set $\p(S)$.}

\noindent
You should satisfy yourself that the biggest and smallest elements of
${\cal P}(S)$ are, respectively, the set $S$ itself and the empty set
$\emptyset$.

\subsection{Operations on Sets}
\label{sec:set-operations}

Given two sets $S$ and $T$, we denote by:
\begin{itemize}
\item
$S \cap T$\index{$S \cap T$: set intersection} the {\it
  intersection}\index{set!operations!intersection}
of $S$ and $T$: the set of elements that belong to {\em both} $S$ and
$T$.
\[ [s \in S \cap T] \ \ \mbox{ means } \ \ 
\Big[ [s \in S] \ \mbox{\bf and } \ [s \in T] \Big]
\]

\item
$S \cup T$\index{$S \cup T$: set union} the {\it
  union}\index{set!operations!union}
of $S$ and $T$: the set of elements that belong to $S$, or to $T$, {\em
  or to both}.  (Because of the ``or both'' qualifier, this operation
is sometimes called {\em inclusive
  union}.)\index{set!operations!inclusive union}
\[ [s \in S \cup T] \ \ \mbox{ means } \ \
\Big[ [s \in S] \ \mbox{\bf or } \ [s \in T]  \ \mbox{\bf or } \ [s
    \in S \cap T] \Big]
\]


\item
$S \setminus T$\index{$S \setminus T$: set difference} is the {\em
  (set) difference}\index{set!operations!set difference} of $S$ and
  $T$: the set of elements that belong to $S$ but not to $T$.
\[ [s \in S \setminus T] \ \ \mbox{ means } \ \
\Big[ [s \in S] \ \mbox{\bf and } \ [s \not\in T] \Big]
\]
(Particularly in the United States, one often encounters the notation
 ``$S-T$''\index{$S - T$: set difference} instead of ``$S \setminus
 T$.'')
\end{itemize}

We illustrate the preceding operations with the sets $S = \{a,b,c\}$
and $T = \{c,d\}$.  For these sets:
\begin{eqnarray*}
S \cap T & = &  \{c\}, \\
S \cup T & = & \{a,b,c,d\}, \\
S \setminus T & = & \{a,b\}.
\end{eqnarray*}

In many set-related situations, the sets of interest will be subsets
of some fixed ``universal'' set $U$.\index{set!universal set}
\begin{quote}
We use the term ``universal'' as in ``universe of discourse,'' not in
the self-referencing sense of a set that contains all other sets as
members, a construct (discussed by philosopher-logician Bertrand
Russell) \index{Russell, Bertrand} which leads to mind-bending paradoxes.
\end{quote}
Given a universal set $U$ and a {\em subset} $S \subseteq U$,
% (the notation meaning that every element of $S$---if there are any---is
%also an element of $U$)
we observe the set-inequalities
\[ \emptyset \ \subseteq \ S \ \subseteq \ U. \]
When studying a context within which there exists a universal set $U$
that contains all other sets of interest, we include within our
repertoire of set-related operations also the operation of {\it
  complementation}\index{set!operations!complementation}
\begin{itemize}
\item
$\overline{S} \ \eqdef \ U \setminus S$,\index{$\overline{S}$: the
  complement of set $S$ relative to a universal set}
the {\em complement} of $S$ (relative to the universal set $U$).

For instance, the set of odd positive integers is the complement of
the set of even positive integers, relative to the set of all positive
integers.
\end{itemize}
We note a number of basic identities involving sets and operations on
them.
\begin{itemize}
\item
$S \setminus T \ = \ S \cap \overline{T}$,
\item
If $S \subseteq T$, then
  \begin{enumerate}
  \item
$S \setminus T \ = \ \emptyset$,
  \item
$S \cap T \ = \ S$,
  \item
$S \cup T \ = \ T$.
  \end{enumerate}
\end{itemize}
Note, in particular, that\footnote{``iff'' abbreviates the common
mathematical phrase, ``if and only if.''}
\[ [S = T] \ \mbox{  iff  } \ \ \Bigl[[S \subseteq T] \mbox{
    {\small\sf and} } [T \subseteq S]\Bigr] \ \mbox{  iff  }
\ \ \Bigl[ (S \setminus T) \cup (T \setminus S) = \emptyset\Bigr].
\]

The operations union, intersection, and complementation---and
operations formed from them, such as set difference---are usually
called the {\em Boolean (set) operations},
\index{Boolean set operations}\index{set!operations!Boolean set operations}
named for the $19$th-century English mathematician George
Boole. \index{Boole, George} There are several important identities
involving the Boolean set operations.  Among the most frequently
invoked are the two ``laws'' attributed to the $19$th-century French
mathematician Auguste De Morgan:
\index{set!operations!De Morgan's Laws}\index{De Morgan's Laws}
\begin{equation}
\label{e.de-morgan}
\mbox{For all sets $S$ and $T$: } \ \left\{
\begin{array}{lcl}
\overline{S \cup T} & = & \overline{S} \cap \overline{T}, \\
 \\
\overline{S \cap T} & = & \overline{S} \cup \overline{T}.
\end{array}
\right.
\end{equation}

\noindent {\em (Algebraic) Closure}.\index{(Algebraic) Closure}
%
We end this section with a set-theoretic definition that occurs in
many contexts.  Let $\cal C$ be any (finite or infinite) collection of
sets, and let $S$ and $T$ be two elements of $\cal C$.  (Note that
$\cal C$ is a set whose elements are sets.)  Think, e.g., of the
concrete example of set intersection.

We say that $\cal C$ is {\em closed} under intersection if whenever
sets $S$ and $T$ (which could be the same set) both belong to $\cal
C$, the set $S \cap T$ also belongs to $\cal C$.  By De Morgan's laws,
$\cal C$'s closure under union implies also its closure under
intersection.

\section{Binary Relations}
\label{s.relation}

\subsection{The Formal Notion of Binary Relation}
\label{s.relation-basic}

We begin our discussion of relations by adding a new (binary) set
operation to our earlier repertoire.  Given (finite or infinite) sets
$S$ and $T$ we denote by $S \times T$\index{$S \times T$} the {\it
  direct product} of $S$ and $T$,\index{direct product of sets} which
is the set of all {\it ordered pairs}\index{ordered pair of set
  elements} whose first coordinate contains an element of $S$ and
whose second coordinate contains an element of $T$.  For example, if
$S = \{a,b,c\}$ and $T = \{c,d\}$, then
\[ S \times T \ =  \{
\langle a,c \rangle,
\langle b,c \rangle,
\langle c,c \rangle,
\langle a,d \rangle,
\langle b,d \rangle,
\langle c,d \rangle\}
\]
The direct-product operation on sets affords us a simple, yet
powerful, formal notion of binary relation.

Given (finite or infinite) sets $S$ and $T$, a {\it relation $\rho$ on
  $S$ and $T$}\index{relation on sets} (in that order) is any subset
\[ \rho \ \subseteq \ S \times T. \]
When $S = T$, we often call $\rho$ a {\em binary relation on (the set)
  $S$}\index{binary relation on a set} (``{\em binary}'' because there
are {\em two} copies of set $S$ being related by $\rho$).

Relations are so common that we use them in every aspect of our lives
without even noticing them.  The relations ``equal to,'' ``less than,''
and ``greater than or equal to'' are simple examples of binary
relations on the integers.  These same three relations apply also to
other familiar number systems such as the rational and real numbers;
only ``equal,'' though, holds (in the natural way) for the complex
numbers.  Some subset of the three relations ``is a parent of,'' ``is
a child of,'' and ``is a sibling of'' probably are binary relations on
(the set of people constituting) your family.  To mention just one
relation with distinct sets $S$ and $T$, the relation ``$A$ is taking
course $X$'' is a relation on
\[ \left( \mbox{the set of all students} \right) \times
   \left( \mbox{the set of all courses} \right).
\]

\ignore{************
We shall see later (Section~\ref{s.pairing}) that there is a formal
sense in which binary relations are all we ever need consider: $3$-set
({\em ternary}) 
relations\index{ternary relation}\index{relation!ternary}---which are
subsets of $S_1 \times S_2 \times S_3$---and $4$-set ({\em
  quaternary}) 
relations\index{quaternary relation}\index{relation!quaternary}---which
are subsets of $S_1 \times S_2 \times S_3 \times S_4$---and so on (for
any finite ``arity''), can all be expressed as binary relations of
binary relations \ldots of binary relations.  As examples: For ternary
relations, we can replace any subset $R$ of $S_1 \times S_2 \times
S_3$ by the obvious corresponding subset $R'$ of $S_1 \times (S_2
\times S_3)$: for each element $\langle s_1, s_2, s_3 \rangle$ of $R$,
the corresponding element of $R'$ is $\langle s_1, \langle s_2, s_3
\rangle \rangle$.  Similarly, for quaternary relations, we can replace
any subset $R''$ of $S_1 \times S_2 \times S_3 \times S_4$ by the
obvious corresponding subset $R'''$ of $S_1 \times (S_2 \times (S_3
\times S_4))$: for each element $\langle s_1, s_2, s_3, s_4 \rangle$
of $R''$, the corresponding element of $R'''$ is $\langle s_1, \langle
s_2, \langle s_3, s_4 \rangle \rangle \rangle$.
\begin{quote}
You should convince yourself that we could achieve the desired
correspondence also by replacing $S_1 \times (S_2 \times S_3)$ with
$(S_1 \times S_2) \times S_3$ and by replacing $S_1 \times S_2 \times
S_3 \times S_4$ by either $((S_1 \times S_2) \times S_3) \times S_4$
or $(S_1 \times S_2) \times (S_3 \times S_4)$.
\end{quote}
************}

By convention, when dealing with a binary relation $\rho \ \subseteq
\ S \times T$, we often write ``$s \rho t$''\index{infix notation for
  a binary relation: $s \rho t$} in place of the more stilted notation
``$\langle s, t \rangle \in \rho$.''  For instance we (almost always)
write ``$5 < 7$'' in place of the strange-looking (but formally
correct) ``$\langle 5,7 \rangle \in \ <$.''

The following operation on relations occurs in many guises, in almost
all mathematical theories.  Let $\rho$ and $\rho'$ be binary relations
on a set $S$.  The {\it composition}
\index{composition! binary relations}
\index{composition of binary relations}
of $\rho$ and $\rho'$ (in that order) is the relation
\[ 
\rho'' \ \eqdef \ \Bigl\{ \langle s, t \rangle \in S \times S \ | \
(\exists t \in S) \Bigl[ [s \rho t] \mbox{ and } [t \rho' u] \Bigr] \Bigr\}.
\]
Note that we have used both of our notational conventions for
relations here.  We also encounter here, for the first time in the
text, but certainly not the last, a new notational convention: the
common ``shorthand'' compound symbol ``$\eqdef$'':
\index{$\eqdef$: ``equals, by definition''}
The sentence ``$X \eqdef Y$'' should be read ``$X$ {\em is (or, equals),
  by definition}, $Y$.''
\begin{quote}
The operation of composition of relations is quite important in the
study of ``relational'' databases.
\end{quote}

\noindent
It is important to be able to assert that elements $s, t \in S$ are
{\em not} $\rho$-related,\index{relation negation} i.e., $\langle s, t
\rangle \not\in S \times S$.  Several notations have been developed
for this purpose.\index{$\widetilde{\rho}$: the negation of relation $\rho$}
\begin{equation}
\label{eq:NOT-rho-notation}
\begin{array}{|c|c|c|c|}
\hline
\mbox{\bf Relation} & \mbox{\bf Notation} & \mbox{\bf Negation} &
\mbox{\bf Standard?} \\
\hline
\hline
\mbox{set membership} & \in & \not\in & \mbox{yes} \\
\hline
\mbox{equality}       & =   & \neq    & \mbox{yes} \\
\hline
\mbox{less than (strong)} & < & \not < \mbox{ or } \geq & \mbox{yes} \\
\hline
\mbox{less than (weak)} & \leq & \not\leq \mbox{ or } > & \mbox{yes} \\
\hline
\mbox{greater than (strong)} & > & \not > \mbox{ or } \leq & \mbox{yes} \\
\hline
\mbox{greater than (weak)} & \geq & \not\geq \mbox{ or } < & \mbox{yes} \\
\hline
\mbox{generic}  & \rho  & \sim\rho \mbox{ or } \widetilde{\rho} &
\mbox{no} \\
\hline
\end{array}
\end{equation}

There are several special classes of binary relations that are so
important that we must single them out immediately, in the following
subsections.


\subsection{Order Relations}
\label{sec:order-relation}

A binary relation $\rho$ on a set $S$ is a {\it partial order
  relation},\index{order relation}\index{order relation!
  partial}\index{partial order}\index{order} or, more briefly, is a
{\it partial order} if $\rho$ is transitive.\index{transitive
  relation} This means that, for all elements $s, t, u \in S$,
\begin{equation}
\label{eq:def-transitive}
\mbox{if } \ \ sRt \ \ \ \mbox{ and } \ \ tRu \ \ \ \mbox{ then }
\  \ sRu.
\end{equation}
The qualifier ``partial'' warns us that some pairs of elements of $S$
do not occur in relation $\rho$.  Number-related orders supply an easy
illustrative example.  Given any two distinct integers, $m$ and $n$,
one of them must be less than the other: either $m < n$, or $n < m$.
In contrast, if we consider {\it ordered pairs} of integers, then
there are pairs of pairs that are not related by the ``less than''
relation in any natural way.  For instance, even though we may agree
that, by a natural extension of the number-ordering relation ``less
than'', $\langle 4, 17 \rangle$ is ``less than'' $\langle 22, 19
\rangle$, we might well not agree on which of $\langle 4, 22 \rangle$
and $\langle 19, 17 \rangle$ is less than the other---or, indeed,
whether either is ``less than'' the other.

In many domains, order relations occur in two ``flavors'', {\em
  strong} and {\em weak}.\index{order!strong}\index{order!weak} For
many such relations $\rho$---consider, e.g., ``less than'' on the
integers---the weak version is denoted by underscoring the strong
one's symbol.  This will be our convention.  Just as $\leq$ denotes
the weak version of $<$, and $\geq$ denotes the weak version of $>$,
we shall denote the weak version of a generic order $\rho$ by
$\underline{\rho}$.\index{$\underline{\rho}$:the weak version of order
  relation $\rho$}.  Strong and weak versions of an order relation
$\rho$ (denoted, respectively, $\rho$ and $\underline{\rho}$) are
distinguished by their behavior under simultaneous membership.  For
illustration, instantiate the following template with $\rho$ being
``$<$'' and with $\rho$ being ``$>$'':

\smallskip

\begin{tabular}{lll}
For a strong order $\rho$: & &
{\bf if} $[s \ \rho \ t]$, {\bf then} $[t \ \widetilde{\rho} \ s]$ \\
For the weak version $\underline{\rho}$ of $\rho$: & &
{\bf if} $[s \ \underline{\rho} \ t]$ {\bf and} $[t \ \underline{\rho}
  \ s]$, {\bf then} $[s = t]$.
\end{tabular}

\subsection{Equivalence Relations}
\label{sec:equiv-relation}

A binary relation $R$ on a set $S$ is an {\it equivalence
  relation}\index{equivalence relation} if it enjoys the following
three properties:
\begin{enumerate}
\item
$R$ is {\em reflexive:} for all $s \in S$, we have $sRs$.
\item
$R$ is {\em symmetric:} for all $s, s' \in S$, we have $sRs'$ whenever
  $s'Rs$.
\item
$R$ is {\em transitive:} for all $s, s', s'' \in S$, whenever we have
  $sRs'$ and $s'Rs''$, we also have $sRs''$.
\end{enumerate}
Sample familiar  equivalence relations are:
\begin{itemize}
\item
The equality relation, $=$, on a set $S$ which relates each $s \in S$
with itself but with no other element of $S$.
\item
The relations $\equiv_{12}$ and $\equiv_{24}$ on integers,
where\footnote{As usual, $|x|$ is the {\em absolute value}, or, {\em
    magnitude} of the number $x$.  That is, if $x \geq 0$, then $|x| =
  x$; if $x < 0$, then $|x| = -x$.}
  \begin{enumerate}
  \item
$n_1 \equiv_{12} n_2$ if and only if $|n_1 - n_2|$ is divisible by
$12$.
  \item
$n_1 \equiv_{24} n_2$ if and only if $|n_1 - n_2|$ is divisible by
$24$.
  \end{enumerate}
We use relation $\equiv_{12}$ (without formally knowing it) whenever
we tell time using a $12$-hour clock and relation $\equiv_{24}$
whenever we tell time using a $24$-hour clock.
\end{itemize}

Closely related to the notion of an equivalence relation on a set $S$
is the notion of a {\it partition} of $S$.  A partition of $S$ is a
nonempty collection of subsets $S_1, S_2, \ldots$ of $S$ that are
\begin{enumerate}
\item
{\em mutually exclusive:}
for distinct indices $i$ and $j$, $S_i \cap S_j = \emptyset$;
\item
{\em collectively exhaustive:}
$S_1 \cup S_2 \cup \cdots = S$.
\end{enumerate}
We call each set $S_i$ a {\it block} of the partition.

One verifies the following Proposition easily.\index{partitions and
  equivalence relations} 

\begin{prop}
A partition of a set $S$ and an equivalence relation on $S$ are just
two ways of looking at the same concept.
\end{prop}

To verify this, we note the following.

\noindent {\small\sf Getting an equivalence relation from a partition}.
%
Given any partition $S_1, S_2, \ldots$ of a set $S$, define the
following relation $R$ on $S$:

\noindent
$sRs'$ if and only if $s$ and $s'$ belong to the same block of the
partition.

\noindent
{\em Relation $R$ is an equivalence relation on $S$.}
To wit, $R$ is reflexive, symmetric, and transitive because collective
exhaustiveness ensures that each $s \in S$ belongs to some block of
the partition, while mutual exclusivity ensures that it belongs to
only one block.

\noindent {\small\sf Getting a partition from an equivalence relation}.
%
To obtain the converse, focus on any equivalence relation $R$ on a set
$S$.  For each $s \in S$, denote by $[s]_R$ the set
\[ [s]_R \ \eqdef \ \{ s' \in S \ | \ sRs' \}; \]
we call $[s]_R$ {\it the equivalence class of $s$ under relation
$R$}.\index{equivalence class}\index{equivalence relation!class}

\noindent
{\em The equivalence classes under $R$ form a partition of $S$}.
To wit: $R$'s reflexivity ensures that the equivalence classes
collectively exhaust $S$; $R$'s symmetry and transitivity ensure that
equivalence classes are mutually disjoint.

The {\it index}\index{index (of an equivalence relation)} of the
equivalence relation $R$ is its number of classes---which can be
finite or infinite.

Let\footnote{Conforming to common usage, we typically use the symbol
  $\equiv$, possibly embellished by a subscript or superscript, to
  denote an equivalence relation.}~$\equiv_1$ and $\equiv_2$ be two
equivalence relations on a set $S$.  We say that the relation
$\equiv_1$ {\em is a refinement of} (or, {\em
  refines})\index{refinement of an equivalence relation} the relation
$\equiv_2$ just when each block of $\equiv_1$ is a subset of some
block of $\equiv_2$.  We leave to the reader the simple verification
of the following basic result.

\begin{theorem}
\label{thm:equality=finest-equiv}
The equality relation, $=$, on a set $S$ refines every equivalence
relation on $S$.  In this sense, it is the finest equivalence relation
on $S$.
\end{theorem}

\subsection{Functions}
\label{sec:function}

One learns early in school that a function from a set $A$ to a set $B$
is a rule that assigns a unique value from $B$ to every value from
$A$.  Simple examples illustrate that this notion of function is more
restrictive than necessary.  Think, e.g., of the operation {\em
  division} on integers.  We learn that division, like multiplication,
is a function that assigns a number to a given pair of numbers.  Yet
we are warned almost immediately not to ``divide by $0$'': The
quotient upon division by $0$ is ``undefined.''  So, division is not
quite a function as envisioned our initial definition of the notion.
Indeed, in contrast to an expression such as ``$4 \div 2$,'' which
should lead to the result $2$ in any programming
environment,\footnote{We are, of course, ignoring demons such as
  round-off error.}~expressions such as ``$4 \div 0$'' will lead to
wildly different results in different programming environments.  Since
``wildly different'' is anathema in any mathematical setting, we deal
with situations such as just described by broadening the definition of
``function'' in a way that behaves like our initial simple definition
under ``well-behaved'' circumstances and that extends the notion in an
intellectually consistent way under ``ill-behaved'' circumstances.
Let us begin to get formal.

A {\it (partial) function from set $S$ to set $T$} is a relation $F
\subseteq S \times T$ that is {\it single-valued;} i.e., for each $s
\in S$, there is {\em at most} one $t \in T$ such that $sFt$.  We
traditionally write ``$F: S \rightarrow T$'' as shorthand for the
assertion, ``$F$ is a function from the set $S$ to the set $T$''; we
also traditionally write ``$F(s) = t$'' for the more conservative
``$sFt$.''  (The single-valuedness of $F$ makes the nonconservative
notation safe.)  We often call the set $S$ the {\em source (set)},
\index{function!source set}
or, the {\it domain}
\index{function!domain}
%
of function $F$, and we call set $T$ the {\em target (set)}
\index{function!target set}
or, the {\it range}
\index{function!range}
%
of function $F$.  When there is always a (perforce, unique) $t \in T$
for each $s \in S$, then we call $F$ a {\em total} function.

\ignore{******* 
Note that our terminology is a bit unexpected: {\em Every total
  function is a partial function;} that is,
``partial''\index{function!partial} is the generic term, and ``total''
is a special case.
*********}

You may be surprised to encounter functions that are not total,
because most of the functions you deal with daily are {\em total}.
Our mathematical ancestors had to do some fancy footwork in order to
make your world so neat.  Their choreography took two complementary
forms.
\begin{enumerate}
\item
They expanded the target set $T$ on numerous occasions.  As just two
instances:
  \begin{itemize}
  \item
They appended both $0$ and the negative integers to the preexisting
positive integers\footnote{The great mathematician Leopold Kronecker
  said, ``God made the integers, all else is the work of man'';
  Kronecker was referring, of course, to the {\em positive}
  integers.}~in order to make subtraction a total function.

  \item
They appended the rationals to the preexisting integers in order to
make division (by nonzero numbers!)~a total function.
  \end{itemize}
The irrational algebraic numbers, the nonalgebraic real numbers, and
the nonreal complex numbers were similarly appended, in turn, to our
number system in order to make certain (more complicated) functions
total.

\item
They adapted the function.  In programming languages, in particular,
true undefinedness is anathema, so such languages typically have ways
of making functions total, via devices such as ``integer division''
(so that odd integers can be ``divided by $2$'') as well as various
ploys for accommodating ``division by $0$.''
\end{enumerate}
The ($20$th-century) inventors of {\em Computation Theory} insisted on
a theory of functions on nonnegative integers (or some transparent
encoding thereof).  The price for such ``pureness'' is that we must
allow functions to be undefined on some arguments.  Thus the theory
renders such functions as ``division by $2$'' and ``taking square
roots'' as being {\em nontotal}: both are defined only on subsets of
the positive integers (the even integers and the perfect squares,
respectively).

Three special classes of functions merit explicit mention.  For each,
we give both a down-to-earth name and a more scholarly Latinate one.

A function $F: S \rightarrow T$ is:
\begin{enumerate}
\item
{\it one-to-one} (or {\it injective}) if for each $t \in T$, there is
at most one $s \in S$ such that $F(s) = t$;

\medskip

{\em Example:}
\begin{itemize}
\item
 ``multiplication by $2$'' is injective:  If I give you an even
  integer $2n$, you can always respond by giving me $n$.
\item
``integer division by $2$'' is not injective---because performing the
  operation on arguments $2n$ and $2n+1$ yields the same answer
  (namely, $n$).
\end{itemize}

An injective function $F$ is called an {\it injection}.

\smallskip

Importantly, each injection $F$ has a {\it functional inverse},
\index{inverse of an injection}
%
which is commonly denoted $F^{-1}$.
\index{$F^{-1}$: functional inverse of injection $F$}
%
and which is defined as follows.  For each $t \in T$:
  \begin{itemize}
  \item
If there is an $s \in S$ such that $F(s) = t$, then
\[ F^{-1}(t) \ = \ s \]

  \item
If there is no $s \in S$ such that $F(s) = t$, then $F^{-1}(t)$ is not defined.
  \end{itemize}
Because $F$ is an {\em injection}, there is at most $s \in S$ such
that $F(s)= t$.  In other words, an element $t \in T$ can occur in the
range of $F$ only because of a single element $s \in S$ in the domain
of $F$.  This means that the preceding definition of $F^{-1}$ is a
valid definition (it is ``well-defined'') and that $F^{-1}$ is a
(partial) function $F^{-1}: T \rightarrow S$ whose domain is the range
of $F$.

\item
{\it onto} (or {\it surjective}) if for each $t \in T$, there is at
least one $s \in S$ such that $F(s) = t$;

\medskip

{\em Example:}
\begin{itemize}
\item
Two surjective functions on the nonnegative integers:
  \begin{itemize}
  \item
``subtraction of $1$'' is surjective, because ``addition of $1$'' is a
total function.
  \item
``taking the square root'' is surjective because the operation of
squaring is a total function.
  \end{itemize}
\item
Two functions on the nonnegative integers that are {\em not} surjective:
  \begin{itemize}
  \item
``addition of $1$'' is not surjective, because, e.g., $0$ is not ``$1$
greater'' than any nonnegative integer.
  \item
``squaring'' is not surjective, because, e.g., $2$ is not the
square of any integer.  (We prove this as
Proposition~\ref{thm:sqrt(2)}.) 
  \end{itemize}
\end{itemize}
A surjective function $F$ is called a {\it surjection}.

\item
{\it one-to-one, onto} (or {\it bijective}) if for each $t \in T$,
there is precisely one $s \in S$ such that $F(s) = t$.

\ignore{**********
{\em Example:} The (total) function $F: \{0,1\}^\star \rightarrow
\{0,1\}^\star$ defined by:
\[
(\forall w \in \{0,1\}^\star) \ F(w) \ = \
\mbox{(the reversal of $w$)}
\]
is a bijection.  The (total) function $F': \{0,1\}^\star \rightarrow
\N$ defined by
\[
(\forall w \in \{0,1\}^\star) \ F(w) \ = \
\mbox{(the integer that is represented by $w$ viewed as a numeral)}
\]
is {\em not} a bijection, due to the possibility of leading $0$'s.
\begin{quote}
A {\it numeral}\index{representation of integers!numeral}
is a sequence of digits that is the ``name'' of a number.  The
numerical value of a numeral $x$ depends on the {\it number 
base},\index{representation of integers!number base}
which is a positive integer $b >1$ that is used to create $x$.  Much
of our focus will be on {\em binary} numerals---which are binary
strings---for which the base is 
$b=2$.\index{representation of integers!base-$2$ representation}\index{representation of integers!binary representation}
For a general number base $b$, the integer denoted by the numeral
$\beta_n \beta_{n-1} \ldots \beta_1 \beta_0$, where each $\beta_i \in
\{0,1, \ldots, b-1\}$, is\index{representation of integers!base-$b$ representation}
\[ \sum_{i=0}^n \ \beta_i b^i. \]
We say that bit $\beta_i$ has {\em lower order}\index{representation of integers!low-order bit}
in the numeral than does $\beta_{i+1}$, because $\beta_i$ is
multiplied by $b^i$ in evaluating the numeral, whereas $\beta_{i+1}$
is multiplied by $b^{i+1}$.
\end{quote}
*********}

A bijective function $F$ is called a {\it bijection}.  When $F$ is a
bijection from $S$ onto $T$, we often write $F: S \leftrightarrow T$.
\end{enumerate}
There is a marvelous theorem that must be mentioned here, even though
it is beyond the scope of the book.\footnote{The theorem can be found
  in texts such as \cite{Birkhoff-MacLane53}.}
\index{The Schr\"{o}der-Bernstein Theorem}
%
The theorem says that, given sets $S$ and $T$: {\em if} there is an
injection $F_1$ that maps elements of $S$ one-to-one to elements of
$T$ {\em and} there is an injection $F_2$ that maps elements of $T$
one-to-one to elements of $S$, {\em then} there is a single bijection
$F$ such that
\begin{itemize}
\item
$F$ maps elements of $S$ one-to-one to elements of $T$;
\item
$F^{-1}$, the {\it functional inverse} of $F$,
\index{inverse of an injection}
maps elements of $T$ one-to-one to elements of $S$.
\end{itemize}

\begin{theorem}[The Schr\"{o}der-Bernstein Theorem]
For any sets $S$ and $T$, if there is an {\em injection} $F^{(S
  \rightarrow T)}: S \rightarrow T$ and an {\em injection} $F^{(T
  \rightarrow S)}: T \rightarrow S$, then there is a {\em bijection}
$F^{(S \leftrightarrow T)}: S \rightarrow T$.
\end{theorem}

\medskip

While the operation of {\it composition},
\index{composition!functions}
as introduced in Section~\ref{s.relation-basic}, is important for
general binary relations, it is a daily staple with relations that are
functions!  Let us be given two functions on a set $S$
\[
F: S \rightarrow S \ \ \ \ \mbox{ and } \ \ \ \ G: S \rightarrow S
\]
The composition of $F$ and $G$, {\em in that order}, is the function
\[ F \circ G: S \rightarrow S \]
defined as follows.
\begin{equation}
\label{eq:functions-composed}
\mbox{For each } \ s \in S \ \ \
F \circ G(s) \ = \ G(F(s)).
\end{equation}
The unexpected change in the orders of writing $F$ and $G$ on the two
sides of equation (\ref{eq:functions-composed}) results from the
existence of two historical schools that both contributed to the
formulation of this material.
\index{composition!functions!notation}
One school cleaved to the tradition of {\it abstract algebra}; they
wanted all expressions to be written with all operators---including
the composition operator $\circ$---in infix notation.  Another school,
which could be called {\it applicative algebraic}, wanted to view
functions as ``applying'' to their arguments.  Both notations have
significant advantages in certain contexts, so both have survived.  It
is a good idea for neophyte readers to be prepared to encounter both
notations---but they have to keep their eyes open regarding the
relative orders of $F$ and $G$.

\medskip

Before progressing with new material, it is worth taking a moment to
verify that the important operation of composition behaves the way one
would want and expect it to---by preserving the type of function being
composed.

\begin{prop}
\label{thm:fn-composition}
Let us be given functions $F: S \rightarrow S$ and $G: S \rightarrow
S$ on the set $S$, together with their composition $F \circ G$, as
defined in (\ref{eq:functions-composed}).

\noindent {\rm (a)}
$F \circ G$ is a function on $S$.

\noindent {\rm (b)}
If $F$ and $G$ are injections, then so also is $F \circ G$.

\noindent {\rm (c)}
If $F$ and $G$ are surjections, then so also is $F \circ G$.

\noindent {\rm (d)}
If $F$ and $G$ are bijections, then so also is $F \circ G$.
\end{prop}

\begin{proof}
We prove each of the four assertions by invoking the underlying
definitions.

\noindent (a)
%
Because $F$ and $G$ are functions on $S$: For each $s \in S$, there is
at most one $t_1 \in S$ such that $F(s) = t_1$ and at most one $t_2
\in S$ such that $G(s) = t_2$.  Hence, we identify three
possibilities.

\smallskip

\begin{tabular}{llll}
\hline
%1. &
{\bf if}
$F$ is defined at $s \in S$  & & & {\bf then} $F(s) \in S$ is unique \\
%  &
{\bf and if} $G$ is defined at $F(s) \in S$ & & & {\bf then} $G(F(s))
= F \circ G(s) \in S$ is unique \\
\hline
%2. &
{\bf if}
$F$ is not defined at $s \in S$ & & & {\bf then} $F \circ G$ is not
defined at $s \in S$ \\
\hline 
%3. & 
{\bf if}
$F$ is defined at $s \in S$ & & & {\bf then} $F(s) \in S$ is unique \\
%  &
{\bf and if} $G$ is not defined at $F(s) \in S$ & & & {\bf then}
$F \circ G$ not defined at $s \in S$ \\
\hline
\end{tabular}

\smallskip

\noindent
Hence, for each $s \in S$, there is at most one $t \in S$ such that $F
\circ G(s) = t$; in other words, $F \circ G$ is a function on $S$.

\medskip

\noindent (b)
%
Focus on any $s \in S$.  Because $F$ is an injection on $S$, there
exists at most one $t \in S$ such that $F(t) = s$.  Because $G$ is an
injection on $S$, there exists at most one $u \in S$ such that $G(u) =
t$.  Thus, there exists at most one $u \in S$ such that $F \circ G(u)
= s$.  Hence, $F \circ G(u)$ is an injection on $S$.

\medskip

\noindent (c)
%
Focus on any $s \in S$.  Because $F$ is a surjection on $S$, there
exists $t \in S$ such that $F(t) = s$.  Because $G$ is a surjection on
$S$, there exists $u \in S$ such that $G(u) = t$.  This means,
however, that $F \circ G(u) = s$.  Because $s \in S$ was arbitrary, it
follows that $F \circ G(u)$ is a surjection on $S$.

\medskip

\noindent (d)
%
If each of $F$ and $G$ is a bijection on $S$, then each is an
injection on $S$, and each is a surjection on $S$.  Then Part (b)
tells us that $F \circ G$ is an injection on $S$, and Part (c) tells
us that $F \circ G$ is a surjection on $S$.  Hence, $F \circ G$ is a
bijection on $S$.  \qed
\end{proof}


\section{Boolean Algebras}
\label{sec:Boolean-Algebra}
\index{Boolean Algebra}

A {\it Boolean algebra} is a mathematical system **HERE

\subsection{The Basics of Boolean Algebras}

\noindent {\it The Booean Operations}.
\index{Boolean Algebra!Operations}


\noindent{\it The Axioms of Boolean Algebra}.
\index{Boolean Algebra!axioms}

\noindent{The algebra of sets}.
\index{Boolean Algebra!the algebra of sets}

\subsection{The Algebra of Propositional Logic}
\label{sec:Propositional-logic}
\index{Propositional logic}\index{The Propositional Calculus}


\subsubsection{Logic via algebraic manipulation}


\noindent{The basic logical connectives}.
\index{Propositional logic!basic connectives}
%
The Boolean set-related operations we discussed in
Section~\ref{sec:set-operations} have important
analogues within the context of Propositional logic.
 {\em logical} analogues of these operations for logical
sentences and their logical {\em truth values},
\index{truth values}
%
{\sc true} and {\sc false}, often denoted $1$ and $0$, respectively:
\index{truth values!notations}
\begin{itemize}
\item
\underline{logical {\small\sf not} ($\sim$)}
\index{logical operation!{\sc not} ($\sim$)}
The operation {\small\sf not} is the logical analogue of the
set-theoretic operation of complementation.  Because always writing
``{\small\sf not}'' makes logical expressions long and cumbersome, we
usually use the prefix-operator $\sim$ to denote {\small\sf not};
i.e., we write \\
\hspace*{.35in}$\sim P$ \ \ rather than the more cumbersome
\ \ {\small\sf not} $P$ \\
to denote the logical complementation of proposition $P$.  Whichever
notation we use, the defining properties of logical complementation
are encapsulated in the following pair of equations
\[
[\sim \mbox{\sc true} = \mbox{\sc false}] \ \ \mbox{ and } \ \ [\sim
  \mbox{\sc false} = \mbox{\sc true}].
\]

\item
\underline{logical {\small\sf or} ($\vee$)}
\index{logical operation!{\small\sf or} ($\vee$)}
\index{logical operation!disjunction ($\vee$)}
\index{logical operation!logical sum ($\vee$)}
%
The operation {\small\sf or}---which is also called {\em disjunction}
or {\em logical sum}---is the logical analogue of the set-theoretic
operation of union.  For convenience and brevity, we usually use the
infix-operator $\vee$ to denote {\small\sf or} in expressions.
Whichever notation we use, the defining properties of logical
disjunction are encapsulated as follows.
\[
[[P \ \vee \ Q] =  \mbox{\sc true}] \ \ \mbox{ if, and only if, } \ \ 
[P = \mbox{\sc true}] \mbox{ or }
[Q = \mbox{\sc true}] \mbox{ or both}.
\]
Note that, as with union, logical {\small\sf or} is {\em inclusive:}
The assertion \\
\hspace*{.35in}$[P \vee Q]$ is \mbox{\sc true} \\
%
is true when {\em both} $P$ and $Q$ are true, as well as when only one
of them is.  Because such inclusivity does not always capture one's
intended meaning, another, {\em exclusive} version of disjunction also
exists, as we see next.

\item
\underline{logical {\small\sf xor} ($\oplus$)}
\index{logical operation!{\small\sf xor} ($\oplus$)}
\index{logical operation!exclusive or ($\oplus$)}
%
The operation {\em exclusive or}---which is also called {\small\sf
  xor}---is a version of disjunction that does not allow both
disjuncts to be true simultaneously.  For convenience and brevity, we
usually use the infix-operator $\oplus$ to denote {\small\sf xor} in
expressions.  Whichever notation we use, the defining properties of
exclusive or are encapsulated as follows. 
\[
[[P \ \oplus \ Q] =  \mbox{\sc true}] \ \ \mbox{ if, and only if, } \ \ 
[P = \mbox{\sc true}] \mbox{ or }
[Q = \mbox{\sc true}] \mbox{ {\em but not} both}.
\]
To emphasize the distinction between $\vee$ and $\oplus$: The
assertion \\
\hspace*{.35in}$[P \oplus Q]$ is \mbox{\sc true} \\
%
is {\em false} when {\em both} $P$ and $Q$ are true.

\item
\underline{logical {\small\sf and} ($\wedge$)}
\index{logical operation!{\small\sf and} ($\wedge$)}
\index{logical operation!conjunction ($\wedge$)}
\index{logical operation!logical product ($\wedge$)}
%
The operation {\small\sf and}---which is also called {\em conjunction}
or {\em logical product}---is the logical analogue of the
set-theoretic operation of intersection.  For convenience and brevity,
we usually use the infix-operator $\wedge$ to denote {\small\sf and}
in expressions.  Whichever notation we use, the defining properties of
logical conjunction are encapsulated as follows.
\[ [[P \ \wedge \ Q] = \mbox{\sc true}]  \ \ \mbox{ if, and only if,
  {\em both} } \ \ 
 [P = \mbox{\sc true}] \mbox{ and } [Q = \mbox{\sc true}]
\]

\item
\underline{logical implication ($\Rightarrow$)}
\index{logical operation!implies ($\Rightarrow$)}
\index{logical operation!implication ($\Rightarrow$)}
\index{logical operation!conditional ($\Rightarrow$)}
\index{logical operation!material implication ($\Rightarrow$)}
%
The logical operation of implication, which is often called {\it
  conditional} and which we usually denote in expressions via the
infix-operator $\Rightarrow$ differs from the other logical operations
we have discussed in a way that the reader must always keep in mind.
In contrast to {\small\sf not}, {\small\sf or}, and {\small\sf and},
whose formal versions pretty much coincide with their informal
versions, the formal version of implication, being formal, fixed, and
precise, carries connotations that we do not always associate with the
informal word ``implies.''  The formal version of implication is
defined as follows.
\[ [[P \ \Rightarrow \ Q] = \mbox{\sc true}]  \ \ \mbox{ if, and only
  if, } \ \
  [ [\sim P] = \mbox{\sc true}] \mbox{ (inclusive) or } [Q = \mbox{\sc true}]
\]
This definition means, in particular, that
  \begin{itemize}
  \item
If proposition $P$  is false, then it implies {\em every} proposition.
  \item
If proposition $Q$ is true, then it is implied by {\em every} proposition.
  \end{itemize}

\item
\underline{logical equivalence ($\equiv$)}
\index{logical operation!is equivalent to ($\equiv$)}
\index{logical operation!biconditional ($\equiv$)}
%
The final logical operation in our toolbox is variously called {\it
  equivalence,} as in: \\
\hspace*{.35in}Proposition $P$ is (logically) equivalent to
Proposition $Q$ \\
and {\it biconditional}.  We usually denote the operation in
expressions via the infix-operator $\equiv$.  The operation is defined
as follows.
\[ 
[[P \ \equiv \ Q] = \mbox{\sc true}]  \ \ \mbox{ if, and only if }
[[P \ \Rightarrow \ Q] = \mbox{\sc true}]  \ \ \mbox{ and } \ \
[[Q \ \Rightarrow \ P] = \mbox{\sc true}]
\]
\end{itemize}

We often use the term ``connective'' rather than ``operation'' to
refer to what we have here called the logical operations of the
Propositional Calclulus.  This is because one often feels that logical
propositions are static statements rather than active prescriptions
for computations.

\medskip

\noindent {\it The (Boolean) algebra of logical operations}
\index{Propositional logic!logic as a Boolean algebra}
\index{the Boolean algebra of logical operations}
\index{Boolean algebra!the Propositional Calculus}

**HERE

The operations of the Propositional Calclulus give rise to a Boolean
algebra that manipulates (logical) propositions in much the way that
the set-oriented Boolean algebra of Section~\ref{sec:Boolean-Algebra}
manipulates sets.


\subsubsection{Logic via Truth Tables}

\noindent{\it The logical connectives via truth tables}.
\index{logical operations!a functional view}
%
If one is willing to view statements in the Propositional Calclulus as
prescriptions for computations, then one can encapsulate the
definitions of the basic logical connectives via functions that map
logical expressions int the truth values {\small\sf true}, or $1$, and
{\small\sf false}, or $0$.  The following tables reproduce the
definitions of Subsection~A within this computational framework.
\begin{equation}
\label{eq:defns-via-tables}
\begin{array}{|c||c|}
\hline
P & \sim P \\
\hline
\hline
0 & 1 \\
\hline
1 & 0 \\
\hline
\end{array}
\hspace*{.5in}
\begin{array}{|c|c||c||c||c||c||c|}
\hline
P & Q & P \vee Q  & P \oplus Q & P \wedge Q & P \Rightarrow Q & P \equiv Q  \\
\hline
\hline
0 & 0 & 0 & 0 & 0 & 1 & 1 \\
\hline
0 & 1 & 1 & 1 & 0 & 1 & 0 \\
\hline
1 & 0 & 1 & 1 & 0 & 0 & 0 \\
\hline
1 & 1 & 1 & 0 & 1 & 1 & 1 \\
\hline
\end{array}
\end{equation}

Note how the truth-value entries in the righthand truth table of
(\ref{eq:defns-via-tables}) reinforce the lessons of Subsection~A.
This awareness is especially essential with the implication-related
operations ($\Rightarrow$ and $\equiv$) that have informal
counterparts in ``real-life'' reasoning.  Think about whether your
intuition agrees with the formal specifications of these operations.

\medskip

\noindent{Logic via truth values}.
\index{Propositional logic!logic via truth values}
%
As demonstrated in Subsection~C, the Propositional Calculus is a
Boolean Algebra when one interprets the logical connectives of
Subsection~A as operations on logical expressions that evaluate to
either {\sc true} (or, $1$) or {\sc false} (or, $0$).  But, the
calculus forms a very special genre of Boolean Algebra, namely, a {\em
  free Boolean Algebra}.\index{Boolean algebra!{\em free} algebra}

\bigskip

\noindent \fbox{
\begin{minipage}{0.95\textwidth}
The meaning of the qualifier {\it free} is easy to state but not so
easy to understand.  Here is a very informal try.

Any sort of algebra is defined as a set of objects accompanied by a
set of basic operations, wherein the operations obey certain
``self-evident'' \index{axioms as ``self-evident truths''}
axioms.\footnote{Indeed, that is what the word ``axiom'' means.}  In
rough, but evocative, terms, the algebra is {\em free} if its
operations do not relate to one another in any way that is not
mandated by the axioms.

An easy illustration of this decidedly {\em not easy} concept is the
following.  There is a (quite important) genre of algebra called a
{\it semi-group}.\index{semi-group} A semi-group $\s$ is specified via
a set (of objects) $S$, together with a binary operation on $S$, which
we denote $\oplus$.  For the (algebraic) structure $\s = \langle S,
\oplus \rangle$ to be a semi-group, it must obey the following two
axioms.
\begin{enumerate}
\item
$\s$ must be {\it closed} under operation $\oplus$; i.e., for all
  $s_1, s_2 \in S$, we must have
\[ (s_1 \oplus s_2) \in S. \]
\item
The operation $\oplus$ must obey the {\em associative law;}
\index{associative law}
i.e., for all $s_1, s_2, s_3 \in S$, we must have
\[ (s_1 \oplus s_2) \oplus s_3 \ = \ s_1 \oplus (s_2 \oplus s_3)
\]
\end{enumerate}

Now, it is easy to verify---we shall discuss this in
Chapter~\ref{ch:arithmetic}---that the (algebraic) structure formed by
the integers coupled with the operation of addition forms a semi-group
(which the notation of Chapter~\ref{ch:arithmetic} would denote
$\langle \Z, + \rangle$).  But this semi-group is {\em not}
free, because integer addition obeys laws beyond just closure and
associativity, namely:
\begin{enumerate}
\item
Addition of integers is {\em commutative}.
\item
Integer addition has an {\em identity}, namely $0$.
\item
Every integer $z \in \Z$ has an {\it additive inverse} (namely, $-z$).
\end{enumerate}

So, it really is meaningful that the Boolean Algebra built upon the
Propositional calculus {\em is} free.
\end{minipage}
}
\bigskip

\noindent
For us, the impact of the ``freeness'' of the Propositional algebra,
{\it qua} Boolean Algebra, is manifest in the following
\index{meta-theorem}
{\em meta-theorem}\footnote{A {\it theorem} exposes some truth within
  the mathematical structure being discussed.  A {\it meta-theorem}
  exposes some truth about the way the discussion can proceed.}~which
is discussed and proved in \cite{Rosser53}.

\begin{theorem}
\label{thm:Prop-calc-isfree}
A propositional expression $E(P, Q, \ldots, R)$ is a theorem of
Propositional logic if, and only if, it to {\sc true} under all truth
assignments to the propositions $P, Q, \ldots, R$.
\end{theorem}

Proving Theorem~\ref{thm:Prop-calc-isfree} is beyond the scope of this
book, but we now present a few illustrative instantiations, annotated
to suggest their ``real-world'' messages.  Each example is accompanied
by a ``real-life'' interpretation and a verifying truth table.

\bigskip

\noindent
\underline{\small\sf The law of double negation} \\
\index{Propositional logic!Truth tables!the law of double negation}
\index{Truth tables!the law of double negation}
This is a formal analogue of the homely adage that ``a double negative
is a positive.''

\begin{prop}
\label{thm:double negation}
For any proposition $P$,
\[ P \ \ \equiv \ \ \sim [\sim P] \]
\end{prop}

\noindent {\it Proof via truth table.}
\begin{equation}
\label{eq:double-neg}
\begin{array}{|c|c||c|}
\hline
P & \sim P & \sim[\sim P] \\
\hline
\hline
0 & 1 & 0 \\
\hline
1 & 0 & 1 \\
\hline
\end{array}
\end{equation}
Note that columns 1 and 3 of truth table (\ref{eq:double-neg}) are
identical.  By Theorem~\ref{thm:Prop-calc-isfree}, this fact verifies
Proposition~\ref{thm:double negation}, the law of double negation.
\qed

\bigskip

\noindent 
\underline{\small\sf The law of contraposition} \\
\index{Propositional logic!Truth tables!the law of contraposition}
\index{Truth tables!the law of contraposition}
This is a very exciting example!  Let us immediately convert this to a
mathematical statement about the Boolean Algebra of Propositional
logic and then prove the statement.  We shall then contemplate the
implications of this law for {\em logic} rather than {\em
  mathematics}.\index{contraposition!inthe Boolean Algenra of propositions}

\begin{prop}
\label{thm:contraposition}
For any propositions $P$ and $Q$,
\[  \left[ [ P \Rightarrow Q ] \ \ \equiv \ \ [ \sim Q
    \Rightarrow \sim P ] \right]
\]
\end{prop}

\noindent {\it Proof via truth table.}
\begin{equation}
\label{eq:contraposition}
\begin{array}{|c|c|c|c||c||c|}
\hline
P & \sim P & Q & \sim Q & \underline{P \Rightarrow Q}
 & \underline{\sim Q \Rightarrow \sim P} \\
\hline
\hline
0 & 1 & 0 & 1 & 1 & 1 \\
\hline
0 & 1 & 1 & 0 & 1 & 1 \\
\hline
1 & 0 & 0 & 1 & 0 & 0 \\
\hline
1 & 0 & 1 & 0 & 1 & 1 \\
\hline
\end{array}
\end{equation}
Note that columns 5 and 6 of truth table (\ref{eq:contraposition}) are
identical.  By Theorem~\ref{thm:Prop-calc-isfree}, this fact verifies
Proposition~\ref{thm:contraposition}, the law of contraposition.
\qed

\smallskip

Now let us reconsider the whole concept of contraposition, including
this law, in the light of {\em logic and reasoning}.
\index{contraposition!in logic, reasoning}

 asserts that the assertion \\
\hspace*{.35in}
``Proposition $Q$ implies Proposition $Q$'' \\
is {\em logically equivalent} to the assertion \\
\hspace*{.35in}
``the negation of Proposition $Q$ implies the negation of Proposition
$P$''.  \\
Think about this!  In any system of reasoning in which a given
proposition is either {\small\sf true} or {\small\sf false} 





\bigskip

\noindent 
\underline{\it De Morgan's Laws}:\index{Propositional logic!Truth
  tables!verify De Morgan's Laws}\index{Truth tables!verify De Morgan's Laws}

\begin{prop}
\label{thm:De-Morgan}
For any propositions $P$ and $Q$:
\begin{itemize}
\item
$[ P \wedge Q ] \ \ \equiv \ \ \sim [ [\sim P] \vee [\sim Q]]$
\item
$[ P \vee Q ] \ \ \equiv \ \ \sim [ [\sim P] \wedge [\sim Q]]$
\end{itemize}
\end{prop}

\noindent {\it Proof via truth table.}
\begin{equation}
\label{eq:DeMorgan}
\begin{array}{|c|c|c|c||c|c|||c|c|}
\hline
P & \sim P & Q & \sim Q 
  & [ P \wedge Q ]
  & [\sim P] \vee [\sim Q]
  & [ P \vee Q ]
  & [\sim P] \wedge [\sim Q] \\
\hline
\hline
0 & 1 & 0 & 1
  & 0
  & 1
  & 0
  & 1 \\
\hline
0 & 1 & 1 & 0
  & 0
  & 1
  & 1
  & 1 \\
\hline
1 & 0 & 0 & 1
  & 0
  & 1
  & 1
  & 1 \\
\hline
1 & 0 & 1 & 0
  & 1
  & 0
  & 1
  & 0 \\
\hline
\end{array}
\end{equation}

Note that columns 5 and 6 of truth table (\ref{eq:DeMorgan}) are
mutually complementary, as are columns 7 and 8.  If we negate (or,
complement) the entries of columns 6 and 8, then we can invoke
Theorem~\ref{thm:Prop-calc-isfree} to verify
proposition~\ref{thm:De-Morgan}, which encapsulates De Morgan's laws
for Propositional logic.  \qed

\medskip

\noindent 
\underline{\small\sf The distributive laws for Propositional
  logic}\index{Propositional logic!Truth tables!the distributive
  laws}\index{Truth tables!the distributive laws for Propositional logic}


\noindent
In numerical arithmetic, multiplication distributes over addition, but
not conversely, so we have a single distributive law for arithmetic
(see Section~\ref{sec:Arithmetic-Laws}).  In contrast, each of logical
multiplication and logical addition distributes over the other, so we
have two distributive laws for Propositional logic.
\begin{itemize}
\item
$ P \vee [ Q \wedge R] \ \ \equiv \ \ [P \vee Q] \wedge R$
\item
$P \wedge [ Q \vee R] \ \ \equiv \ \ [P \wedge Q] \vee R$
\end{itemize}
{\small
\begin{equation}
\label{eq:distrib-law}
\begin{array}{|c|c|c||c|c|c|c|||c|c||c|c|}
\hline
P & Q & R
  & [P \vee Q]
  & [P \wedge Q]
  & [Q \wedge R] 
  & [Q \vee R] 
  & P \vee [ Q \wedge R]
  & [P \vee Q] \wedge [P \vee R]
  & P \wedge [ Q \vee R]
  & [P \wedge Q] \vee [P \wedge R] \\
\hline
\hline
0 & 0 & 0
  & 0
  & 0
  & 0
  & 0
  & 0
  & 0
  & 0
  & 0 \\ 
\hline
0 & 0 & 1
  & 0
  & 0
  & 0
  & 1
  & 0
  & 0
  & 0 
  & 0 \\
\hline
0 & 1 & 0
  & 1
  & 0
  & 0
  & 1
  & 0
  & 0
  & 0
  & 0 \\
\hline
0 & 1 & 1
  & 1
  & 0
  & 1
  & 1
  & 1
  & 1
  & 0
  & 0 \\
\hline
1 & 0 & 0
  & 1
  & 0
  & 0
  & 0
  & 1
  & 1
  & 0
  & 0 \\
\hline
1 & 0 & 1
  & 1
  & 0
  & 1
  & 0
  & 1
  & 1
  & 1
  & 1 \\
\hline
1 & 1 & 0
  & 1
  & 1
  & 0
  & 1
  & 1
  & 1
  & 1
  & 1 \\
\hline
1 & 1 & 1
  & 1
  & 1
  & 1
  & 1
  & 1
  & 1
  & 1
  & 1 \\
\hline
\end{array}
\end{equation}
}

Note that columns 8 and 9 of truth table (\ref{eq:distrib-law}) are
identical, as are columns 10 and 11.  By
Theorem~\ref{thm:Prop-calc-isfree} this fact verifies the distributive
laws for Propositional logic.

\subsubsection{Satisfiability problems}
\label{sec:Satisfiability}

Satisfiability problems deal with propositional formulae that are
populated by entities that can assume the truth values {\small\sf
  true} and {\small\sf false}. \index{logical expressions!truth values}
The {\em underlying} entities are {\it logical variables},
\index{logical expressions!logical variables}
i.e., variables that range over the truth values.  The {\em actual}
entities that appear in each formula are {\it logical literals}, 
\index{logical expressions!logical literals}
i.e., instances of logical variables in either their {\em true} or
{\em complemented} forms.
\index{logical expressions!logical literals!true or complemented}
In detail:
\begin{itemize}
\item
In its {\em true} form, a literal evaluates to {\small\sf true}
precisely when its associated variable does.
\item
In its {\em complemented} form, a literal evaluates to {\small\sf
  true} precisely when its associated variable evaluates to {\small\sf
  false}.
\end{itemize}
The following simple example should clarify these terms.

\begin{tabular}{ll}
{\bf The formula}:  & $(\bar{x} \vee y) \wedge (x \vee \bar{y})$ \\
{\bf The variables}: & $x$ and $y$ \\
{\bf The literals}:  & $x$ and $y$ (true form); \ $\bar{x}$ and $\bar{y}$
(complemented form)
\end{tabular}
\[
\begin{array}{ccccccccccc}
( & \bar{x} & \vee & y & ) & \wedge & ( & x & \vee & \bar{y} & ) \\
  & \uparrow &     & \uparrow & & & & \uparrow & & \uparrow & \\
  & \mbox{complemented} &  & \mbox{true}  & & & & \mbox{true} &
        & \mbox{complemented} &  \\
  & \mbox{literal} & & \mbox{literal} & & & & \mbox{literal} & &
  \mbox{literal} & 
\end{array}
\]

\paragraph{\small\sf A. {\sf SAT}: the original {\sf NP}-complete problem}

The general form of {\it The Satisfiability Problem for the
  Propositional Calculus} 
\index{The Satisfiability Problem (for the Propositional Calculus)}
is denoted {\sf SAT}. \index{{\sf SAT}: The Satisfiability Problem} 
\index{{\sf SAT}}\index{{\sf SAT}!definition}
The problem can be viewed in two formally distinct but algorithmically
equivalent ways.
\begin{itemize}
\item
In the {\it combinatorial formulation}:
\index{Satisfiability problem!combinatorial formulation}
\index{{\sf SAT}!combinatorial formulation}
  \begin{itemize}
  \item
The Satisfiability Problem is specified by a set of {\it logical
  clauses}, each clause being a {\it set of {\em logical} variables},
i.e., variables that range over the truth values {\small\sf true} and
{\small\sf false}.
  \item
The Satisfiability question is: Can one assign truth values to all of
the logical variables of the formula in such a way that every clause
ends up with at least one {\small\sf true} literal?
  \end{itemize}
\item
In the {\it formal logical formulation}:
\index{Satisfiability problem!formal logical formulation}
\index{{\sf SAT}!formal logical formulation}
  \begin{itemize}
  \item
The Satisfiability Problem is specified by a propositional formula
$\Phi$ that is  a {\it conjunction of disjuncts of logical literals}.

In the lingo of the ``in-crowd'', these expressions are said to be in
{\it POS} form, \index{POS-form expression} shorthand for a {\it
  (logical) product of (logical) sums}.
  \item
The Satisfiability question is: Can one assign truth values to all of
the logical variables of formula $\Phi$ in a way that has every disjunct
evaluate to {\small\sf true}?
  \end{itemize}
\end{itemize}
The reader should be able to prove that these are just two views of
the same logical problem.  {\Arny This is a good exercise to verify
  command of the underlying concepts.}

Until 1971, POS expressions were largely viewed simply as the duals of
the somewhat more common {\it SOP}-form expressions, 
\index{SOP-form expression}, shorthand for {\it sums of products}.  A
discovery in 1971 by Stephen A.~Cook \index{Cook, Stephen A.}  changed
this view, by exposing, in \cite{Cook71}, how POS-form expressions can
be used to model a number of computational phenomena whose importance
was only beginning to be appreciated at that time.  These phenomena
were {\it nondeterminism in computation}
\index{nondeterminism in computation} and {\it computational complexity}.
\index{computational complexity}

\medskip

{\it Nondeterminism} can best be thought of in one of two ways.  The
first, more concrete, way to think about a nondeterministic computing
device is as a device that can ``make guesses'' at various junctures
of a computation, rather than performing some prolonged search through
plausible alternative fixed paths.  The second, more idealisitic, way
to think about a nondeterminism is by imagining that a computing
device has the capability of spawning alternative universes that it
can proceed through ``in parallel.''

\bigskip

\noindent \fbox{
\begin{minipage}{0.95\textwidth}
We place the phrase ``in parallel'' within quotes because parallel
computation is a concrete computational paradigm, while
nondeterministic computation is a conceptual framework for thinking
about the computational process.
\end{minipage}
}
\bigskip

\noindent 
Nondeterminism was invented during the 1950s, under a variety of
names, as a conceptual framework for understand the computing power of
{\it finite automata}.  \index{finite automata} The concept's
appearance in the seminal work \cite{RabinS59} by Michael O.~Rabin
\index{Rabin, Michael O.} and Dana Scott \index{Scott, Dana}
established the standard way of formulating and discussing the topic.

\medskip

By the early 1960s, computation theorists recognized the importance of
developing conceptual, mathematical, tools for discussing, analyzing,
and quantifying the {\it complexity} of computations.  Sample
motivating questions were:
\begin{itemize}
\item
Is it really harder to multiply two numbers than to add them?
\item
Does multiplying two $n \times n$ matrices really require
$\Omega(n^3)$ operations?
\item
Does computational time correlate with program size?
\end{itemize}
For close to a decade, models for studying such questions were
proposed, discarded, improved, and embellished, but no consensus
developed about an intellectually uniform approach to the topic.  This
situation largely changed because of Cook's work---and nondeterminism
played a significant role in the change.

\medskip

In 1971, Cook published, in \cite{Cook71}, a revolutionary new
foundation for the field of {\it computational complexity}.
\index{computational complexity} Cook achieved this goal by adapting
conceptual frameworks\footnote{These frameworks were the legacy of
  intellectual giants such as Kurt G\"{o}del \index{G\"{o}del, Kurt}
  (in \cite{Goedel31}) and Alan M.~Turing \index{Turing, Alan M.} (in
  \cite{Turing36}).}~from the decades-old field known variously as
{\it computability theory} \index{computability theory} and {\it
  computation theory}. \index{computation theory}
Whereas computability theory enabled one to study questions about the
{\em feasibility} of computing a function $f$:
\begin{itemize}
\item
Can one write a program to compute function $f$?
\end{itemize}
Cook's adaptation enabled one to rigorously study questions of the
following types about the {\em complexity} of computing functions $f$
and $g$.
\begin{itemize}
\item
Why is it easy (computationally) to prove that $f(x) = y$, but hard to
compute $f(x)$?
\item
Why can you adapt any program that computes function $f$ to a
``similar'' program that computes function $g$, even though these
functions do not apparently have any relationship to one another?
\item
Is function $f$ inherently harder to compute than function $g$?
\end{itemize}
Cook's ingenious adaptation created a new theory that was named {\it
  (the theory of) {\sf NP}-completeness}, \index{{\sf NP}-completeness}
\index{{\sf NP}-complete computational problems} for reasons that are
explained in sources such as \index{GareyJ79}.  In very informal, but
hopefully evocative terms, {\sf NP}-complete computational problems
are problems whose solutions are {\em easy to check} but {\em hard to
  find}.  (Of course, the adjectives ``hard'' and ``easy'' have
technical meanings, but developing these meanings is beyond the scope
of this text.  These concepts are developed ``gently'' but
systematically in texts such as \index{Rosenberg09}.)

Tying up loose ends: The notion {\em easy to check} is formalized
using the concept of computational nondeterminism that we just
discussed.  And, the technical apparatus used to represent
nondeterminism in Cook's formulation is manifest in the disjunctions
that abound in POS-form expressions---and in the associated
Satisfiability problem {\sf SAT}!

The reason we relate this story at this point is that the problem {\sf
  SAT} was the first ``natural'' problem that was shown by Cook to be
{\sf NP}-complete!  (The term ``{\sf NP}-complete'' does not occur in
\cite{Cook71}; there was not yet a name for this concept.)  To round
out the story, within a year, Richard M.~Karp \index{Karp, Richard M.}
and colleagues had greatly lengthened the list of computational
problems that Cook's nascent theory encompassed.  The details of the
theory of {\sf NP}-completeness are beyond the scope of this text, but
we do want the reader to recognize the following.

It remains mathematically {\em possible} that {\sf NP}-complete
problems can be solved ``efficiently''---meaning, technically, ``in
time polynomial in the size of the description of the problem''.  (For
the {\sf SAT} problem, this size is measured in terms of the number of
literals in the POS popositoinal formula.)  However it is
mathematically {\em certain} that if any one problem in this class has
a polynomial-time solution, then every problem in this class does.
This is because of the following intellectually exciting fact, which
is known as {\it Cook's Theorem} \index{Cook's Theorem}.  Even without
the technical details---which are available in sources such as
\cite{GareyJ79,Rosenberg09}---the excitement of this result should be
clear.

\begin{theorem}[Cook's Theorem \cite{Cook71}]
\label{thm:Cook}
{\bf (a)}
The Satisfiability Problem for the Propositional Calculus, {\sf SAT},
is {\sf NP}-complete.

\noindent{\bf (b)}
All {\sf NP}-complete problems can be efficiently ``translated'' as
one another.
\end{theorem}

The detailed meaning of part (b) of Theorem~\ref{thm:Cook} is the
following.  {\em Let {\sf A} and {\sf B} be {\sf NP}-complete
  problems.  There exists a polynomial-time computable function
  $f_{{\sf A} \rightarrow {\sf B}}$ that produces an instance of
  problem {\sf B} when given an instance of problem {\sf A}.}

\medskip

Given the half-century that has passed since the work of Cook and
Karp, and given the {\em practical} importance of many of the problems
that are known to be {\sf NP}-complete, it is widely {\em suspected}
that no {\sf NP}-complete problem can be solved via a polynomial-time
algorithm.

\medskip

There is a large, vibrant literature concerning approaches for
lessening the negative computational impacts of {\sf
  NP}-completeness---by considering significant special cases of {\sf
  NP}-complete problems, by developing approximate solutions to such
problems, and by developing heuristics for such problems, whose
behavior cannot be guaranteed but which seem to work well ``in
practice''.


\paragraph{\small\sf B. 2SAT: SAT with $2$ literals per clause}

Once a framework for discussing the complexity of computations
received general acceptance, researchers began to investigate the
boundaries within the framework: How, and how much, could one
``weaken'' an {\sf NP}-complete problem before one lost the problem's
{\sf NP}-complete?  One of the earliest examples of this activity
occurred within the paper that actually invented the framework,
namely, \cite{Cook71}!  A careful analysis revealed the following
strengthening of Theorem~\ref{thm:Cook}(a).

\begin{theorem}[Cook's Theorem: stronger version]
\label{thm:Cook-3SAT}
{\sf 3SAT}, the {\sf SAT} problem with $3$ literals per clause is {\sf
  NP}-complete.
\end{theorem}
\index{{\sf 3SAT}: the {\sf SAT} problem with $3$ literals per clause}
\index{{\sf SAT}!{\sf 3SAT}: {\sf SAT} with $3$ literals per clause}

Of course, the natural question raised by Theorem~\ref{thm:Cook-3SAT}
is: What is the complexity of {\sf 2SAT}: the variant {\sf SAT}
problem in which each clause has only $2$ literals?
\index{{\sf 2SAT}: the {\sf SAT} problem with $2$ literals per clause}
\index{{\sf SAT}!{\sf 2SAT}: {\sf SAT} with $2$ literals per clause}
This section is dedicated to answering this question.  We include this
computation-theoretic material in our ``pure'' mathematics text
because it turns out that the answer reveals, in an elegant manner, an
interesting, nonobvious intersection region for the fields of
mathematical logic, computational complexity, algorithms, and graph
theory; indeed, we shall defer developing this answer until we 
introduce the basic notions concerning graphs, in
Section~\ref{sec:graph-model-2SAT}.

\begin{prop}
\label{thm:2SAT}
The {\sf 2SAT} problem can be solved in polynomial time.

\noindent
That is, given any instance $\Phi$ of {\sf 2SAT}, one can determine in
time polynomial in the number of literals in $\Phi$ whether there
exists a satisfying assignment of truth-values to the variables of
$\Phi$.
\end{prop}

The proof of Proposition~\ref{thm:2SAT} requires the basic
connectivity-related notions underlying graphs.  These are available
in Section~\ref{sec:connectivity-notions}, and the proof follows
immediately, in Section~\ref{sec:graph-model-2SAT}.



\subsection{Connecting Mathematical Logic with Logical
  Reasoning}\index{Propositonal logic!connection with logical reasoning}
\label{sec:practical-logic}

\subsubsection{A formal notion of {\em implication}, and its
  implications}
\label{sec:implication}

In everyday discourse, we all employ an intuitive notion of {\it implication}.
\index{implication}
When we make the assertion \\
\hspace*{.35in}Proposition $A$ {\it implies} Proposition $B$ \\
what we usually have in mind is \\
\hspace*{.35in}If Proposition $A$ is true, then Proposition $B$ istrue. \\
But this, or any, homespun meaning of the word/concept ``implies''
raises many questions.
\begin{itemize}
\item
What if Proposition $A$ is {\em not} true?  Are there any inferences
we can draw?

\item
Is there any relation between the assertion \\
\hspace*{.35in}Proposition $A$ {\it implies} Proposition $B$ \\
and its {\it converse}\index{implication!converse}
assertion \\
\hspace*{.35in}Proposition $B$ {\it implies} Proposition $A$?

\item
If we know that Proposition $B$ is true, shouldn't it be ``implied by
every other proposition---perhaps even a {\em false} one?
\end{itemize}
This section is devoted to discussing the {\em formal} notion of
implication
\index{implication!formal notion}
\index{implication!formal notion!''pro and con''}
%
that was adopted by mathematical philosophers in the 19th century.
The {\em advantage} of having such a formal notion is that it will
answer all questions of the sort we have just posed.  The {\em
  disadvantage} of having such a formal notion is that the way in
which the notion answers some of our questions may be rather counter
to one's untutored intuition.
