%version of 05-15-19

\chapter{ARITHMETIC}
\label{ch:arithmetic}


\hfill
\begin{quote}
{\it Die Mathematik ist die K\"{o}nigin der Wissenschaften und die
Zahlentheorie ist die K\"{o}nigin der Mathematik.}

[Mathematics is the queen of sciences and number theory
  $\langle$often, ``arithmetic''$\rangle$ is the queen of
  mathematics.]

\hfill
\begin{tabular}{l}
Quoted in {\it Gauss zum Ged√§chtniss} (1856) by \\
Wolfgang Sartorius von Waltershausen 
\end{tabular}
\end{quote}

\bigskip

Previous chapters have given us sets, numbers, and numerals, the
simple objects that we use to count and measure and aggregate.  The
current chapter is devoted to expounding the rules of {\em
  arithmetic}, the system which allows us to manipulate these objects
and to develop complex objects out of the simple ones.

\section{The Basic Arithmetic Operations}
\label{sec:Arithmetic-Tools}
\index{laws of arithmetic}

The basic tools of arithmetic consist of a small set of operations on
numbers/numerals, together with two special integers which play
important roles with respect to the operations.  Since these entities,
the operations and special numbers, are so tightly intertwined, we
discuss them simultaneously.

\smallskip

\noindent {\small\sf The two special integers.}
\index{arithmetic!the arithmetic identities, $0$ and $1$}
The integers zero ($0$)
\index{number!zero ($0$), the additive identity} 
and one ($1$),
\index{number!one ($1$), the multiplicative identity}
play special roles as the tools of arithmetic are applied to all four
classes of numbers described in Chapter~\ref{ch:numbers-numerals}.

\smallskip

\noindent {\small\sf The operations of arithmetic.}\index{arithmetic!basic operations}
%
Arithmetic on the classes of numbers described in
Chapter~\ref{ch:numbers-numerals} is built upon a rather small
repertoire of operations.  When we say that an operation produces a
number ``of the same sort'', we mean that it produces
\begin{itemize}
\item
an integer result from integer arguments;
\item
a rational (number) result from rational (number) arguments;
\item
a real (number) result from real (number) arguments;
\item
a complex (number) result from complex (number) arguments;
\end{itemize}
The fundamental operations on numbers are, of course, familiar to the
reader.  Our goal in discussing them is to stress the laws that govern
the operations.  Along the way, we also introduce a few operations
that are less familiar but no less important.

\subsection{Unary (single-argument) operations}
\label{sec:unary-ops}


\subsubsection{Negating and reciprocating numbers}
\index{arithmetic!basic operations!negation}

\noindent {\it i. The operation of {\em negation}}:
\index{arithmetic!basic operations!negating}
\begin{itemize}
\item
is a {\em total function} on the sets $\Z, \Q, \R, \C$.  It replaces
a number $a$ by its {\em negative},
\index{number!negative}
which is a number of the same sort.

The {\it negative of $a$} is denoted $-a$ and is usually articulated
``minus $a$'' or ``negative $a$''.
\item
is a {\em partial function} on the nonnegative subsets
of $\Z, \Q, \R, \C$.  It replaces a number $a$ by its negative, $-a$,
whenever both $a$ and $-a$ belong to the nonnegative subset being
operated on.
\end{itemize}
Zero ($0$) is the unique {\it fixed point}\index{function!fixed
  point}\index{arithmetic!negation!fixed point} of the operation,
meaning that $0$ is the unique number $a$ such that $a = -a$.

\medskip

\noindent {\it ii. The operation of {\em reciprocation}}:
\index{arithmetic!basic operations!reciprocal}
\begin{itemize}
\item
\index{arithmetic!basic operations!reciprocating}
is a {\em total function} on the sets $\Q, \R, \C$, which replaces each
number $a$ by its {\em reciprocal}, 
\index{number!reciprocal}
a number of the same sort.

The {\it reciprocal of $a$} is denoted $1/a$ or $\displaystyle {1
  \over a}$; we shall employ whichever notation enhances legibility.
It is usually articulated ``$1$ over $a$''.

\item
is {\em undefined} on every integer $a$ except for $a=1$.
\end{itemize}

\subsubsection{Floors, ceilings, magnitudes}
\index{arithmetic!basic operations!floor}
\index{arithmetic!basic operations!ceiling}
\index{arithmetic!basic operations!absolute value}
\index{arithmetic!basic operations!magnitude}

\noindent {\it i. The operations of {\em taking floors and ceilings}}
are total operations on the sets $\N, \Z, \Q, \R$.
\begin{itemize}
\item
The {\it floor} of a number $a$, also called {\it the integer part}
\index{arithmetic!basic operations!integer part of a number}
\index{arithmetic!basic operations!floor of a number}
of $a$, is denoted $\lfloor a \rfloor$.  It is the largest integer
which does not exceed $a$; i.e.,:
\[
\lfloor a \rfloor \ \eqdef \ \max_{b \in {\mathbb{N}}} \Big[ b \ \leq a \Big]
\]
\item
The {\it ceiling} of a number $a$
\index{arithmetic!basic operations!ceiling of a number}
of $a$, denoted $\lceil a \rceil$, is the smallest integer which is 
not smaller than $a$:
\[
\lceil a \rceil \ \eqdef \ \min_{b \in {\mathbb{N}}} \Big[ b \ \geq a \Big]
\]
\end{itemize}
The the operations of taking floors and ceilings are two ways to {\em
  round} rationals and reals to their ``closest''
integers.\index{arithmetic!basic operations!rounding to ``closest''
  integer}

\medskip

\noindent {\it ii. The operations of taking {\em absolute values/magnitudes}}:
\index{arithmetic!basic operations!absolute value, magnitude}
%
Let $a$ be a real number.  The {\it absolute value}, or, {\it
  magnitude}, of $a$, denoted $|a|$ equals either $a$ or $-a$,
whichever is positive.  For a complex number $a$, the definition of
$|a|$ is more complicated: it is a measure of $a$'s ``distance'' from
the ``origin'' complex number $0 + 0 \cdot i$.  In detail:
\[
|a| \ = \ \left\{
\begin{array}{cl}
a & \mbox{ if } \ \big[a \in \R \big] \ \ \mbox{ and } \ \ \big[a \geq
  0\big] \\
-a & \mbox{ if } \ \big[a \in \R\big] \ \ \mbox{ and } \ \ \big[a <
  0\big] \\
\sqrt{b^2 + c^2} &  \mbox{ if } \ \big[a \in \C\big]  \ \ \mbox{ and }
     \ \ \big[a = (b+ci)\big]
\end{array}
\right.
\]

\subsubsection{Factorials (of nonnegative integers)}
\index{arithmetic!basic operations!factorial (of a nonnegative integer)}

The {\it factorial} of a nonnegative integer $n \in \N$, which is
commonly denoted $n!$,
\index{arithmetic!basic operations!$n!$: factorial of $n \in \N$}
\index{arithmetic!basic operations!factorial of nonnegative integer}
is the function defined via the following recursion.
\begin{equation}
\label{eq:n-factorial-recursion}
\mbox{\sc fact}(n) \ = \ \left\{
\begin{array}{cl}
1 & \mbox{  if } \ n=0 \\
n \cdot \mbox{\sc fact}(n-1) & \mbox{  if } \ n>0
\end{array}
\right.
\end{equation}
We can ``unwinding'' the recursion in (\ref{eq:n-factorial-recursion})
to derive an iterative expression for $n!$.  In what follows, we
denote multiplication by the familiar symbol $\times$ rather than the
centered dot ($\cdot$) to enhance legibility.  We finds that, for all
$n \in \N$,
\begin{equation}
\label{eq:n-factorial-direct}
n! \ = \ \mbox{\sc fact}(n) \ = \ 
n \times (n-1) \times (n-2) \times \cdots \times 2 \times 1
\end{equation} 
A $3$-step inductive argument validates this ``unwinding'':
\begin{enumerate}
\item
If $n =0$, then {\sc fact}$(n) = 1$, by definition
(\ref{eq:n-factorial-recursion}).
\item
Assume, for induction, that the expansion in
(\ref{eq:n-factorial-direct}) is valid for a given $k \in N$:
\[ \mbox{\sc fact}(k) \ = \ k \times (k-1) \times (k-2) \times \cdots
\times 2 \times 1 \] 
\item
Then:
\[
\begin{array}{lclll}
\mbox{\sc fact}(k+1) & = & (k+1) \cdot \mbox{\sc fact}(k)
  & & \mbox{by (\ref{eq:n-factorial-recursion})} \\
  & = &
(k+1) \times k \times (k-1) \times (k-2) \times \cdots \times 2 \times 1
  & & \mbox{by induction}
\end{array}
\]
\end{enumerate}


\subsection{Binary (two-argument) operations}
\label{sec:binary-operators}

\subsubsection{Addition and Subtraction}
\index{arithmetic!basic operations!addition}
\index{arithmetic!basic operations!subtraction}

The operation of {\it addition}\index{arithmetic!addition} is a {\em
  total function} that replaces any two numbers $a$ and $b$ by a
number of the same sort.  The resulting number is the {\em sum of $a$
  and $b$}\index{arithmetic!addition!sum} and is denoted $a+b$.

\noindent
The operation of {\it subtraction}\index{arithmetic!subtraction} is a
{\em total function} on the sets $\Z, \Q, \R, \C$, which replaces any
two numbers $a$ and $b$ by a number of the same sort.  The resulting
number is the {\em difference of $a$ and $b$}
\index{arithmetic!subtraction!difference} and is denoted $a-b$.  On
the nonnegative subsets of the sets $\Z, \Q, \R, \C$---such as $\N$,
which is the largest nonnegative subset of $\Z$---subtraction is a
{\em partial function}, which is defined only when $a \geq b$.

Subtraction can also be defined as follows.  For any two numbers $a$
and $b$, {\em the difference of $a$ and $b$ is the sum of $a$ and the
  negation of $b$}:
\[ a-b \ = \ a + (-b) \]

\medskip

\noindent {\em The special role of $0$ under addition and subtraction.}
The number $0$ is the {\it identity} under addition and
  subtraction;\index{number!additive identity}
\index{number!identity under addition}\index{identity!additive}
terminologically, $0$ is an {\it additive identity}.  This means that,
for all numbers $a$,
\[ a+0 \ = \ a-0 \ = \ a. \]
One shows as follows that $0$ is {\em the} additive identity.

\begin{prop}
\label{thm:unique-add-iden}
The number $0$ is the unique additive identity.
\end{prop}

\begin{proof}
Assume, for contradiction, that there is another additive identity,
call it $0'$.  By definition of ``additive identity'', we would then
have
\[ 0 \ = \ 0 + 0' \ = \ 0' \]
Thus, all additive identities are equal, meaning that there is only
one.  \qed
\end{proof}

\medskip

\noindent {\em The special role of $1$ under addition and subtraction.}
For any integer $a$, there is no integer between $a$ and $a+1$ nor
between $a-1$ and $a$.  For this reason, on the sets $\Z$ and $\N$,
one often singles out the following special cases of addition and
subtraction, especially in reasoning about situations that are indexed
by integers.  Strangely, these operations have no universally accepted
notations.
\begin{itemize}
\item
The {\it successor} operation\index{arithmetic!integers!successor} is
a {\em total function} on both $\N$ and $\Z$, which replaces an
integer $a$ by the integer $a+1$.
\item
The {\it predecessor} operation\index{arithmetic!integers!predecessor}
is a {\em total function} on $\Z$, which replaces an integer $a$ by
the integer $a-1$.  It is a {\em partial function} on $\N$, which is
defined only when the argument $a$ is positive (so that $a-1 \in \N$).
\end{itemize}

The operations of addition and subtraction are {\em mutually inverse
  operations}, in the sense of the following definition.  Functions
$f$ is a {\it functional inverse}\index{functional inverse} of
function $g$ if for all arguments $x$,
\begin{equation}
\label{eq:functional-inverse}
f(g(x)) \ = \ x
\end{equation}
In suggestive terminology, applying function $f$ ``undoes'' the result
of applying function $g$.  Within the current context, each of
addition and subtraction is a functional inverse of the other, so that
equation (\ref{eq:functional-inverse}) takes the following dual form.
\index{arithmetic!integers!additive inverse} 
\index{arithmetic!integers!addition and subtraction are mutually inverse}
\[
a \ = \ (a+b) -b \ = \ (a-b) +b
\]

\subsubsection{Multiplication and Division}
\index{arithmetic!basic operations!multiplication}
\index{arithmetic!basic operations!division}

The operation of {\it multiplication}\index{arithmetic!multiplication}
is a {\em total function} that replaces any two numbers $a$ and $b$ by
a number of the same sort.  The resulting number is the {\em product
  of $a$ and $b$}\index{arithmetic!multiplication!product} and is
denoted either $a \cdot b$ \index{arithmetic!multiplication!$a \cdot  b$}
or $a \times b$.\index{arithmetic!multiplication!$a \times b$}
We shall usually favor the former notation, except when the latter
enhances legibility.

The operation of {\it division}\index{arithmetic!division} is a {\em
  partial function} on all of our sets of numbers.  Given two numbers
$a$ and $b$, the result of dividing $a$ by $b$---{\em when that result
  is defined}---is the {\it quotient of $a$ by $b$;}
\index{arithmetic!division!When is $a/b$ defined?}
\index{arithmetic!division!quotient}
\index{arithmetic!division!quotient!$a/b$}
\index{arithmetic!division!quotient!$a \div b$}
\index{arithmetic!division!quotient!${a \over b}$}
it is denoted by one of the following three notations:
\[  a/b, \ \ \ a \div b, \ \ \ {a \over b}  \]
The {\it quotient of $a$ by $b$} is defined precisely when {\em both}

\noindent
\hspace*{.35in}(1) $b \neq 0$: one can never divide by $0$ \\
\hspace*{.35in}{\em and} \\
\hspace*{.35in}(2) there exists a number $c$ such that $a = b \cdot c$.

\noindent
Assuming that condition (1) holds, {\em condition (2) always holds
  when $a$ and $b$ belong to $\Q$ or $\R$ or $\C$}.

Division can also be defined as follows.  For any two numbers $a$
and $b$, {\em the quotient of $a$ and $b$ is the product of $a$ and the
reciprocal of $b$} (assuming that the latter exists):
\[ a/b \ = \ a \times (1/b). \]
Computing reciprocals of nonzero numbers in $\Q$ and $\R$ is standard
high-school level fare; computing reciprocals of nonzero numbers in
$\C$ requires a bit of calculational algebra which we do not cover.
For completeness, we note that the reciprocal of the {\em nonzero}
complex number $a + bi \in \C$ is the complex number $c+di$ where
\[ c \ = \ \frac{a}{a^2 + b^2} \ \ \ \ \
\mbox{ and } \ \ \ \ \
d \ = \ \frac{-b}{a^2 + b^2}.
\]

\medskip

\noindent {\em The special role of $1$ under multiplication and division.}
%
The number $1$ is the {\it identity} under the operations of
multiplication and division;\index{number!multiplicative identity}
\index{number!identity under multiplication}\index{identity!multiplicative}
terminologically, $1$ is a {\it multiplicative identity}.  This means
that, for all numbers $a$,
\[ a \cdot 1 \ = \ a \cdot (1/1) \ = \ a. \]
One shows as follows that $1$ is {\em the} multiplicative identity.

\begin{prop}
\label{thm:unique-mult-iden}
The number $1$ is the unique multiplicative identity.
\end{prop}

\begin{proof}
Assume, for contradiction, that there is another multiplicative
identity, call it $1'$.  By definition of ``multiplicative identity'',
we would then have
\[ 1 \ = \ 1 \times 1' \ = \ 1' \]
Thus, all multiplicative identities are equal.  \qed
\end{proof}

\medskip

\noindent {\em The special role of $0$ under multiplication and division.}
%
The number $0$ is the {\it annihilator} under
multiplication.\index{multiplicative annihilator} This means that, for
all numbers $a$
\[ a \times 0 \ = \ 0. \]
Using reasoning paralleling that of
Propositions~\ref{thm:unique-mult-iden}
and~\ref{thm:unique-mult-iden}, the reader can prove that $0$ is {\em
  the unique} multiplicative annihilator.

\begin{prop}
The number $0$ is the unique multiplicative annihilator.
\end{prop}

\medskip

The operations of multiplication and division are mutually inverse
operations,
\index{arithmetic!integers!multiplicative inverse}
\index{arithmetic!integers!multiplication and division are mutually inverse}
in the sense of equation (\ref{eq:functional-inverse}): each can be
used to ``undo'' the other.
\[ a = (a \times b) \div b \ = \ (a \div b) \times b  \]

\subsubsection{Exponentiation and Taking Logarithms}
\label{sec:exponentiation}

A conceptually powerful notational construct is the operation of {\it
  exponentiation}, \index{arithmetic!basic operations!exponentiation}
i.e., {\it raising a number to a power}.
\index{arithmetic!raising a number to a power}
For real numbers $a$ and $b$, the {\it $b$th power} of $a$, denoted
$a^b$ is defined by the system of equations
\begin{equation}
\label{eq:power-def}
\begin{array}{llll}
\mbox{for all numbers $a>0$} & & & a^0 = 1 \\
 & & & \\
\mbox{for all numbers $a, b, c$} & & & a^b \cdot a^c = a^{b+c}.
\end{array}
\end{equation}
This deceptively simple definition has myriad consequences which we
often take for granted.
\begin{itemize}
\item
{\em Multiplication of exponentials is accomplished via addition
  within the exponents.}

\item
{\em For all numbers $a>0$, the number $a^0 = 1$.}

This follows (via cancellation) from system (\ref{eq:power-def}) via
the fact that
\[ a^b \cdot a^0 \ = \ a^{b+0} \ = \ a^b \ = \ a^b \cdot 1.  \]

\item
{\em For all numbers $a >0$, the number $a^{1/2}$
\index{$a^{1/2}$: the square root of number $a$}
is the {\it square root} of $a$;\index{square root} i.e., $a^{1/2}$ is
the (unique, via cancellation) number $b$ such that $b^2 = a$.}
(Another common notation for the number $a^{1/2}$ is
$\sqrt{a}$.)\index{$\sqrt{a}$: the square root of number $a$}

This follows from system (\ref{eq:power-def}) via the fact that
\[ a \ = \ a^1 \ = \ a^{(1/2) + (1/2)} \ = \ a^{1/2} \cdot a^{1/2} \ = \
\left(a^{1/2}\right)^2. \]

\item
{\em For all numbers $a>0$ and $b$, the number $a^{-b}$ is the {\it
    multiplicative inverse} of $a^b$, meaning that $a^b \times a^{-b}
  = 1$}

This follows from system (\ref{eq:power-def}) via the fact that
\[ a^b \cdot a^{-b} \ = \ a^{(b + (-b))} \ = \ a^0 \ = \  1 \]
\end{itemize}
When the power $b$ is a positive integer, then definition
(\ref{eq:power-def}) can be cast in the following attractive inductive
form:
\begin{equation}
\label{eq:power-def-integer}
\begin{array}{llll}
\mbox{for all numbers $a>0$} & & & a^0 = 1 \\
 & & & \\
\mbox{for all numbers $a$ and integers $b$} & & & a^{b+1} = a \times
a^b.
\end{array}
\end{equation}

\medskip

We are now prepared to manipulate and employ powers that are integral
or fractional, positive, zero, or negative.

\bigskip

Just as addition has its inverse operation, subtraction, and
multiplication has its inverse operation, division, the operation of
exponentiation has its inverse operation, {\it taking logarithms}.
Our main discussion of logarithms appears in
Section~\ref{sec:logarithmic-fns}; we just define the operation here.

For positive real numbers $a$ and $b$, the operation of taking
logarithms is defined by the following dual equation.
\index{arithmetic!basic operations!taking logarithms}
\[ \log_b (b^a) \ = \ b^{\log_b a} \ = \ a \]
We shall note in Section~\ref{sec:logarithmic-fns} that most of the
main properties of this operation can be inferred from properties of
the operation of exponentiation.

\bigskip

\noindent \fbox{
\begin{minipage}{0.95\textwidth}
We have remarked in several places about the importance of noticing
patterns as we ``do'' mathematics.  A pattern that leads to some
beneficial insights has emerged in the current section.  The pattern
suggests that in certain senses:
\begin{itemize}
\item
The operations of addition, multiplication, and exponentiation
``behave'' similarly to one another.
\item
The operations of subtraction, division, and taking logarithms
``behave'' similarly to one another.
\item
The operations addition and subtraction relate to one another \\
\hspace*{.2in}in much the same way as \\
the operations multiplication and division relate to one another \\
\hspace*{.2in}and in much the same way as \\
the operations exponentiation and taking logarithms relate to one
another.
\end{itemize}
This pattern is ``encoded'' by the following suggestive figure.
\[ 
\begin{array}{|ccccc|}
\hline
\mbox{addition}
  & \approx &
\mbox{multiplication}
  & \approx &
\mbox{exponentiation} \\
\updownarrow & &
\updownarrow & &
\updownarrow \\
\mbox{subtraction}
  & \approx &
\mbox{division}
  & \approx &
\mbox{taking logarithms} \\
\hline
\end{array}
\]

Of course, patterns are {\em de}scriptive rather than {\em
  pre}scriptive, so one must use them only as hints to be followed,
not as facts to be depended on.
\end{minipage}
}
\bigskip


\subsubsection{Binomial coefficients and Pascal's triangle}
\label{sec:binomial-coeff}
\index{arithmetic!basic operations!binomial coefficient}

We close our catalogue of arithmetic operations with a binary
operation on the set $\N \times \N$\footnote{In advanced contexts, one
  encounters binomial coefficients with non-integer arguments.}~known
as the {\it binomial coefficient}.  The versatility of this
operation---and the origin of the unexpected word ``coefficient'' in
its name---will become clear throughout our text.  We now lay the
groundwork for the many applications of binomial coefficients.

%in Section~\ref{sec:Binomial-thm} and in
%Chapters~\ref{ch:Summation},~\ref{ch:Recurrences},
%and~\ref{ch:prob-stat}.

\medskip

Let $n$ and $k \leq n$ be nonnegative integers (i.e., elements of
$\N$).  The {\it binomial coefficient} usually articulated as ``{\it
  $n$ choose $k$}'',
\index{$n$ choose $k$: the binomial coefficient $\displaystyle {n \choose k}$}
and usually denoted either as $\displaystyle {n \choose k}$ or as
$\Delta_{n,k}$, is the number \index{binomial coefficient}
\begin{equation}
\label{eq:binom-coeff}
\Delta_{n,k} \ = \
{n \choose k} \ \eqdef \ \frac{n!}{k!(n-k)!} \ = \
\frac{n(n-1)(n-2) \cdots (n-k+1)}{k (k-1)(k-2) \cdots 1}
\end{equation}
Many of the secrets of these wonderful numbers---including the fact
that they are {\em integers}---can be deduced from the following
results.

\begin{prop}
\label{thm:manipulate-binom-coeff}
For all $n, k \in \N$ with $k \leq n$:

{\rm (a)} The symmetry rule:
\index{binomial coefficients!symmetry rule}
\begin{equation}
\label{eq:symmetry-binom-coeff}
{n \choose k} \ = \ {n \choose {n-k}}
\end{equation}

{\rm (b)} The addition rule:
\index{binomial coefficients!addition rule}
\begin{equation}
\label{eq:add-binom-coeff}
{n \choose k} \ + \ {n \choose {k+1}} \ = \ {{n+1} \choose {k+1}}
\end{equation}
\end{prop}

\begin{proof}
($a$)
We verify equation (\ref{eq:symmetry-binom-coeff}) by invoking the
defining equation (\ref{eq:binom-coeff}) plus the commutativity of
multiplication (see Section~\ref{sec:Arithmetic-Laws}):
\begin{eqnarray*}
{n \choose k} & = & \frac{n!}{k!(n-k)!} \\
              & = & \frac{n!}{(n-k)!k!} \\
              & = & {n \choose {n-k}}
\end{eqnarray*}

\noindent ($b$)
We verify equation (\ref{eq:add-binom-coeff}) by explicitly adding the
fractions exposed by equation (\ref{eq:binom-coeff}):
\begin{eqnarray*}
{n \choose k} \ + \ {n \choose {k+1}}
  & = &
\frac{n!}{k!(n-k)!} \ + \ \frac{n!}{(k+1)!(n-k-1)!} \\
  & = &
n! \cdot \frac{(k+1) + (n-k)} {(k+1)!(n-k)!} \\
  & = & 
\frac{(n+1)!}{(k+1)!(n-k)!} \\
  & = &
{{n+1} \choose {k+1}}
\end{eqnarray*}
We shall encounter a rather different verification of the addition
rule in Chapter~\ref{ch:prob-stat}, which exploits a central combinatorial
aspect of binomial coefficients: $\displaystyle {n \choose k}$ is the
number of ways of picking $k$ objects out of a set of $n$ objects.
  \qed
\end{proof}

\ignore{*************

{\Denis I added the alternative proof here, may be it is better to move it in chapter PROBA?
What do you think?}
Here is another simple proof using a combinatorial argument.

${{n+1} \choose {k+1}}$ is the way to choose $k+1$ items among $n+1$.

Let consider the set of the $n+1$ first integers.
When we are looking for an element, say $x$, there are two complementary situations:
$x$ is chosen or not.

If it is chosen, it remains to choose $k$ integers among the $n$ remaining one to make the $k+1$.
If it is not, there are $k+1$ to choose among the $n$ remaining ones.

Summing up both cases leads to the results.

\qed
\end{proof}

\bigskip

{\Denis I added the following property, may be a little harder but I think very relevant...}
There is another interesting property that was proposed by Leonard Euler using an algebraic approach.

\begin{prop}
\label{thm:algebraicApproachExponential}

Algebraic approach of the binomial coefficients.
\begin{equation}
(1+x)^n \ = \ \sum {n \choose k} x^k
\end{equation}

Corollary: $2^n = \sum {n \choose k}$

\end{prop}

{\Denis see the coherency with section 5.3.3}

\begin{proof}
write $(1+x)^n$ as the $n$ products,
each term of this product can be interpreted as choose $1$ or choose $x$,
this way, we obtain the coefficient of $x^k$...
\qed
\end{proof}
****************}

\bigskip

We shall have a lot to say about binomial coefficients throughout the
text.
\begin{itemize}
\item
In Section~\ref{sec:polynomials}, binomial coefficients take center
stage in the proof of Newton's {\it Binomial Theorem}; the name
``binomial coefficient'' emerges from this use of the operation.
\item
In Section~\ref{sec:arithmetic-series}, binomial coefficients play a
prominent role in evaluating arithmetic summations.
\item
In Section~\ref{sec:bilinear-recurrences}, binomial coefficients
provide an important example of {\em bilinear} recurrences and their
computational importance.
\item
In Chapter~\ref{ch:prob-stat}, we see how binomial coefficients are
indispensable when studying myriad topics related to {\em counting};
some examples:
  \begin{itemize}
  \item
What are the relative likelihoods of various $5$-card deals from a
fair $52$-card deck?
  \item
What is the likelihood of observing $15$ {\sc head}s and $25$ {\sc
  tail}s in $40$ flips of a fair coin?
  \item
What are the comparative operation-count costs of Merge-Sort and
Quick-Sort when sorting $n$ keys; cf.~\cite{CLRS}?
  \end{itemize}
\end{itemize}


\subsection{Rational arithmetic: A specialized computational exercise}
\label{sec:Rational-arithmetic}
\index{number!rational!arithmetic}

In Section~\ref{sec:rationals} we defined the rational numbers and
reviewed why they were needed to compensate for the general lack of
multiplicative inverses for integers within the set $\Z$.  But we did
not review how to perform arithmetic on the elements of the set $\Q$.
We make up for this shortcoming now.  Of course, the reader will have
encountered arithmetic on rationals long ago---but we are now
reviewing the topic for two reasons.  First, we want to reinforce the
systematic nature of the progression from integers and their
arithmetic to rationals and their arithmetic.  Second, we want to
provide the reader with a set of valuable exercises to reinforce the
mathematical thinking skills whose presentation is our main goal.

\medskip

The basic arithmetic operations on the rational numbers obey rules
that are adapted from the corresponding rules for integers.  For all
$p/q$ and $r/s$ in $\Q$, these rules take the following form:
\[
\begin{array}{llcccl}
\mbox{\small\sf Addition:} & 
{\displaystyle
{p \over q} + {r \over s} }
  & & = & &
{\displaystyle
 \frac{p \cdot s + r \cdot q}{q \cdot s} }  \\
 & & & & & \\
\mbox{\small\sf Subtraction:} &
{\displaystyle
{p \over q} + {r \over s} }
  & & = & &
{\displaystyle
{p \over q} + {(-r) \over s} } \\
 & & & & & \\
\mbox{\small\sf Multiplication:} &
{\displaystyle
{p \over q} \cdot {r \over s} }
  & & = & &
{\displaystyle
\frac{p \cdot r}{r \cdot s} } \\
  & & & & & \\
\mbox{\small\sf Division:} &
{\displaystyle
{p \over q} \div {r \over s} }
  & & = & &
{\displaystyle
{p \over q} \cdot {s \over r} }
\end{array}
\]

An essential component of the culture of mathematics is to ensure that
these definitions do supply an orderly extension of the corresponding
rules for integer arithmetic, as expounded in
Section~\ref{sec:Arithmetic-Laws}.  We leave to the reader the
valuable exercise of verifying, in particular, that rational
arithmetic:
\begin{itemize}
\item
works correctly when the argument rational numbers are, in fact,
integers, i.e., when $q = s = 1$ in the preceding table;
\item
treats the number $0$ appropriately, i.e., as an additive identity and
a multiplicative annihilator;
\item
treats the number $1$ appropriately, i.e., as a multiplicative identity;
\item
obeys the laws outlined in Section~\ref{sec:Arithmetic-Laws}.

Verifying the distributivity of rational multiplication over rational
addition is an especially valuable exercise because of the required
amount of manipulation.
\end{itemize}


\section{The Laws of Arithmetic, with Applications}
\label{sec:Arithmetic-Laws}
\index{arithmetic!basic laws}

This section is devoted to enumerating the basic laws of arithmetic on
the reals, rationals, and reals.  Anyone seeking to ``do mathematics''
should be able to employ these laws cogently in rigorous argumentation.

\subsection{The Commutative, Associative, and Distributive Laws} 
\index{commutative law!arithmetic}
\index{commutative law!addition}
\index{commutative law!multiplication}
\index{arithmetic!commutative law}

\noindent {\it (i) The commutative law.}
For all numbers $x$ and $y$:
\[
\begin{array}{llc}
\mbox{\it for addition:}
  & & x+y \ = \ y+x  \index{commutativity of addition} \\
\mbox{\it for multiplication:}
  & & x \times y \ = \ y \times x \index{commutativity of multiplication}
\end{array}
\]
Commutativity of addition (resp., of multiplication) allows one to add
(resp., to multiply) strings of numbers in any order. 

\medskip

\noindent {\it (ii) The associative law.}
\index{associative law for arithmetic}
\index{arithmetic!associative law}
For all numbers $x$, $y$, and $z$:
\[
\begin{array}{llc}
\mbox{\it for addition:}
  & &
(x+y)+z \ = \ x+(y+z) \\
\mbox{\it for multiplication:}
  & & 
(x \times y) \times z \ = \ x \times (y \times z).
\end{array}
\] 
Associativity of addition (resp., of multiplication) allows one to
write strings of additions (resp., of multiplications) without using
parentheses for grouping.

\medskip

\noindent {\it (iii) The distributive law.}
\index{distributive law for arithmetic}
\index{arithmetic!distributive law}
For all numbers $x$, $y$, and $z$,
\begin{equation}
\label{eq:distr-law}
x \times (y + z) \ = \ (x \times y) + (x \times z).
\end{equation}
One commonly articulates this law as, ``{\em Multiplication
  distributes over addition.}''

\smallskip

One of the most common uses of the distributive law reads equation
(\ref{eq:distr-law}) ``backwards,'' thereby deriving a formula for
{\em factoring} \index{arithmetic!factoring} complex expressions that
involve both addition and multiplication.

Easily, addition does {\em not} distribute over multiplication; i.e.,
in general, $x + y \cdot z \ \neq \ (x+y) \cdot (x+z)$.  Hence, when
we see the expression
\[ x + y \cdot z, \]
we know that the multiplication is performed before the addition.  In
other words,

 {\em Multiplication takes priority over addition.}
\index{arithmetic!priority of multiplication over addition}

\noindent This priority permits us to write the righthand side of
equation (\ref{eq:distr-law}) without parentheses, as in
\[ x \cdot (y + z) \ = \ x \cdot y + x \cdot z. \]

\medskip

By invoking the preceding laws multiple times, we can derive a recipe
for multiplying complicated arithmetic expressions.  We illustrate
this via the ``simplest'' complicated expression, $(a+b) \cdot (c+d)$.

\begin{prop}
\label{prop:(a+b)(c+d)}
For all numbers $a, b, c, d$:\footnote{Because multiplication takes
  priority over addition, the absence of parentheses in the righthand
  side of equation (\ref{eq:(a+b)(c+d)}) does not jeopardize
  unambiguity.}
\begin{equation}
\label{eq:(a+b)(c+d)}
(a+b) \cdot (c+d) \ = \ a \cdot c + a \cdot d + b \cdot c + b \cdot d
\end{equation}
\end{prop}

\begin{proof}
We perform a sequence of operations on the lefthand expression in
equation (\ref{eq:(a+b)(c+d)}), justifying each by an invocation of
the commutative and associative laws.
\[
\begin{array}{lclll}
(a+b) \cdot (c+d) & = & (a+b) \cdot c \ + \ (a+b) \cdot d
& & \mbox{distributive law} \\ 
  & = & c \cdot (a+b) \ + \ d \cdot (a+b)
& & \mbox{commutativity of multiplication} \ (2 \times) \\
  & = & c \cdot a + c \cdot b + d \cdot a + d \cdot b 
& & \mbox{distributive law} \ (2 \times) \\
  & = & a \cdot c + b \cdot c + a \cdot d + b \cdot d
& & \mbox{commutativity of multiplication} \ (4 \times) \\
  & = &  a \cdot c + a \cdot d + b \cdot c + b \cdot d
& & \mbox{commutativity of addition}
\end{array}
\]
We thus finally derive the righthand expression in equation
(\ref{eq:(a+b)(c+d)}).  \qed
\end{proof}


We close our short survey of the laws of arithmetic with the following
important two-part law.
\begin{itemize}
\item
{\it The law of inverses}.\index{inverse laws for
  arithmetic}\index{laws of arithmetic!inverse laws}
%
  \begin{itemize}
  \item
Every number $x$ has an {\em additive inverse},\index{additive inverse}
i.e., a number $y$ such that $x+y =0$.  This inverse is $x$'s {\it
  negative}, $-x$.\index{additive inverse!negative as additive inverse}
  \item
Every {\em nonzero} number $x \neq 0$ has a {\em multiplicative
  inverse},\index{multiplicative inverse} i.e., a number $y$ such that
$x \cdot y = 1$.  This inverse is $x$'s {\it reciprocal},
$1/x$.\index{multiplicative inverse!reciprocal as multiplicative inverse}
  \end{itemize}
\end{itemize}

\ignore{******************

\subsection{A Fun Result: A ``Trick'' for Squaring Certain Integers}

Sometimes only basic knowledge is needed to craft amusing
``tricks''---we know that they are not really tricks at all!---that
are really rigorous applications of principles that we have learned.
Here is an ``old chestnut'' example that may inspire you to design
your own. 

If someone presents you with a number that has a numeral that ends in
$5$, then there is a simple way to square the number mentally.  For
instance, if someone says

\hspace{.25in}``$n = 25$''

\noindent
then you can instantly respond

\hspace{.25in}``$n^2 = 625$''

\noindent
If the challenge is

\hspace{.25in}``$n = 75$''

\noindent
then your response is

\hspace{.25in}``$n^2 = 5625$''

\noindent
Let's make this ``game'' mathematical.

\begin{prop}
\label{thm:75x65=4925}
Let $n$ be any number that has a $2$-digit decimal numeral of the form

\hspace{.25in}$\delta \ 5$ \ \ \ \ $(\delta \in \{ 0,1,2,3,4,5,6,7,8,9\})$.

\noindent
Then the square of $n$ is the integer

\hspace{.25in}$25 \ + \ \delta \cdot (\delta +1)$. 
\end{prop}

\begin{proof}
We can rewrite the premise of the proposition in the form
\[ n \ = \ 10 \cdot \delta + 5 \]
It is now easy to invoke Proposition~\ref{prop:(a+b)(c+d)} and the
distributive law to compute that

\[ n^2 \ = \ 100 \cdot \delta \cdot (\delta+1) + 25. \]
To wit: 
\[
\begin{array}{lclll}
n^2 & = & (10 \cdot \delta + 5)^2 & & \mbox{Given} \\
    & = & 100 \cdot \delta^2 \ + \ 100 \cdot delta \ + \ 25
              & & \mbox{the proposition} \\
    & = & 100 \cdot (\delta^2 \ + \ \delta) \ + \ 25
              & & \mbox{factoring: distributive law} \\
    & = & 100 \cdot \delta \cdot (\delta + 1) \ + \ 25
              & & \mbox{factoring: distributive law} \\
\end{array}
\]
A parlor trick has become a mathematical demonstration!
\qed
\end{proof}
**********************}


\section{Polynomials and Their Roots}
\label{sec:polynomials}

Among the most basic functions are {\it polynomials}.
\index{algebraic function!polynomial}
\index{function!algebraic!polynomial} \index{polynomial}
A polynomial on the $k$ {\it variables} \index{polynomial!variables}
in the set $\{v_1, v_2, \ldots, v_k\}$ is any function that can be
written as a sum of {\it monomials}, \index{algebraic function!monomial}
\index{function!algebraic!monomial} \index{monomial}
each of the form
\begin{equation}
\label{eq:monomial}
a_{\langle c_1, i_1 \rangle, \langle c_2, i_2 \rangle, \ldots, 
\langle c_k, i_k \rangle} 
v_1^{c_1} \times v_2^{c_2} \times \cdots \times v_k^{c_k}
\end{equation}
In this expression: the subscripted symbols of the form $a_z$ denote
numbers that are called the {\it coefficients}
\index{polynomial!coefficients} \index{monomial!coefficient} of the
monomial or polynomial.  The symbols $c_i$ are {\em nonnegative
  integers} that are the {\it powers} \index{polynomial!powers}
\index{monomial!powers} to which the variables are raised.  Here, for
illustration, is a specific small polynomial, with integer
coefficients, on the variables $x_1$ and $x_2$.  (By convention a
variable does not appear when its power is $0$.)
\[ 3 x_1^7 \cdot x_2^{19} \ + \ x_1^3 \cdot x_2 \ + \ 45 x_2^{9} \ + \ 
7 x_1^{11} \cdot x_2^{5}.
\]
Another way to look polynomials is as the class of functions that are
formed from {\it variables} and {\it numbers} using the basic
algebraic operations: addition/subtraction and
multiplication/division.

\smallskip

Polynomials on a single variable---the case $k=1$---are said to be
{\it univariate} (a Latinate form of ``having a single variable'').
\index{polynomial!univariate} Polynomials on two variables---the case
$k=2$---are said to be {\it bivariate} (a Latinate form of ``having
two variables'').  These simplest polynomials are usually the only
ones graced by Latinate nicknames.  Our interest in this section will
mostly be on univariate polynomials
(Section~\ref{sec:univariate-polynomials}); we do spend some time on
one particular family of bivariate polynomials which has many
important applications (Section~\ref{sec:bivariate-polynomials}); and
we do describe informally an advanced result on general multivariate
polynomials which has tremendous importance to the foundations of
computing (Section~\ref{sec:Hilberts-Tenth}).

\bigskip

A problem of particular interest when discussing a polynomial $P(x_1,
x_2, \ldots, x_k)$ is to discover vectors of values of the variables
$\{x_i\}$ that cause $P$ to {\em vanish}, i.e., to evaluate to $0$.
Each such vector, $\langle r_1, r_2, \ldots, r_k)$ is called a {\it
  root} \index{polynomial!root} of $P$.  As a very simple example, the
roots of the polynomial
\[ P(x,y) \ \ = \ \ x^2 \ - \ 2xy \ + \ y^2 \]
are precisely the $2$-place vectors (i.e., ordered pairs)
$\langle k,k \rangle$, i.e., ordered pairs whose first and second
components are identical, $x=y$.

The problem of finding the roots of polynomials has garnered
tremendous attention for centuries, both for its practical
applications and its theoretical implications.  Historically, two of
the major problems regarding roots of polynomials are:
\begin{itemize}
\item
{\em Find all roots of a given polynomial.}

Most of the studies of this problem are found in algorithmic
settings---courses, books, software.  Yet, there are many valuable
mathematical lessons to be learned under the aegis of this problem.
We discuss this problem for univariate polynomials in
Section~\ref{sec:univariate-polynomials}.

\item
{\em Determine whether a given multivariate polynomial has any
integer roots.}

This problem is often found under the rubric of {\it Diophantine
  analysis}, so named in honor of the Greek mathematician Diophantus
of Alexandria, \index{Diophantus of Alexandria} who has been called
the father of algebra for his seminal studies of the process of
equation-solving.  By its very nature---seeking a ``{\sc yes}''-``{\sc
  no}'' answer rather than an actual root-finding solution---this
problem has been studied largely in theoretical or mathematical
settings.  While most work on this subject uses advanced techniques,
hence is beyond the scope of the current text, there is one result in
this domain whose underlying message is so profound that we at least
tell its ``story'' in Section~\ref{sec:Hilberts-Tenth}.
\end{itemize}

\bigskip

\index{algebraic function!polynomial}
\index{algebraic function} \index{function!algebraic}
Although our primary focus is on polynomials, our concern with their
roots forces us to expand our horizon to include functions that are
{\it algebraic}, in the sense of being solutions of equations of the
form $p(x) = 0$, where $p$ is a polynomial.  This expanded view
mandates that we be prepared to discuss and analyze ``polynomials''
within which variables are raised to fractional powers.


\subsection{Univariate Polynomials and Their Roots}
\label{sec:univariate-polynomials}
\index{polynomials!univariate}

We focus throughout this section on univariate polynomials with
complex coefficients.  We are concerned with using mathematics to
elucidate {\em the structure} of the roots of a given univariate
polynomial; we leave to our algorithmic siblings the practicalities of
actually computing the roots.  We discuss two topics which have strong
lessons for the endeavor of doing mathematics.  In
Section~\ref{sec:fund-thm-algebra}, we discuss univariate polynomials
of arbitrary (maximum) degree $d$.  We derive a number of results
about the $d$ complex roots that every degree-$d$ polynomial
has---highlighting the {\it Fundamental Theorem of Algebra}, the
storied result that verifies the existence of all these roots.  In
Section~\ref{sec:poly-by-radical}, we focus on the quest for
``simple'' formulas for roots, in terms of algebraic operations and
\index{radical} {\it radicals}---square roots, cube roots, etc.  (Of
course, ``radicals'' are just another framework for discussing
fractional powers.)  We will note that such formulas are readily
calculated for degree-$2$ ({\it quadratic}) polynomials,
\index{quadratic polynomials} \index{polynomials!univariate!quadratic}
arduously calculated for degree-$3$
({\it cubic})\index{cubic polynomials} \index{polynomials!univariate!cubic}
polynomials, computable only in principle for degree-$4$ ({\it
  quartic})\index{quartic polynomials} \index{polynomials!univariate!quartic}
polynomials---and {\em nonexistent} for polynomials of degree $5$
({\it quintic})\index{quintic polynomials}\index{polynomials!univariate!quintic}
and higher.

\subsubsection{The $d$ roots of a degree-$d$ polynomial}
\label{sec:fund-thm-algebra}

When we discussed the history of our number system, in
Section~\ref{sec:numbers}, we remarked that each step in the
progression from the integers ($\Z$) to the rational numbers ($\Q$),
to the real numbers ($\R$), and finally to the complex numbers ($\C$)
was motivated by a perceived deficiency in the number system up to
that point.  We then commented that the complex numbers were the
culmination of this process, in that they were {\it algebraically
  complete}. \index{algebraically complete}  We promised at that time
to clarify the meaning of that term---and the time has come to do
that.  The ``algebraic completeness'' of the complex numbers resides
in the fact that:

{\em Polynomials with complex coefficients can always be solved.}

\noindent
The common way of stating this powerful property is via the storied
theorem known as {\it The Fundamental Theorem of Algebra}.

\index{The Fundamental Theorem of Algebra}
\begin{theorem}[The Fundamental Theorem of Algebra]
\label{thm:fund-thm-algebra}
Every degree-$d$ univariate polynomial with complex coefficients has
$d$ complex roots.
\end{theorem}

Despite its rather simple form, Theorem~\ref{thm:fund-thm-algebra}
resisted formal proof for literally centuries, resisting the proof
attempts of mathematical luminaries such as Fermat, 
\index{Fermat, Pierre de} Euler, \index{Euler, Leonhard} Lagrange,
\index{Lagrange, Joseph-Louis} and Laplace.
\index{Laplace, Pierre-Simon (marquis de)}
The theorem was finally proved in the early 19th century by the
amateur (!)~French mathematician Jean-Robert Argand.
\index{Argand, Jean-Robert}
Argand publicized his proof in 1806 \cite{Argand}, but the proof
became well-known only when it appeared in the famous {\it Cours
  d'analyse} \cite{Cauchy21} of Augustin-Louis Cauchy,
\index{Cauchy, Augustin-Louis}
whose name we have encountered in earlier sections.

\medskip

A few cultural/historical comments are in order.
\begin{itemize}
\item
The many failed attempts at proving Theorem~\ref{thm:fund-thm-algebra}
should not be viewed as casting shadows over the luster of any of the
greats who failed.  In most of the cases, the failures pointed out new
mathematics that had yet to be developed.  Such is the trajectory of
mathematics and the sciences!
\item
Despite the word ``fundamental'' in the name of
Theorem~\ref{thm:fund-thm-algebra}, the result is no longer viewed as
{\em the} fundamental theorem of algebra.  The field of algebra has
grown in a variety of directions in the past two centuries, so the
{\em Theory of Equations}, which was largely coextensive with the
field of algebra into the 19th century is presently just one branch of
the field.
\item
Despite the word ``algebra'' in the name of
Theorem~\ref{thm:fund-thm-algebra}, there does not yet exist an {\em
  algebraic} proof of the theorem: Some input from some other area of
mathematics enters every known proof in some essential
way.\footnote{One of the authors reports learning a proof that
  borrowed results from topology during his undergraduate days.}
\item
The march toward Theorem~\ref{thm:fund-thm-algebra} took literally
millennia.  Among the luminaries who made major contributions along
the way were the following pioneers in the Theory of (Solving)
Equations:
  \begin{itemize}
  \item
{\it Diophantus of Alexandria}. \index{Diophantus of Alexandria} 
He authored a series of books known collectively as {\it Arithmetica},
\index{Diophantus of Alexandria!{\it Arithmetica}}
which expounded the basics of a theory of solving equations.  These
books earned Diophantus the name ``father of algebra''.
\index{Diophantus of Alexandria!father of algebra}
   \item
{\it Muhammad ibn Musa al-Khwarizmi}.
\index{Al-Khwarizmi!Muhammad ibn Musa al-Khwarizmi}
Known better as {\em Al-Khwarizmi}, this Persian mathematician and
scholar of the $9$th century lent his name to the modern term {\it
  algorithm}.\index{Al-Khwarizmi!eponym of ``algorithm''} His
extensive writings, especially the book \cite{Al-Khwarizmi},
introduced into Europe both the Hindu-Arabic numerals
\index{Al-Khwarizmi!Hindu-Arabic numerals} that we all use in everyday
discourse and the elements of what was known  of the field
of algebra in the $9$th century---particularly the solution of
equations.\footnote{The important role of the Middle East in the
  development of mathematics is testified to eloquently by the origin
  of the word ``algebra''.  According to the {\it Oxford English
    Dictionary}, this word comes from the Arabic ``{\it al-jabr}'',
  which literally means ``reunion of broken parts''---an allusion to
  the manipulation of terms in algebraic
  computations. \index{algebra!etymology of the word}}
   \item
{\it Ren\'{e} Descartes}. \index{Descartes, Ren\'{e}}
The $17$th-century mathematican and philosopher Descartes is credited
with establishing the algebraic notation that we use to this day.
The reader should not minimize the importance of notation until trying
to perform arithmetic using Roman numerals!  At a more abstract level,
Descartes's invention of {\em Analytical Geometry}
\index{Descartes, Ren\'{e}!inventor of Analytical Geometry}
enabled the use of geometric concepts and techniques in algebra.  Both
of these contributions were of incalculable value in the history
leading to Theorem~\ref{thm:fund-thm-algebra}.
   \end{itemize}
\end{itemize}

\medskip

The earliest proofs of Theorem~\ref{thm:fund-thm-algebra} were not
{\em constructive}---they did not provide a roadmap for actually
finding the roots of a given polynomial.  There currently do exist
proofs of the Theorem that are constructive, in the following sense.
Given a degree-$d$ polynomial $P(x)$, these proofs determine a disk in
two-dimensional space which contains all $d$ roots of $P(x)$.  How
might such a disk emerge from a mathematical argument?  Here is a {\em
  very simple} illustration which focuses on a {\em very special} type
of polynomial.

Let us be given a degree-$d$ polynomial with real coefficients:
\[ P(x) \ \ = \ \ a_d x^d \ + \ a_{d-1} x^{d-1} \ + \ a_{d-2} x^{d-2}
\ + \cdots + \ a_1 x \ + \ a_0
\]
Since we are interested only studying {\em the roots of} $P(x)$, we
lose no generality by insisting that $P(x)$ be {\em monic},
\index{polynomial!monic} i.e., have leading coefficient $a_d = 1$.
(The reader should verify this assertion!)  When $P(x)$ is a monic
polynomial, we can rewrite it in the following form:
\[ P(x) \ \ = \ \ 
x^d \cdot \left( 1 \ + \ \frac{a_{d-1}}{x} \ + \ \frac{a_{d-2}}{x^2}
\ + \cdots + \ \frac{a_1}{x^{d-1}} \ + \ \frac{a_0}{x^d} \right)
\]
The important benefit of this rewriting is that it makes the following
inequality totally clear.
\[ P(x) \ \ > \ \ 0 \ \ \ \ \mbox{ for all } \ \ \ x >
d \cdot \left(|a_{d-1}| \ + \ |a_{d-2}|
\ + \cdots + \ |a_1| \ + \ |a_0| \right)
\]
And, this inequality implies that all real roots of $P(x)$, if there
are any, lie within the region
\[ x \ \leq \ d \cdot \left(|a_{d-1}| \ + \ |a_{d-2}| \ + \cdots +
\ |a_1| \ + \ |a_0| \right) \]

\medskip

This is just one simple example, but it does illustrate that one can
sometimes delimit bounded regions within which all of $P(x)$'s roots
must lie.  Numerous texts (and research papers) deal with the general
problem of computing the roots of polynomials; see, e.g.,
\cite{MacDuffee}.


\subsubsection{Solving polynomials by radicals}
\label{sec:poly-by-radical}

The problem of {\it solving} arbitrary degree-$d$ polynomials---i.e.,
of discovering their $d$ roots, as promised by
Theorem~\ref{thm:fund-thm-algebra}---is computationally very complex,
even for moderately low degrees.  (This assertion can be made
mathematically precise, but the required notions are beyond the scope
of this text.)  For univariate polynomials of low degree, there do
exist computationally feasible root-finding algorithms.  Indeed, for
polynomials of {\em very} low degree---specifically, degrees $2$, $3$
and $4$---there actually exist ``simple'' {\em formulas} that specify
the polynomial's roots.  The word ``simple'' is used in a technical
sense here: it refers to a formula that can be constructed using the
following algebraic operations: adding/subtracting two quantities,
multiplying/dividing two quantities, and raising a quantity to a
rational power.  Because the last of these operations is often
expressed by using a {\em radical sign}, \index{radical sign} rather
than an exponent---as when we write $\sqrt{x}$ for
$x^{1/2}$---these formulas are often referred to as {\it solutions
  by radicals}.  \index{polynomial!solution by radicals}
\index{solution by radicals}  

The remainder of this section is devoted to deriving the {\em
  quadratic} formula---the one that specifies the roots of any
degree-$2$ {\it (quadratic)} \index{polynomials!quadratic}
polynomial---and the {\em cubic} formula---the one that specifies the
roots of any degree-$3$ {\it (cubic)} polynomial.
\index{polynomials!cubic} We shall observe that the cubic
formula is so onerous calculationally that it is seldom actually
written out; it is instead specified {\em algorithmically}.  The {\em
  quartic} formula---the one that specifies the roots of any
degree-$4$ {\it (quartic)} \index{polynomials!quartic} polynomial---is
so complex that it is virtually never written out.  The courageous
reader can attack the quartic formula as an exercise, using the
conceptual techniques which we derive here.

It is useless to try to solve polynomials of degrees $> 4$ by
radicals.  In the early 19th century, a (mathematically brilliant, for
sure!)~French teenager, Evariste Galois \index{Galois, Evariste},
proved that one cannot solve the degree-$5$ {\it (quintic)}
\index{polynomials!quintic} polynomial by radicals, no matter how much
abstruse computation one is willing to do!  Galois achieved this
result by developing a mathematical theory, which has since been named
for him. \index{Galois theory} Tragically for the world of
mathematics, Galois was killed in a duel just two years after
announcing his theory via a memoir submitted to the Paris Academy of
Sciences.

\bigskip

\noindent
On to solving low-degree polynomials:

\medskip

\paragraph{\small\sf A. Solving quadratic polynomials by radicals}

\noindent
We derive the {\it quadratic formula}, which solves an arbitrary
quadratic polynomial \index{Polynomial!quadratic!generic} with real
coefficients: \index{polynomial!solving a quadratic polynomial} 
\begin{equation}
\label{eq:generic-quadratic-1}
P(x) \ = \  ax^2 \ + \ bx \ + \ c \ \ \mbox{  where  } \ b,c \in \R;
\ a \in \R \setminus \{0\}
\end{equation}
While the formula and its derivation are specialized to the structure
of quadratic polynomials, several aspects of the derivation can be
extrapolated to polynomials of higher degree.  The formula that we
derive is announced in the following proposition.  \index{quadratic
  formula}

\begin{prop}
\label{thm:quadratic-formula}
The two roots, $x_1$ and $x_2$, of the generic quadratic polynomial
(\ref{eq:generic-quadratic-1}) are:
\begin{eqnarray}
\nonumber
x_1 & = & \frac{-b \ + \ \sqrt{b^2 -4ac}}{2a} \\
\label{eq:generic-quadratic-4}
    &   & \\
\nonumber
x_2 & = & \frac{-b \ - \ \sqrt{b^2 -4ac}}{2a}.
\end{eqnarray}
\end{prop}


\begin{proof}
We find the roots of $P(x)$ by solving the polynomial equation $P(x) =
0$.  We simplify our task by dividing both sides of this equation by
$a$; easily, this does not impact the two solutions we seek, since it
merely replaces $P(x)$ by a monic polynomial that shares the same
roots.  We thereby reduce the root-finding problem to the solution of
the equation
\begin{equation}
\label{eq:generic-quadratic-2}
x^2 \ + \ \frac{b}{a} x \ = \ - \frac{c}{a}.
\end{equation}
The technique of {\it completing the square}
\index{polynomial!quadratic!completing the square} gives us an easy
path to solving this equation.  This technique involves adding to both
sides of the equation a variable-free expression $E$ that turns the
lefthand expression into a perfect square.  In the case of equation
({eq:generic-quadratic-2}), the expression
\[ E \ = \ \frac{b^2}{4a^2} \]
does the job, because
\[
x^2 \ + \ \frac{b}{a} x \ + \ E \ \ = \ \
x^2 \ + \ \frac{b}{a} x \ + \ \frac{b^2}{4a^2}
   \ \ = \ \ \left( x \ + \ \frac{b}{2a} \right)^2
\]
We have thereby converted equation (\ref{eq:generic-quadratic-2}) to
the equation
\begin{equation}
\label{eq:generic-quadratic-3}
\left( x \ + \ \frac{b}{2a} \right)^2
% \ \ = \ \ \frac{b^2}{4a^2} - \frac{c}{a}
 \ \ = \ \ \frac{b^2 - 4ac}{4a^2}.
\end{equation}
Elementary calculation on equation (\ref{eq:generic-quadratic-3})
identifies $P(x)$'s two roots as the values $x_1$ and $x_2$ specified
in (\ref{eq:generic-quadratic-4}).  \qed
\end{proof}

\bigskip

\noindent
We close our treatment of quadratic polynomials with a few comments
that may inspire some readers to read further about roots of quadratic
polynomials.

{\bf 1.}
Using a common shorthand, the expressions for $x_1$ and $x_2$ in
(\ref{eq:generic-quadratic-4}) are often abbreviated by using the
operator $\pm$, which is a shorthand for ``plus or minus''.
\index{$\pm$: plus or minus} The quadratic formula can then be written
in the following familiar form:
\[
x \ = \  \frac{1}{2a} \left( -b \ \pm \ \sqrt{b^2 \ - \ 4ac} \right).
\]

\smallskip

{\bf 2.}
The reader can verify that our proof essentially proceeds by
replacing $P(x)$'s variable $x$ with the variable
\[ u \ = \ x + \frac{b}{2a}. \]
This replacement streamlines the process of completing the square and
finding the solutions $x_1$ and $x_2$.  We presented a more elementary
version of the proof so that the reader could watch the solution
process proceed step by step.  As we turn now to the clerically more
complex solution of the cubic polynomial, we get around some of the
calculational complexity by employing the variable-substitution
strategem.

\smallskip

{\bf 3.}
When we look carefully at the roots $x_1$ and $x_2$ in
(\ref{eq:generic-quadratic-4}), we note that there are three genres of
solutions.  These can be discriminated in the graphical form of
$P(x)$, as illustrated in Fig.~\ref{fig:SecondDegreeInit}.  (Note that
the {\em horizontal} displacement of $P(x)$'s plot is irrelevant; it
is only the {\em vertical} displacement that affects the nature of
$P(x)$'s roots.)
\begin{figure}[htb]
\begin{center}
       \includegraphics[scale=0.325]{FiguresArithmetic/SecondDegreeInit}
\caption{$P(x)$ has: (left) two real roots; (center) one real root;
  (right) no real root.}
\label{fig:SecondDegreeInit}
\end{center}
\end{figure}
\begin{itemize}
\item
The solutions $x_1$ and $x_2$ are distinct real numbers.

This corresponds to the lefthand plot in the figure: $P(x)$ crosses
the (real) $X$-axis at two distinct points.

\item
The solutions $x_1$ and $x_2$ are identical real numbers.

This corresponds to the central plot in the figure: $P(x)$
crosses/touches the (real) $X$-axis at precisely one point.

\item
The solutions $x_1$ and $x_2$ are distinct {\em non}-real numbers.

This corresponds to the righthand plot in the figure: $P(x)$ never
touches the (real) $X$-axis.
\end{itemize}

\ignore{***************
\begin{equation}
\label{eq:generic-quadratic-geo}
P(x) \ = \  ax^2 \ + \ bx \ + \ c \ \ \mbox{  where  } \ b,c \in \R;
\ a \in \R \setminus \{0\}
\end{equation}

Let consider that $a>0$, the case where $a<0$ is symmetric.
Fig.~\ref{fig:SecondDegreeInit} gives an illustration of the possible solutions.


In the case where there are two solutions, we first determine the coordinates of the minimum of $P(x)$.
This is obtained by a simple derivative whose solution is equal to~$x^*=\frac{-b}{2a}$,
the corresponding ordinate is:
\[
%\label{eq:generic-quadratic-1}
y^* \ = \ a.(\frac{-b}{2a})^2 \ + \ b.(\frac{-b}{2a}) \ + \ c \ = \ \frac{-b^2+ 4ac}{4a}
\]

Let $\Delta$ denote the discriminent $b^2 - 4ac$.

From Fig.~\ref{fig:SecondDegree}, the two roots we are looking for are symmetric in regard to $x^*=\frac{-b}{2a}$.
\begin{figure}[htb]
\begin{center}
       \includegraphics[scale=0.4]{FiguresArithmetic/SecondDegree}
\caption{Geometric interpretation of solving $P(x)=0$.}
\label{fig:SecondDegree}
\end{center}
\end{figure}
******************}


\bigskip

\paragraph{\small\sf B. Solving cubic polynomials by radicals}

We derive a formula for the roots of an arbitrary cubic polynomial
\index{Polynomial!cubic!generic} with real coefficients:
\index{polynomial!solving a cubic polynomial} 
\begin{equation}
\label{eq:generic-cubic-1}
P(x) \ = \  ax^3 \ + \ bx^2 \ + \ cx \ + \ d \ \ \mbox{  where  }
\ b,c, d  \in \R;\ a \in \R \setminus \{0\}
\end{equation}
Although the so-called {\em cubic formula} that we derive, and its
derivation, are specialized to the structure of degree-$3$
polynomials, the reader will recognize several aspects of the
derivation that are akin to our derivation of the quadratic formula.
Because the cubic formula is so complex in form, we present the
formula's proof/derivation {\em before} presenting the formula.

\begin{proof} {\it (Derivation of the cubic formula)}

\noindent {\it Step 1.} Convert $P(x)$ to a {\em monic}
cubic polynomial $P^{(1)}(x)$ which shares $P(x)$'s roots.

\noindent
This is accomplished via a change of coefficents that rewrites $P(x)$
as the monic cubic polynomial
\begin{equation}
\label{eq:generic-cubic-2}
P^{(1)}(x) \ = \  x^3 \ + \ Bx^2 \ + \ Cx \ + \ D
\end{equation}
The required change of coefficents is specified implicitly as follows
\[
B \ = \ \frac{b}{a}; \ \ \
C \ = \ \frac{c}{a}; \ \ \
D \ = \ \frac{d}{a}
\]

%It should be clear that the polynomials $P(x)$ and $P^{(1)}(x)$ have
%the same roots.  {\Arny We should include this as a simple exercise.}

\medskip

\noindent {\it Step 2.} Convert $P^{(1)}$ to a {\em reduced form}
\index{polynomial!cubic!monic!reduced form}
cubic polynomial $P^{(2)}$ which shares $P$'s roots.  By
``reduced'', we mean that $P^{(2)}$ has no quadratic term---i.e., no
term in which the variable is raised to the power $2$.

\noindent
This transformation is accomplished as follows.  We make the
transformation of variable
\begin{equation}
\label{eq:cubic-substitute-y-for-x} 
y \ = \ x \ + \ \frac{B}{3}
\end{equation}
in (\ref{eq:generic-cubic-2}).  We thereby convert $P^{(1)}(x)$, a
polynomial in variable $x$, into the following polynomial in variable
$y$:

\begin{eqnarray}
\nonumber
P^{(2)}(y) & = &  \left(y - \frac{B}{3} \right)^3
 \ + \ B \left(y - \frac{B}{3} \right)^2
 \ + \ C \left(y - \frac{B}{3} \right) \ + \ D \\
%\nonumber
%           & = &
%y^3 \ - \ B y^2 \ + \ \frac{B^2}{9} y \ - \ \frac{B^3}{27}
%\ + \ By^2 \ - \ \frac{2B^2}{3} y \ + \ \frac{B^3}{9} 
%\ + \ Cy \ - \ \frac{BC}{3}  \ + \ D \\
\label{eq:generic-cubic-3}
           & = &
y^3 \ + \
\left( \frac{B^2}{9} \ - \ \frac{2B^2}{3} \ + \ C  \right) y
\ + \ \left( \frac{2 B^3}{27}  \ - \ \frac{BC}{3}  \ + \ D \right)
\end{eqnarray}
For simplicity, we rewrite $P^{(2)}(y)$, which clearly is in reduced
form, as
\begin{eqnarray}
\label{eq:generic-cubic-4}
P^{(2)}(y) & = & y^3 \ + \ E y \ + \ F \\
\nonumber
\mbox{where} & & \\
\nonumber
E & = & \frac{B^2}{9} \ - \ \frac{2B^2}{3} \ + \ C \\
\nonumber
F & = & \frac{2 B^3}{27}  \ - \ \frac{BC}{3}  \ + \ D
\end{eqnarray}


\medskip

\noindent {\it Step 3.} Convert $P^{(2)}(y)$ to its {\em associated}
quadratic polynomial.

\noindent
We accomplish this step by applying a transformation attributed to the
$16$th-century French mathematician Fran\c{c}ois Vi\`{e}te
\index{Vi\`{e}te, Fran\c{c}ois} (often referred to by his Latinized
name, Franciscus Vieta) \index{Vieta, Franciscus}; see
\cite{Hazewinkel}.  Vieta's transformation converts $P^{(2)}(y)$ to a
quadratic polynomial by means of the variable-substitution
\begin{equation}
\label{eq:cubic-substitute-z-for-y}
y \ = \ z \ - \ \frac{E}{3z}
\end{equation}
in (\ref{eq:generic-cubic-4}).  We thereby obtain (after calculations
involving several cancellations) an expression
\begin{eqnarray}
\nonumber
P^{(3)}(z) & = & \left( z \ - \ \frac{E}{3z} \right)^3
\ + \ E \left(z \ - \ \frac{E}{3z} \right) \ + \ F \\
\label{eq:generic-cubic-5}
  & = &
z^3 \ - \ \frac{E^3}{27z^3}  \ + \ F
\end{eqnarray}
Clearly, $P^{(3)}(z)$ is not a polynomial in $z$---because of the term
in which variable $z$ appears in the denominator---but it is a
valuable stepping stone because the function $P^{(3)}(z)$
vanishes---i.e., $P^{(3)}(z) = 0$---precisely when the following
polynomial (in the ``variable'' $z^3$) vanishes.
\[ P^{(4)}(z^3) \ = \ (z^3)^2 \ + \ (z^3) F \ - \ \frac{E^3}{27}. \]
We wrote both instances of $z^3$ in the expression for $P^{(4)}(z^3)$
within parentheses to facilitate the view of $P^{(4)}(z^3)$ as a
(quadratic) polynomial in the ``variable'' $z^3$.  The quadratic
formula (\ref{eq:generic-quadratic-4}) provides us with two roots for
$P^{(4)}$, which we express as the following two values for $z^3$
(abbreviated via the shorthand operator $\pm$).
 \index{$\pm$: plus or minus}
\begin{equation}
\label{eq:solve-cubic-for-z3}
(z^3) \ = \
%\frac{1}{2} \left(-F \ \pm \ \sqrt{F^2 + 4 \frac{E^3}{27}} \right) \ = \ 
- \frac{F}{2} \ \pm \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\end{equation}

We can now derive all solutions for the variable $z$ in equation
(\ref{eq:solve-cubic-for-z3}) via back-substitution in transformation
(\ref{eq:cubic-substitute-z-for-y}).  But completing this derivation
requires a bit of background.

Theorem~\ref{thm:fund-thm-algebra} assures us that the polynomial
$P^{(2)}(z)$ has three roots.  In order to compute these roots, we
invoke a truly remarkable result that is known as \index{Euler's
  formula} {\it Euler's formula}, in honor of its discoverer, the
much-traveled $18$th-century mathematician Leonhard
Euler. \index{Euler, Leonhard} This result/formula exposes a
fundamental relationship among:
\begin{itemize}
\item
the imaginary unit \index{$i$: the imaginary unit} $i$
\item
the ratio of the circumference of a circle to its radius,
$\pi \ = \ 3.141592653 \cdots$
\index{$\pi$: the ratio of the circumference of a circle to its radius}
\item
the base of natural logarithms, Euler's constant $e \ = \ 2.718281828 \cdots$
\index{Euler's constant}\index{$e$:the base of natural logarithms}
\end{itemize}

\begin{theorem}[Euler's formula]
\label{thm:Euler's-formula}
\[ e^{i \pi} \ = \ -1. \]
\end{theorem}

Back to equation (\ref{eq:solve-cubic-for-z3}):
Theorem~\ref{thm:fund-thm-algebra} tells us that within the complex
number system $\C$, the cubic polynomial
\[ u^3 \ - \ 1 \]
has three distinct roots.  These numbers are known as the {\em
  primitive $3$rd roots of unity} \index{primitive $3$rd roots of
  unity} and are traditionally denoted $\omega^0$, $\omega^1$, and
$\omega^2$.  Using Theorem~\ref{thm:Euler's-formula}, we can provide
explicit values for these numbers, namely:
\[ \omega^0 \ = \ 1; \ \ \ \ \
\omega^1 \ = \ e^{2i \pi/3}; \ \ \ \ \
\omega^2 \ = \ e^{4i \pi/3}.
\]

\medskip
When we unite the abbreviated double equation
(\ref{eq:solve-cubic-for-z3}) for $z^3$ with Euler's formula
(Theorem~\ref{thm:Euler's-formula}), we discover {\em six} solutions
for the variable $z$, namely, {\small
\begin{equation}
\label{eq:cubic-solution-1}
\begin{array}{ccrrrrrccr}
z_1 & = &
{\displaystyle
\omega^0 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3} 
}
  & & & & &
z_2 & = &
{\displaystyle
\omega^0 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
} \\
z_3 & = &
{\displaystyle
\omega^1 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
  & & & & & 
z_4 & = &
{\displaystyle
\omega^1 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
} \\
z_5 & = &
{\displaystyle
\omega^2 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
  & & & & &
z_6 & = &
{\displaystyle
\omega^2 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
\end{array}
\end{equation}
}

The algorithmically interesting portion of the process of solving
cubics by radicals is now complete.  The remainder of the process
consists of ``reversing'' the two transformations,
(\ref{eq:cubic-substitute-z-for-y}) and
(\ref{eq:cubic-substitute-y-for-x}), that have taken us from the
problem of solving a polynomial in $x$ to the problem of solving a
polynomial in $z$.  The calculations that embody this reverse
transformation are onerous when solved symbolically, so we make do
with some exercises in which the reader will solve numerical
instances.  The most interesting and noteworthy feature of these
exercise will be the observation of ``collapsing'' of intermediate
expressions, whose impact is to leave us with with only {\em three}
solutions for $x$---which is the number promised by
Theorem~\ref{thm:fund-thm-algebra}---rather than the six solutions
that the array (\ref{eq:cubic-solution-1}) of $z$-values would lead
one to expect.

While the promise of a visually appealing cubic analogue of the
quadratic formula (\ref{eq:generic-quadratic-4}) is appealing, the
actual cubic formula is so complex visually that it offers no
important insights.  The curious reader can find renditions of the
formulas on the web.  \qed
\end{proof}

\subsection{Bivariate Polynomials: The Binomial Theorem}
\label{sec:bivariate-polynomials}
\label{sec:Binomial-thm}
\index{polynomials!bivariate}
\index{The Binomial Theorem}

A polynomial $P$ is {\em bivariate} if each of its summand
monomials---cf. (\ref{eq:monomial})---has the form
\[ a \cdot x^b \cdot y^c \]
In this expression: $x$ and $y$ are the monomial's {\em two} variables
(``two'' because the polynomial is {\em bivariate}); $a$ is the
monomial's coefficient; $b$ and $c$ are the respective powers of
variables $x$ and $y$ in this monomial.

Probably the simplest bivariate polynomials are the ones in the
following family.
\begin{equation}
\label{eq:binomial-polys}
\mbox{For } \ n \in \N^+, \ \ \
P_n(x,y) \ \eqdef \ (x+y)^n.
\end{equation}
There are lessons to be learned from manipulating the structure of
these polynomials, so let us begin to expand them using the arithmetic
techniques we have learned earlier.
\begin{eqnarray*}
P_1(x,y) \ = \
(x+y)^1 & = & x+y  \\
P_2(x,y) \ = \
(x+y)^2 & = & (x+y) \cdot (x+y) \\
        & = & x^2 + 2xy + y^2 \\
P_3(x,y) \ = \
(x+y)^3 & = & (x+y) \cdot (x^2 + 2xy + y^2) \\
   & = & (x^3 + 2x^2y +  xy^2) + (x^2y + 2xy^2 + y^3) \\
   & = & x^3 + 3x^2y + 3xy^2 + y^3  
\end{eqnarray*}

Let us stop to review what we are observing.  We have remarked before
that doing mathematics can sometimes involve a wonderfully
exciting---and quite sophisticated---pattern-matching game.  So, let
us pattern-match!
\begin{enumerate}
\item
The coefficients of the expanded $P_1(x,y)$ are $\langle 1,1 \rangle$.
\item
The coefficients of the expanded $P_2(x,y)$ are $\langle 1,2,1 \rangle$.
\item
The coefficients of the expanded $P_3(x,y)$ are $\langle 1,3,3,1 \rangle$.
\end{enumerate}
There is a pattern emerging here.  Can you spot it?  Where have we
seen a pattern of tuples that begins in the same manner?  As a rather
broad hint, look at Fig.~\ref{fig:pascal-triangle}!  Could the
coefficients of each $P_n$ possibly be the successive binomial
coefficients
\[ {n \choose 0}, \ \ {n \choose 1}, \ \ \ldots, \ \ {n \choose
  {n-1}}, \ \ {n \choose n}
\]
Let us use induction to explore this possibility by expanding a
generic $P_n$ with symbolic ``dummy'' coefficients and see what this
says about $P_{n+1}$.  To this end, let $a_{n,n-r}$ denote the
coefficient of $x^{n-r} y^r$ in the expansion of $P_n(x,y)$.  Using
our symbolic coefficients $a_{n,k}$, we have (omitting multiplication
symbols to conserve horizontal space):
\begin{eqnarray*}
P_n(x,y) & = &
 x^n \ + \ a_{n,n-1} x^{n-1} y \ + \cdots + \\
         &   & \hspace*{.15in}
a_{n,n-r} x^{n-r} y^r \ + \ a_{n,n-r-1} x^{n-r-1} y^{r+1}
\ + \ a_{n,n-r-2} x^{n-r-2} y^{r+2} \\
         &   & \hspace*{.15in}
+ \cdots + \ a_{n,1} x y^{n-1} \ + \ y^n
\end{eqnarray*}
Continuing with this symbolic evaluation, we have:
\begin{eqnarray}
\nonumber
x \cdot P_n(x,y) & = &
 x^{n+1} \ + \ a_{n,n-1} x^n y \ + \cdots + \\
\nonumber
         &   & \hspace*{.15in}
a_{n,n-r} x^{n-r+1} y^r \ + \ a_{n,n-r-1} x^{n-r} y^{r+1}
\ + \ a_{n,n-r-2} x^{n-r-1} y^{r+2} \\
\label{eq:xPk}
         &   & \hspace*{.15in}
+ \cdots + \ a_{n,1} x^2 y^{n-1} \ + \ x y^n
\end{eqnarray}
and
\begin{eqnarray}
\nonumber
y \cdot P_n(x,y) & = &
 x^n y \ + \ a_{n,n-1} x^n y^2 \ + \cdots + \\
\nonumber
         &   & \hspace*{.15in}
a_{n,n-r} x^{n-r} y^{r+1} \ + \ a_{n,n-r-1} x^{n-r-1} y^{r+2}
\ + \ a_{n,n-r-2} x^{n-r-2} y^{r+3} \\
\label{eq:yPk}
         &   & \hspace*{.15in}
+ \cdots  + \ a_{n,1} x^2 y^n \ + \ y^{n+1}
\end{eqnarray}
Because
\[ P_{n+1}(x+y) \ = \ (x+y) \cdot P_n(x,y)
                \ = \ x \cdot P_n(x,y) \ + \ y \cdot P_n(x,y),
\]
the symbolic coefficient $a_{n-r+1,r}$ of $x^{n-r+1} y^r$ in
$P_{n+1}(x+y)$ is the sum of the following symbolic coefficients in
$P_n(x,y)$:
\begin{center}
$\bullet$
the coefficient $a_{n,n-r}$ of $x^{n-r}y^r$ \ \ \ \ \
and \ \ \ \ \
$\bullet$
the coefficient $a_{n,n-r+1}$ of $x^{n-r+1}y^{r-1}$
\end{center}
By induction, then, for all $n,r \in \N$ with $r \leq n$,
\[ a_{n,r} + a_{n,r+1} \ = \ a_{n+1,r+1} \]
Combining this equation with the observed initial conditions
\[ a_{1,0} \ = \ a_{1,1} \ = \ 1 \]
we see that each coefficient $a_{n,r}$ is actually the binomial
coefficient $\displaystyle {n \choose r}$.

The preceding observation is attributed to the renowned English
mathematician/physicist Isaac Newton \index{Newton, Isaac} and is
enshrined in Newton's famous {\it Binomial Theorem}.  In fact, the
calculations preceding the observation constitute a proof of this
seminal result.

\ignore{**********
so that, finally,
\begin{eqnarray*}
             & = &
 x^{k+1} \ + \ (k+1) x^k y \ + \ \cdots \ + \
   (a_{k,k-r-1} + a_{k,k-r}) x^{k-r} y^{r+1} \\
             &   & \ \ \ + \
   (a_{k,k-r-2} + a_{k,k-r-1}) x^{k-r-1} y^{r+2}
 \ + \ \cdots \ + \ (k+1) xy^k \ + \ y^{k+1} \\
      & = &
 x^{k+1} \ + \ (k+1) x^k y \ + \ \cdots \ + \
 a_{k+1,k-r} x^{k-r} y^{r+1} \\
            &    & \ \ \ + \  a_{k+1,k-r-1} x^{k-r-1} y^{r+2}
 \ + \ \cdots \ + \ (k+1) xy^k \ + \ y^{k+1}
\end{eqnarray*}
*******}

\begin{theorem}[The Binomial Theorem]
\label{thm:Binomial-theorem}
For all $n \in \N$,
\begin{equation}
\label{eq:binomial-theorem}
(x+y)^n \ \ = \ \
\sum_{i=0}^n \ \ {n \choose i} x^{n-i} y^i.
\end{equation}
\end{theorem}
\index{The Binomial Theorem!binomial coefficients}
\index{binomial coefficients!The Binomial Theorem}
\index{polynomials!bivariate!The Binomial Theorem}


\subsection{$\oplus$ Integer Roots of Polynomials: Hilbert's $10$th Problem}
\label{sec:Hilberts-Tenth}
\index{Hilbert's Tenth Problem}

This section is devoted to describing a true milestone in the history
of mathematics, of logic, and of computing.  Even though the
mathematical details necessary to fully describe the analyses that
lead to this blockbuster result go way beyond the scope of an
introductory text, this result has cultural lessons that make even its
story valuable.  Moreover, once one {\em really} understands the
result, one recognizes that the material in
Sections~\ref{sec:Q-Z-R-cardinality} and~\ref{sec:pairing}---both of
which the reader {\em does} have background for, hence, access
to---supply essential mathematical underpinnings for the result.

In 1900, the influential German mathematician David Hilbert
\index{Hilbert, David} set forth $23$ problems to serve as a ``bucket
list''\footnote{The ``bucket'' is not yet empty.  A number of
  Hilbert's original $23$ are yet to be solved.}~for the world
mathematics community at the dawn of the twentieth
century.\footnote{Hilbert's list was published in 1902
  \cite{Hilbert02}; it was translated into English by Mary Frances
  Winston Newson. \index{Newson, Mary Frances Winston} Newson was the
  first American woman to receive a doctorate from the University of
  G\"{o}ttingen, which was probably the world's premier university for
  mathematical research in the late 19th and early 20th centuries.}
Of Hilbert's $23$ problems, the $10$th stands out within the world of
computing. \index{Hilbert's Tenth Problem} Stated in modern
terminology, with a modern perspective, the Problem can be stated as
follows.
\begin{description}
\item[{\bf Hilbert's Tenth Problem}]
{\it Develop an algorithm that will decide---via a {\sc yes} or {\sc
    no} answer---whether a given multivariate polynomial with integer
  coefficients has any integer roots.  }
\end{description}
This problem attracted the attentions of many of the best mathematical
minds of the $20$th century.  Building on the work of American
mathematicians Martin Davis \index{Davis, Martin} and Julia Robinson
\index{Robinson, Julia}, the Problem was finally resolved by the Russian
then-graduate student Yuri Matiyasevich \index{Matiyasevich, Yuri}.

\begin{theorem}[Hilbert's Tenth Problem, Resolved]
\label{thm:Hilberts-10th}
There is no algorithm that, when presented with an arbitrary
multivariate polynomial $P$ with integer coefficients, will correctly
determine whether $P$ has any integer roots.
\end{theorem}

The long history leading to Matiyasevich's proof of
Theorem~\ref{thm:Hilberts-10th} is documented in
\cite{Davis73,DavisH73,DavisMR76,Matiyasevich93}.  Although the
mathematics needed even to understand Hilbert's Tenth Problem and its
resolution are beyond an introductory text, a few words are in order
about a fundamental way in which 20th-century mathematics turned the
tables on all of mathematics to that point in history.

\medskip

Reaching back historically, it is worth pondering that---as we note in
Section~\ref{sec:classical-v-modern-proofs}---one of the main
movements in mathematics, beginning in the 19th century, had as its
goal to codify the notion of ``rigorous proof''.  Toward this end, one
of Hilbert's goals in 1900---indeed, his $10$th one---was to have
mathematicians rigorously prove that the notions of ``Truth'' and
``Provability'' coincided, at least in ``elementary'' areas of
mathematics.

\smallskip

This hope was, in fact, the first of Hilbert's dreams to be dashed: In
the early 1930s, Austrian expatriate, then Princeton professor, Kurt
G\"{o}del \index{G\"{o}del, Kurt} proved his celebrated {\it
  Incompleteness Theorem} \cite{Goedel31}.
\index{G\"{o}del's Incompleteness Theorems}
Stated {\em very(!)}~informally, this theorem asserts that in any
``reasonable'' mathematical system, there will always be {\sc true}
statements that cannot be proved.

\bigskip

\noindent \fbox{
\begin{minipage}{0.95\textwidth}
The crucial word ``reasonable'' in this statement means---again, very
informally---that the system is capable of asserting elementary facts
about triples, $x, y, z$, of positive integers:
\begin{itemize}
\item
One of the given integers, say $z$, is the sum of the other two ($z =
x+y$).
\item
One of the given integers, again say $z$, is the product
of the two others ($z = x \times y$).
\end{itemize}
\end{minipage}
}
\bigskip

\noindent
Building on G\"{o}del's insights, English mathematician Alan Turing
\index{Turing, Alan} soon thereafter proved---again, stated {\em
  very(!)}~informally---that no matter how much ``smarts'' we build
into a digital computer using digital logic, there will always be
functions---even $0$-$1$ valued functions---that the computer cannot
compute \cite{Turing36}.  Theorem~\ref{thm:Hilberts-10th} can be
reworded to assert that the integer-root-finding behavior called for
in Hilbert's Tenth Problem is one of Turing's {\it uncomputable}
\index{uncomputable function} functions!\footnote{The connections
  between G\"{o}del's results and Turing's, and the manner in which
  both authors' work connects to the work of German logician Georg
  Cantor \index{Cantor, Georg} on infinite sets of numbers,
  \cite{Cantor74,Cantor78} are described in \cite{Rosenberg09} using
  consistent terminology to discuss all three authors' work.}



\section{Exponential and Logarithmic Functions}
\label{sec:exponential+logarithm}

This section introduces the fundamentals of the important functions
that emerge from the operations of exponentiation and taking
logarithms.  These functions are mutually inverse, in the sense of
equation (\ref{eq:functional-inverse}).

\subsection{Basic Definitions}
\label{sec:exponential-function}


\index{exponential function}
\index{function!exponential}
A function $f$ is {\it exponential} if there is a positive number $b$
such that, for all $x$,
\begin{equation}
\label{eq:exponential-defn}
f(x) \ = \ b^x.
\end{equation}
The number $b$ is the {\it base}\index{base of exponential} of
$function f(x)$.  The basic arithmetic properties of exponential
functions are easily derived from the recurrent system of equations
(\ref{eq:power-def}).  The important message is that multiplication
``at the ground level'' is addition at the ``exponent level'':
\[ b^x \times b^y \ = \ b^{x+y} \]




{\Denis May be we should talk here about the exponentiation rapide (fast exponentiation?)}
\medskip
$x^n$ is obtained by a decomposition of $n$ into powers of $2$,
for instance, $x^10$ is $((x^{2})^2)^2.x^2$...

\index{logarithmic function}
\index{function!logarithmic}
Given an integer $b >1$ (mnemonic for ``base''), the {\em base-$b$
  logarithm}\index{base-$b$ logarithm}
%
of a real number $a > 0$ is denoted $\log_b a$ and defined by the
equation\index{$\log_b a$: the base-$b$ logarithm of number $a$}
\begin{equation}
\label{eq:logarithm-defn}
a \ = \ b^{\log_b a}.
\end{equation}
Logarithms are partial functions: $\log_b a$ is not defined for
non-positive arguments.

The base $b = 2$ is so prominent in the contexts of computation theory
and information theory that we commonly invoke one of two special
notations for $\log_2 a$: (1) we often elide the base-$2$ subscript
and write $\log a$;\index{$\log(a)$: base-$2$ logarithm of number $a$}
(2) we employ the specialized notation $\ln a$\index{$\ln(a)$:
  base-$2$ logarithm of number $a$}.  Notationally:
\[ \log_2 a \ \eqdef \ \log a \ \eqdef \ \ln a \]

We leave to the reader the easy verification, from
(\ref{eq:logarithm-defn}), that the {\it base-$b$ logarithmic
  function}, defined by
\begin{equation}
\label{eq:log-function-defn}
f(x) \ = \ \log_b x
\end{equation}
is the functional inverse of the base-$b$ exponential function.

\subsection{Learning from logarithms (and exponentials)}

Definition (\ref{eq:logarithm-defn}) exposes and---even more
importantly---explains myriad facts about logarithms that we often
take for granted.

\begin{prop}
For any base $b >1$, for all numbers $x >0$, $y>0$,
\[ \log_b (x \cdot y) \ = \ \log_b x \ + \ \log_b y \]
\end{prop}

\begin{proof}
Definition (\ref{eq:logarithm-defn}) tells us that $x = b^{\log_b x}$
and $y = b^{\log_b y}$.  Therefore,
\[ x \cdot y \ = \ b^{\log_b x} \cdot b^{\log_b y} \ = \
b^{\log_b x \ + \ \log_b y}, \]
by the laws of powers.  Taking base-$b$ logarithms of the first and
last terms in the chain yields the claimed equation.
\qed
\end{proof}



Many students believe that the following result is a {\em convention}
rather than a consequence of the basic definitions.  {\em The logarithm
  of $1$ to any base is $0$.}

\begin{prop}
For any base $b >1$,
\[ \log_b 1 \ = \ 0 \]
\end{prop}

\begin{proof}
We note the following chain of equalities.
\[  b^{\log_b x} \ = \ b^{\log_b (x \cdot 1)} 
\ = \ b^{(\log_b x) + (\log_b 1)} 
\ = \ b^{\log_b x} \cdot b^{\log_b 1}
\]
Hence, $b^{\log_b 1} \ = \ 1$.  If $\log_b 1$ did not equal $0$, then
$b^{\log_b 1}$ would exceed $1$.  \qed
\end{proof}

\begin{prop}
For all bases $b > 1$ and all numbers $x, y$,
\[ x^{\log_b y} \ = \ y^{\log_b x} \]
\end{prop}

\begin{proof}
We invoke (\ref{eq:logarithm-defn}) twice to remark that
\[ \left[x^{\log_b y} \ = \ b^{(\log_b x) \cdot (\log_b y)}\right]
\ \ \mbox{ and } \ \ 
\left[y^{\log_b x}\ = \ b^{(\log_b y) \cdot (\log_b x)}\right] \]
The commutativity of addition completes the verification.  \qed
\end{proof}

\begin{prop}
For any base $b >1$,
\[ \log_b (1/x) \ = \ - \log_b x \]
\end{prop}

\begin{proof}
This follows from the fact that $\log_b 1 =0$, coupled with the
product law for logarithms.
\[ \log_b x + \log_b (1/x) \ = \ \log_b (x \cdot (1/x))
\  = \ \log_b 1 \ = \ 0 
\]
\qed
\end{proof}

\begin{prop}
For any bases $a, b >1$,
\begin{equation}
\label{eq:log-exp-0}
\log_b x \ = \ \left(\log_b a \right) \cdot \left( \log_a x \right).
\end{equation}
\end{prop}

\begin{proof}
We begin by noting that, by definition,
Note that
\begin{equation}
\label{eq:log-exp-1}
 x \ = \ b^{\log_b x} \ = \ a^{\log_a x} .
\end{equation}
Let us take the base-$b$ logarithm of the second and third expressions
in (\ref{eq:log-exp-1}) and then invoke the product law for logarithms.
From the second expression in (\ref{eq:log-exp-1}), we find that
\begin{equation}
\label{eq:log-exp-2}
 \log_b \left(b^{\log_b x} \right) \ = \ \log_b x .
\end{equation}
From the third expression in (\ref{eq:log-exp-1}), we find that
\begin{equation}
\label{eq:log-exp-3}
 \log_b \left( a^{\log_a x} \right) \ = \
\left(\log_b a \right) \cdot \left( \log_a x \right).
\end{equation}
We know from (\ref{eq:log-exp-1}) that the righthand expressions in
(\ref{eq:log-exp-2}) and (\ref{eq:log-exp-3}) are equal, whence
(\ref{eq:log-exp-0}).   \qed
\end{proof}

If we set $x = b$ in (\ref{eq:log-exp-0}), then we find the following
marvelous equation.

\begin{prop}
For any integers $a, b >1$,
\begin{equation}
\left(\log_b a \right) \cdot \left( \log_a b \right) \ = \ 1 \ \ \ \ \
\mbox{ or, equivalently, } \ \ \ \ \
\log_b a \ = \ \frac{1}{\log_a b} .
\end{equation}
\end{prop}


\subsection{Exponentials and logarithms within information theory}
\label{sec:count-strings}

The student should recognize and be able to reason about the following
facts.  If one has an alphabet of $a$ letters/symbols and must provide
distinct string-label ``names'' for $n$ items, then at least one
string-name must have length no shorter than $\lceil \log_a n \rceil$.

\begin{prop}
\label{thm:bound-stringnames-lgth-k}
Say that one must assign distinct labels to $n$ items, via strings
over an alphabet of $a$ symbols.  Then at least one string-label must
have length no shorter than $\lceil \log_a n \rceil$.
\end{prop}

\begin{proof}
Let $\Sigma$ be an alphabet of $a$ symbols.  For each integer $k \geq
0$, let $\Sigma^{(k)}$ denote the set of all length-$k$ strings over
$\Sigma$.  The bound of Proposition~\ref{thm:bound-stringnames-lgth-k}
follows by counting the number of strings of various lengths over
$\Sigma$, because each such string can label at most one item.  Let
us, therefore, inductively evaluate the cardinality $|\Sigma^{(k)}|$
of each set $\Sigma^{(k)}$.
\begin{itemize}
\item
$|\Sigma^{(0)}| =1$

This is because the null-string $\varepsilon$ \index{$\varepsilon$:
  the null string, of length $0$} \index{null string $\varepsilon$}
is the unique string in $\Sigma^{(0)}$; i.e., $\Sigma^{(0)} = \{
\varepsilon \}$.

\item
$|\Sigma^{(k+1)}| = |\Sigma| \cdot |\Sigma^{(k)}|$.

This reckoning follows from the following recipe for creating all
strings over $\Sigma$ of length $k+1$ from all strings of length $k$.
\[
\Sigma^{(k+1)} \ = \ \{ \sigma x \ | \ \sigma \in \Sigma \ \ \mbox{
  and } \ \ x \in \Sigma^{(k)} \}
\]
In other words, every length-$(k+1)$ string over $\Sigma$ is obtained
by taking a length-$k$ string $x$ over $\Sigma$ and {\em prepending}
\index{prepend a symbol to a string} to it a symbol from $\Sigma$,
i.e., adding $x$ an additional leftmost symbol.

This recipe is correct because
  \begin{itemize}
  \item
Each string in $\Sigma^{(k+1)}$, as constructed, has length $k+1$.

This is because the recipe adds a single symbol to a length-$k$
string.
  \item
For each string $x \in \Sigma^{(k)}$, there are $|\Sigma|$ distinct
strings in $\Sigma^{(k+1)}$, as constructed.

This is because each string in $\Sigma^{(k+1)}$ begins with a distinct
symbol from $\Sigma$.

  \item
$\Sigma^{(k+1)}$, as constructed, contains all strings of length $k+1$
over $\Sigma$.

This is because for each $\sigma \in \Sigma$ and each $x \in
\Sigma^{(k)}$, the string $\sigma x$ is in $\Sigma^{(k+1)}$, as
constructed.
  \end{itemize}
\end{itemize}
We thus have the following recurrence.
\begin{eqnarray*}
|\Sigma^{(0)}| & = & 1 \\
|\Sigma^{(k+1)}| & = & |\Sigma| \cdot |\Sigma^{(k)}| \ \ \ \ 
\mbox{ for } \ k \geq 0
\end{eqnarray*}
Using the Master Theorem of Section~\ref{sec:masterTheorem}, we thus
find explicitly that:

\noindent
For each $\ell \in \N$,
\[ |\Sigma^{(\ell)}| \ \ = \ \ \frac{|\Sigma|^{\ell+1} \ - \ |\Sigma|}
{|\Sigma| -1} \ \ \leq \ \ c \cdot |\Sigma|^{\ell}
\]
for some constant $c$.  In order for this quantity to reach the value
$n$, we must have
\[ \ell \ > \ d \cdot \log_{|\Sigma|} n   \]
for some small constant $d$.  \qed
\end{proof}

The following result can be considered anther way of looking at
Proposition~\ref{thm:bound-stringnames-lgth-k}.

\begin{prop}
\label{thm:Num-strings-lgth-k}
The number of distinct strings of length $k$ over an alphabet of $a$
symbols is $a^k$.
\end{prop}

\begin{proof}
As in Proposition~\ref{thm:bound-stringnames-lgth-k}, let us focus on
the generic $a$-letter alphabet $\Sigma \ = \ \{\sigma_1, \sigma_2,
\ldots, \sigma_a\}$.  We argue by induction on string-length $k$.

\noindent
{\it Bases.}
The induction we develop can start either with strings of length $k=0$
or with strings of length $k=1$.  Some people countenance the {\it
  null string} \index{word!the null word} \index{string!the null string}
$\varepsilon$,  \index{word!$\varepsilon$: the null word}
\index{string!$\varepsilon$: the null string} which contains no
symbols, hence has length $0$ as a legitimate string; others insist
that the status ``string'' can be enjoyed only by non-null strings.
This is purely a matter of taste.

At any rate, everyone agrees that, if one accepts $\varepsilon$ as a
legitimate string over alphabet $\Sigma$, then there is only $a^0 = 1$
such word; this is the case $k=0$ of the proposition.  And, everyone
agrees that, if you insist on non-null tsrings, then there are $a^1 =
a$ such strings over $\Sigma$, one for each symbol $\sigma \in
\Sigma$; this is the case $k=1$ of the proposition.  No matter which
side of the fence you choose to stand on, we have verified our
induction's base case.

\noindent
{\it The inductive hypothesis.}
Say that for all string-lengths $k$ up through $n$, there are $a^k$
distinct words of length $k$ over $\Sigma$.

\noindent
{\it Extending the induction.}
Take each length-$n$ string $x$ over $\Sigma$, and {\em append}
\index{append a symbol to a string} to it, in turn, each of $\Sigma$'s
$a$ symbols, i.e., add each symbol to $x$ as an additional rightmost
symbol.  One thereby creates $a$ new strings each obviously distinct
$x$, namely, $x \sigma_1$, $x \sigma_2$, \ldots, $x \sigma_a$.  We
have thus created $a^{n+1}$ distinct length-$(n+1)$ words over
$\Sigma$ from $\Sigma$'s $a^n$ distinct length-$n$ words.

The induction is extended, which completes the proof.  \qed
\end{proof}

\section{$\oplus$ Pointers to Specialized Topics}

Emerging technologies prolifically give rise to novel specialties that
grab the imagination of the computer-literate public.  Since many such
specialties build upon mathematical concepts and tools, they afford
the designer of a mathematics course an opportunity to add enrichment
to a syllabus that is dominated by the ``classics.''  During the first
decades of the 21st century, application areas such as {\it robotics}
or {\it data science} or {\it data mining} or {\it computer security}
or {\it energy-awareness} each can benefit from supplements to a
standard mathematical syllabus---of course, at levels consistent with
the students' preparation.  We now flesh out this suggestion by
discussing two supplements which, broadly construed, could fit under
the mantle of {\it Arithmetic}.

\subsection{Norms and Metrics for Tuple-Spaces}
\label{sec:Ln-norms}

By the 1950s, computers had become sophisticated enough to handle
structured data computer users became sophisticated enough to think in
terms of structured models.  Thus, from the 1960s and 1970s onward to
the present, one sees {\it tuple-spaces} \index{tuple-spaces} used to
algorithmic advantage in application areas as varied as {\it
  databases}, \cite{Codd70} {\it parallel computing},
\cite{BlumS77,Shinahr74} and {\it robotics}
\cite{Marchese96,Rosenberg12}.  While the specific concepts drawn from
the study of tuple-spaces vary a bit from one application area to
another, certain concepts recur---and among these, the {\it norms} and
{\it metrics} that allow us to talk about notions such as ``distance''
play a very important role.  Of course, notions of ``distance''
relevant to databases may differ from those used in, e.g., robotic
path planning, but in very many applcations it is important to know
whether tuple $t_1$ is ``closer to'' tuple $t_2$ than to tuple $t_3$.

It is an often-frustrating phenomenon that new application areas all
too frequently rename concepts and tools that it inherits from its
predecessors.  So how shall we name the norms and metrics that we
inject into our mathmeatics curriculum?  Probably the most scholarly
course is to employ the names that honor the early 20th century French
mathematician and function theorist Henri L\'{e}on Lebesgue
\index{Lebesgue, Henri L\'{e}on} whose $L$-measures
\index{Lebesgue's $L$-measures} form the classical taxonomy of norms
and distances in tuple-spaces.

We focus on {\em two-dimensional} tuple-spaces, although everything we
say generalizes in a straightforward manner to arbitrary
finite-integer dimensionalities.  And, we restrict attention to the
three $L$-measures that are the most common in application areas such
as informatics and robotics.  Focus on arbitrary $2$-tuples of numbers
$z_1 = \langle x_1, y_1 \rangle$ and $z_2 = \langle x_2, y_2 \rangle$.

\medskip

\noindent{\it The $L^1$ measure.} \index{$L^1$ measure}
\begin{itemize}
\item
The $L^1$-{\it norm} \index{$L^1$ measure!norm} of $z_1$ is the sum of
the magnitudes of its coordinates, i.e.,
\[ L^1(z_1) \ = \ |x_1| + |y_1|.  \]  

\item
The $L^1$-{\it distance} \index{$L^1$ measure!distance} between $z_1$
and $z_2$ is the sum of the magnitudes of the differences between the
coordinates of the two pairs, i.e.,
\[ L^1(z_1, z_2) \ = \ |x_1 - x_2| + |y_1 - y_2|. \]
\end{itemize}
$L^1$-distance is also called {\it Manhattan distance}
\index{Manhattan distance} or {\it rook distance},
\index{rook distance} because a path between $z_1$ and $z_2$, viewed
as points in $2$-dimensional space, in which adjacent points are unit
$L^1$-distance apart, follows a rectilinear grid-like pattern such one
observes in a map of Manhattan or in the path taken by a rook in
chess.  From a less picturesque point of view, one observes that each
point in $2$-dimensional space is unit $L^1$-distance from four other
points, one each in the four NEWS (north, east, south, west) compass
directions.  If one maps out the points in $2$-dimensional space that
are at successive $L^1$-distances from a ``center'' point $z$, then
one observes the {\it $L^1$-disc} \index{$L^1$ measure!disc} of
Fig.~\ref{fig:Ln-discs}(left), which generalizes in higher dimensions
to the {\it $L^1$-sphere}. \index{$L^1$ measure!sphere}
\begin{figure}[hbt]
\[
\begin{array}{ccc}
\begin{array}{|ccccccc|}
\hline
  &   &   & \fbox{3} &   &   &   \\
  &   & \fbox{3} & \fbox{2} & \fbox{3} &   &   \\
  & \fbox{3} & \fbox{2} & \fbox{1} & \fbox{2} & \fbox{3} &   \\
\fbox{3} & \fbox{2} & \fbox{1} & \fbox{0} & \fbox{1} & \fbox{2} & \fbox{3} \\
  & \fbox{3} & \fbox{2} & \fbox{1} & \fbox{2} & \fbox{3} &   \\
  &   & \fbox{3} & \fbox{2} & \fbox{3} &   &   \\
  &   &   & \fbox{3} &   &   &   \\
\hline
\end{array}
  & \hspace*{.5in} &
\begin{array}{|ccccccc|}
\hline
\fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} \\
\fbox{3} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{3} \\
\fbox{3} & \fbox{2} & \fbox{1} & \fbox{1} & \fbox{1} & \fbox{2} & \fbox{3} \\
\fbox{3} & \fbox{2} & \fbox{1} & \fbox{0} & \fbox{1} & \fbox{2} & \fbox{3} \\
\fbox{3} & \fbox{2} & \fbox{1} & \fbox{1} & \fbox{1} & \fbox{2} & \fbox{3} \\
\fbox{3} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{2} & \fbox{3} \\
\fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} & \fbox{3} \\
\hline
\end{array}
\end{array}
\]
\caption{The radius-$3$ $L^1$-disc (left) and $L^\infty$-disc (right),
  with annotated $L^1$-distances and $L^\infty$-distances,
  respectively, from the ``center'' point.}
\label{fig:Ln-discs}
\end{figure}

\medskip

\noindent{\it The $L^\infty$ measure.} \index{$L^\infty$ measure}
\begin{itemize}
\item
The $L^\infty$-{\it norm} \index{$L^\infty$ measure!norm} of $z_1$ is
the larger of the magnitudes of its coordinates, i.e.,
\[ L^\infty(z_1) \ = \ \max(|x_1|, \ |y_1|).  \]  

\item
The $L^\infty$-{\it distance} \index{$L^\infty$ measure!distance}
between $z_1$ and $z_2$ is the larger of the magnitudes of the
differences between the coordinates of the two pairs, i.e.,
\[ L^\infty(z_1, z_2) \ = \ \max(|x_1 - x_2|, \  |y_1 - y_2|). \]
\end{itemize}
$L^\infty$-distance is also called {\it king's-move distance},
\index{king's-move distance} because each step in a path between $z_1$
and $z_2$, in which adjacent points are unit $L^\infty$-distance apart
follows a pattern such as one observes in a path taken by a king in
chess.  From a less picturesque point of view, one observes that each
point in $2$-dimensional space is unit $L^\infty$-distance from eight
other points, one each in the eight compass directions N, NE, E, SE,
S, SW, W, NW.  If one maps out the points in $2$-dimensional space
that are at successive $L^\infty$-distances from a ``center'' point
$z$, then one observes the {\it $L^\infty$-disc} of
Fig.~\ref{fig:Ln-discs}(right), \index{$L^\infty$ measure!disc} which
generalizes in higher dimensions to the
{\it $L^\infty$-sphere}. \index{$L^\infty$ measure!sphere}

\smallskip

As one observes in Fig.~\ref{fig:Ln-discs}, the structures of the
$L^1$-disc and the $L^\infty$-disc are, respectively, dominated by
diagonals and squares; see the ``equipotential'' lines in the figure.
One can exploit this fact when crafting algorithms that are governed
by these norms.  Indeed, we invoke the structures of these discs as we
develop the ``pairing functions'' which we showcase in
Section~\ref{sec:building-pairing-functions}.

\bigskip

We have not (yet) mentioned what is likely the most familiar
$L$-measure, namely, the $L^2$ measure which is named for Euclid in
honor of his celebrated treatise on geometry, {\it The Elements}.
\index{Euclid!{\it The Elements}} For completeness, we now define this
measure, but we do not develop the subject further because it is not
frequently used when dealing with the discrete mathematical structures
that are the primary foci of this text.

\medskip

\noindent{\it The $L^2$ measure.} \index{$L^2$ measure}
\begin{itemize}
\item
The $L^2$-{\it norm} \index{$L^2$ measure!norm} of $z_1$ is the
Euclidean distance of the point from the {\it origin} of
$2$-dimensional space, \index{origin of $2$-dimensional space}
which is the point $\langle 0,0 \rangle$:
\[ L^2(z_1) \ = \ \sqrt{x_1^2 + y_1^2}.  \]  

\item
The $L^2$-{\it distance} \index{$L^2$ measure!distance} between $z_1$
and $z_2$ is the Euclidean distance between the points in
$2$-dimensional space, i.e.,
\[ L^2(z_1, z_2) \ = \ \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}. \]
\end{itemize}


\subsection{Edit-Distance: Measuring Closeness in {\em String Spaces}}

From the earliest days of digital computing, the challenge of
employing computers as ``editorial assistants'' was viewed as a prime
domain within which to achieve a significant practical payoff.  An
early problem within this domain was to determine of ``how different''
two strings, $x$ and $y$, over an alphabet $\Sigma$ are from one
another.  The quotation marks in the preceding sentence acknowledges
that it is actually not obvious how to measure the ``distance''
between $x$ and $y$ in a way that matters.  The intersecting need of
three separate developing communities finally supplied the definition
that has generally been accepted.  Roughly contemoraneously, during,
say, the decade from 1955--1965:
  \begin{itemize}
  \item
Computational linguists attempted to enlist computers in the
processing of natural language.  The goal of automatic translation
from Russian to English became the ``holy grail'' of this community.
Reminiscences by Anthony G.~Oettinger, \index{Oettinger, Anthony G.}
a pioneer in this effort, appear in \cite{Hutchins00}.
   \item
As the potential power of computers became appreciated, more
sophisticated process-specification languages began to emerge.  From
the earliest days of this development, programming languages became
more ``language-like''.  One of the earliest entrants in this arena
was the business-oriented language COBOL, which was inspired by early
work of the computing giant Grace Murray Hopper 
\index{Hopper, Grace Murray} and developed under the leadership of
Jean E.~Sammet; \index{Sammet, Jean E.} see \cite{Sammet78}.  A second
early entrant was the string-processing language COMIT developed under
the leadership of Victor Yngve \index{Yngve, Victor} with an eye
toward applications such as language processing; see \cite{Yngve}.
Finally, the ever-evolving FORTRAN language was developed in the
mid-1950s by a small team at IBM, headed by John Backus;
\index{Backus, John} see \cite{Backus-etal57}.  FORTRAN was the first
programming language that aimed to enable scientists to specify the
problems they wished to compute in a manner that was at least
reminiscent of the mathematical notation they would use to communicate
with one another.
   \item
As it became clear that FORTRAN was a step toward satisfying the
programming needs of an enormous market, a veritable army of systems
programmers began to develop increasingly sophisticated processors for
programming languages.  The era of ``smart compilers'' was dawning.
   \end{itemize}

A common feature in the preceding developments was that people were
typing more as they used computers.  Inevitably, therefore, they were
making more typing mistakes.  A very specific computing challenge
arose, to make much more concrete the desire to understand how close a
string $x$ was to a string $y$.  Could one develop an algorithm that
would rewrite one of these strings as the other while ``editing'',
or, rewriting, as few symbols as possible?  While such an algorithm
would not completely solve the ``mistyping'' problem---consider, e.g.,
that the strings

\hspace*{.25in} ``SORTH'' \ \ and \ \ ``NOUTH''

\noindent
are both ``edit-distance $1$'' from both

\hspace*{.25in} ``SOUTH'' \ \ and \ \ ``NORTH''

\noindent
hence cannot be ``corrected'' automatically---being able to correct a
mistyped string to its edit-nearest legitimate string would probably
be very useful in practice.  Happily, although this algorithmic
problem was more difficult than many had imagined, it did admit the
efficient elegant solution that appears in \cite{WagnerF74}.






\ignore{**************
\section{Useful Nonalgebraic Notions}
\label{sec:extra-functions}

\subsection{Nonalgebraic Notions Involving Numbers}
*************}

\ignore{************

This section is devoted to showing that,
even when a number system lacks intrinsic desirable ordering
properties, we can sometimes endow the system with ``inherited
access'' to such properties by devising a {\em bijection} that {\it
  encodes}

%
the system's numbers as nonnegative integers.  We focus here on two
important number systems, the rational and complex numbers, $\Q$ and
$\C$, whose ordering properties are much weaker than those of the
nonnegative integers $\N$: Neither $\Q$ nor $\C$ is {\em
  well-ordered}; $\C$ is not even {\em totally} ordered.
\[ \approx \approx \approx \approx \approx \approx \approx \approx \approx \approx \]
{\em Well-ordering} is an especially welcome property because it
enables algorithms that are structured as a linear recursion that
``counts down'' from an argument $n$.  Well-ordering guarantees that
there is a ``bottom'' that will terminate the downward recursion.
\[ \approx \approx \approx \approx \approx \approx \approx \approx \approx \approx \]

\noindent
Two simplifications facilitate our quest for bijective encodings
\[ \varepsilon_1: \Q \leftrightarrow \N \ \ \ \ \ \mbox{ and }
\ \ \ \ \ \varepsilon_2: \C \leftrightarrow \N
\]
\begin{itemize}
\item
We abstract both $\Q$ and $\C$ as the set $\N \times \N$.  Since both
$\Q$ and the 
\index{numbers!complex integers}
%
{\it complex integers}---complex numbers $a+bi$ where $a,b \in
\N$---can be encoded as ordered pairs of nonnegative integers, this is
a natural abstraction.

Of course, using this abstraction to encode $\Q$ into $\N$ ignores
common (integer) divisors of a fraction's numerator and denominator,
whose elimination preserve the fraction's value.  A very simple
algorithm would compensate for this.

\item
We slightly change our agenda and focus on bijections between $\N^+
\times \N^+$ and $\N^+$, where $\N^+$ is the set of {\em positive}
integers.
\index{number!integer!positive integer}
\index{$\N^+$:number!integer!positive integer}

Focusing on {\em positive} integers rather than {\em nonnegative}
integers somewhat simplifies certain mathematical expessions.
\end{itemize}
The structures of the domain $\N \times \N$ and the range $\N$ of the
bijective encodings of interest have led to the name {\it pairing
  functions}
\index{pairing function}\index{pairing function as encoding}
for these bijections.

\smallskip
**************}


