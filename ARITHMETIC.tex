%version of 02-25-19

\chapter{ARITHMETIC}
\label{ch:arithmetic}

Previous chapters have given us sets, numbers, and numerals, the
simple objects that we use to count and measure and aggregate.  The
current chapter is devoted to expounding the rules of {\em
  arithmetic}, the system which allows us to manipulate these objects
and to develop complex objects out of the simple ones.

\section{The Basic Arithmetic Operations}
\label{sec:Arithmetic-Tools}
\index{laws of arithmetic}

The basic tools of arithmetic consist of a small set of operations on
numbers/numerals, together with two special integers which play
important roles with respect to the operations.  Since these entities,
the operations and special numbers, are so tightly intertwined, we
discuss them simultaneously.

\smallskip

\noindent {\small\sf The two special integers.}
\index{arithmetic!the arithmetic identities, $0$ and $1$}
The integers zero ($0$)
\index{number!zero ($0$), the additive identity} 
and one ($1$),
\index{number!one ($1$), the multiplicative identity}
play special roles as the tools of srithmetic are applied to all four
classes of numbers we have described in
Chapter~\ref{ch:numbers-numerals}.

\smallskip

\noindent {\small\sf The operations of arithmetic}\index{arithmetic!basic operations}
%
Arithmetic on the four classes of numbers described in
Chapter~\ref{ch:numbers-numerals} is built upon a rather small
repertoire of operations.  When we say that an operation produces a
number ``of the same sort'', we mean that it produces
\begin{itemize}
\item
an integer result from integer arguments;
\item
a rational (number) result from rational (number) arguments;
\item
a real (number) result from real (number) arguments;
\item
a complex (number) result from complex (number) arguments;
\end{itemize}
The fundamental operations on numbers are, of course, familiar to the
reader.  Our goal in discussing them is to stress the laws that govern
the operations.  Along the way, we also introduce a few operations
that are less familiar but no less important.

\subsection{Unary (single-argument) operations}
\label{sec:unary-ops}


\subsubsection{Negating and reciprocating numbers}
\index{arithmetic!basic operations!negation}

\noindent {\it i. The operation of {\em negation}}:
\index{arithmetic!basic operations!negating}
\begin{itemize}
\item
is a {\em total function} on the sets $\Z, \Q, \R, \C$.  It replaces
a number $a$ by its {\em negative},
\index{number!negative}
a number of the same sort, denoted $-a$.
\item
is a {\em partial function} on the nonnegative subsets
of $\Z, \Q, \R, \C$.  It replaces a number $a$ by its negative, $-a$,
whenever both $a$ and $-a$ belong to the nonnegative subset being
operated on.
\end{itemize}
Zero ($0$) is the unique {\it fixed point}\index{function!fixed
  point}\index{arithmetic!negation!fixed point} of the operation,
meaning that $0$ is the unique number $a$ such that $a = -a$.

\medskip

\noindent {\it ii. The operation of {\em reciprocation}}:
\index{arithmetic!basic operations!reciprocal}
\begin{itemize}
\item
\index{arithmetic!basic operations!reciprocating}
is a {\em total function} on the sets $\Q, \R, \C$, which replaces each
number $a$ by its {\em reciprocal}, 
\index{number!reciprocal}
a number of the same sort, denoted $1/a$ or $\displaystyle {1 \over
  a}$.  We shall employ whichever notation enhances legibility.

\item
is {\em undefined} on every integer $a$ except for $1$.
\end{itemize}

\subsubsection{Floors, ceilings, magnitudes}
\index{arithmetic!basic operations!floor}
\index{arithmetic!basic operations!ceiling}
\index{arithmetic!basic operations!absolute value}
\index{arithmetic!basic operations!magnitude}

\noindent {\it i. The operations of {\em taking floors and ceilings}}
are total operations on the sets $\N, \Z, \Q, \R$.
\begin{itemize}
\item
The {\it floor} of a number $a$, also called {\it the integer part}
\index{arithmetic!basic operations!integer part of a number}
\index{arithmetic!basic operations!floor of a number}
of $a$, denoted $\lfloor a \rfloor$, is the largest integer that does
not exceed $a$; i.e.,:
\[
\lfloor a \rfloor \ \eqdef \ \max_{b \in {\mathbb{N}}} \Big[ b \ \leq a \Big]
\]
\item
The {\it ceiling} of a number $a$
\index{arithmetic!basic operations!ceiling of a number}
of $a$, denoted $\lceil a \rceil$, is the smallest integer that is 
not smaller than $a$:
\[
\lceil a \rceil \ \eqdef \ \min_{b \in {\mathbb{N}}} \Big[ b \ \geq a \Big]
\]
\end{itemize}
Thus, the operations of taking floors and ceilings are two ways to
{\em round} rationals and reals to their ``closest''
integers.\index{arithmetic!basic operations!rounding to ``closest'' integer}

\medskip

\noindent {\it ii. The operations of taking {\em absolute values/magnitudes}}:
\index{arithmetic!basic operations!absolute value, magnitude}
%
Let $a$ be a real number.  The {\it absolute value}, or, {\it
  magnitude}, of $a$, denoted $|a|$ equals either $a$ or $-a$,
whichever is positive.  For a complex number $a$, the definition of
$|a|$ is more complicated: it is a measure of $a$'s ``distance'' from
the ``origin'' complex number $0 + 0 \cdot i$.  In detail:
\[
|a| \ = \ \left\{
\begin{array}{cl}
a & \mbox{ if } \ [a \in \R] \ \ \mbox{ and } [a \geq 0] \\
-a & \mbox{ if } \ [a \in \R] \ \ \mbox{ and } [a < 0] \\
\sqrt{b^2 + c^2} &  \mbox{ if } \ [a \in \C]  \ \ \mbox{ and } [a = (b+ci)]
\end{array}
\right.
\]

\subsubsection{Factorials (of nonnegative integers)}
\index{arithmetic!basic operations!factorial (of a nonnegative integer)}

The {\it factorial} of a nonnegative integer $n \in \N$, which is
commonly denoted $n!$,
\index{arithmetic!basic operations!$n!$: factorial of $n \in \N$}
\index{arithmetic!basic operations!factorial of nonnegative integer}
is the function defined via the following recursion.
\begin{equation}
\label{eq:n-factorial-recursion}
\mbox{\sc fact}(n) \ = \ \left\{
\begin{array}{cl}
1 & \mbox{  if } \ n=0 \\
n \cdot \mbox{\sc fact}(n-1) & \mbox{  if } \ n>0
\end{array}
\right.
\end{equation}
By ``unwinding'' the recursion in (\ref{eq:n-factorial-recursion}),
one finds that, for all $n \in \N$,
\begin{equation}
\label{eq:n-factorial-direct}
n! \ = \ \mbox{\sc fact}(n) \ = \ 
n \cdot (n-1) \cdot (n-2) \cdot \cdots \cdot 2 \cdot 1
\end{equation} 
A $3$-step inductive argument validates this ``unwinding'':
\begin{enumerate}
\item
If $n =0$, then {\sc fact}$(n) = 1$, by definition
(\ref{eq:n-factorial-recursion}).
\item
Assume, for induction, that the expansion in
(\ref{eq:n-factorial-direct}) is valid for a given $k \in N$:
\[ \mbox{\sc fact}(k) \ = \ k \cdot (k-1) \cdot (k-2) \cdot \cdots
\cdot 2 \cdot 1 \] 
\item
Then:
\[
\begin{array}{lclll}
\mbox{\sc fact}(k+1) & = & (k+1) \cdot \mbox{\sc fact}(k)
  & & \mbox{by (\ref{eq:n-factorial-recursion})} \\
  & = &
(k+1) \cdot k \cdot (k-1) \cdot (k-2) \cdot \cdots \cdot 2 \cdot 1
  & & \mbox{by induction}
\end{array}
\]
\end{enumerate}


\subsection{Binary (two-argument) operations}
\label{sec:binary-operators}

\subsubsection{Addition and Subtraction}
\index{arithmetic!basic operations!addition}
\index{arithmetic!basic operations!subtraction}

The operation of {\it addition}\index{arithmetic!addition} is a {\em
  total function} that replaces any two numbers $a$ and $b$ by a
number of the same sort.  The resulting number is the {\em sum of $a$
  and $b$}\index{arithmetic!addition!sum} and is denoted $a+b$.

\noindent
The operation of {\it subtraction}\index{arithmetic!subtraction} is a
{\em total function} on the sets $\Z, \Q, \R, \C$, which replaces any
two numbers $a$ and $b$ by a number of the same sort.  The resulting
number is the {\em difference of $a$ and $b$}
\index{arithmetic!subtraction!difference} and is denoted $a-b$.  On
the nonnegative subsets of the sets $\Z, \Q, \R, \C$---such as $\N$,
which is the largest nonnegative subset of $\Z$---subtraction is a
{\em partial function}, which is defined only when $a \geq b$.

Subtraction can also be defined as follows.  For any two numbers $a$
and $b$, {\em the difference of $a$ and $b$ is the sum of $a$ and the
  negation of $b$}; i.e.,
\[ a-b \ = \ a + (-b) \]

{\em The special role of $0$ under addition and subtraction.}
%
The number $0$ is the {\it identity} under addition and
  subtraction.\index{number!additive identity}\index{number!identity
  under addition}\index{identity!additive}
%
This means that, for all numbers $a$,
\[ a+0 \ = \ a-0 \ = \ a. \]

{\em The special role of $1$ under addition and subtraction.}
%
For any integer $a$, there is no integer between $a$ and $a+1$ or
between $a-1$ and $a$.  For this reason, on the sets $\Z$ and $\N$,
one often singles out the following special cases of addition and
subtraction, especially in reasoning about situations that are indexed
by integers.  Strangely, these operations have no universally accepted
notations.
\begin{itemize}
\item
The {\it successor} operation\index{arithmetic!integers!successor} is
a {\em total function} on both $\N$ and $\Z$, which replaces an
integer $a$ by the integer $a+1$.
\item
The {\it predecessor} operation\index{arithmetic!integers!predecessor}
is a {\em total function} on $\Z$, which replaces an integer $a$ by
the integer $a-1$.  It is a {\em partial function} on $\N$, which is
defined only when the argument $a$ is positive (so that $a-1 \in \N$).
\end{itemize}

The operations of addition and subtraction are said to be {\em
  mutually inverse operations}\index{arithmetic!integers!additive
  inverse} \index{arithmetic!integers!addition and subtraction are
  mutually inverse} of each other because each can be used to ``undo''
the other:
\[
a \ = \ (a+b) -b \ = \ (a-b) +b
\]

\subsubsection{Multiplication and Division}
\index{arithmetic!basic operations!multiplication}
\index{arithmetic!basic operations!division}

The operation of {\it multiplication}\index{arithmetic!multiplication}
is a {\em total function} that replaces any two numbers $a$ and $b$ by
a number of the same sort.  The resulting number is the {\em product
  of $a$ and $b$}\index{arithmetic!multiplication!product} and is
denoted either $a \cdot b$ \index{arithmetic!multiplication!$a \cdot  b$}
or $a \times b$.\index{arithmetic!multiplication!$a \times b$}
We shall usually favor the former notation, except when the latter
enhances legibility.

The operation of {\it division}\index{arithmetic!division} is a {\em
  partial function} on all of our sets of numbers.  Given two numbers
$a$ and $b$, the result of dividing $a$ by $b$---{\em when that result
  is defined}---is the {\it quotient of $a$ by $b$}
\index{arithmetic!division!When is $a/b$ defined?}
\index{arithmetic!division!quotient}
\index{arithmetic!division!quotient!$a/b$}
\index{arithmetic!division!quotient!$a \div b$}
\index{arithmetic!division!quotient!${a \over b}$}
and is denoted by one of the following three notations: $a/b$, $a \div
b$, $\displaystyle{a \over b}$.  The {\it quotient of $a$ by $b$} is
defined precisely when {\em both}

\noindent
\hspace*{.35in}(1) $b \neq 0$: one can never divide by $0$ \\
\hspace*{.35in}{\em and} \\
\hspace*{.35in}(2) there exists a number $c$ such that $a = b \cdot c$.

\noindent
Assuming that condition (1) holds, {\em condition (2) always holds
  when $a$ and $b$ belong to $\Q$ or $\R$ or $\C$}.

Division can also be defined as follows.  For any two numbers $a$
and $b$, {\em the quotient of $a$ and $b$ is the product of $a$ and the
reciprocal of $b$} (assuming that the latter exists); i.e.,
\[ a/b \ = \ a \cdot (1/b). \]
Computing reciprocals of nonzero numbers in $\Q$ and $\R$ is standard
high-school level fare; computing reciprocals of nonzero numbers in
$\C$ requires a bit of calculational algebra which we do not cover.
For completeness, we note that the reciprocal of the {\em nonzero}
complex number $a + bi \in \C$ is the complex number $c+di$ where
\[ c \ = \ \frac{a}{a^2 + b^2} \ \ \ \ \
\mbox{ and } \ \ \ \ \
d \ = \ \frac{-b}{a^2 + b^2}.
\]

{\em The special role of $1$ under multiplication and division.}
%
The number $1$ is the {\it identity} under the operations of
multiplication and division.\index{number!multiplicative
  identity}\index{number!identity under
  multiplication}\index{identity!multiplicative}
%
This means that, for all numbers $a$,
\[ a \cdot 1 \ = \ a \cdot (1/1) \ = \ a. \]

{\em The special role of $0$ under multiplication and division.}
%
The number $0$ is the {\it annihilator} under
multiplication.\index{multiplicative annihilator} This means that, for
all numbers $a$
\[ a \cdot 0 \ = \ 0. \]

The operations of multiplication and division are said to be {\em
  inverse operations}\index{arithmetic!integers!multiplicative
  inverse} \index{arithmetic!integers!multiplication and division are
  mutually inverse} because, when both operations can be applied, each
can be used to ``undo'' the other:
\[ a = (a \cdot b) \div b \ = \ (a \div b) \cdot b.  \]

\subsubsection{Exponentiation: Raising a number to a power}
\label{sec:exponentiation}

A conceptually powerful notational construct is the operation of {\it
  exponentiation}, \index{arithmetic!basic operations!exponentiation}
i.e., {\it raising a number to a power}.
\index{raising a number to a power}
For real numbers $a$ and $b$, the {\it $b$th power} of $a$, denoted
$a^b$ is defined by the system of equations
\begin{equation}
\label{eq:power-def}
\begin{array}{llll}
\mbox{for all numbers $a>0$} & & & a^0 = 1 \\
 & & & \\
\mbox{for all numbers $a, b, c$} & & & a^b \cdot a^c = a^{b+c}.
\end{array}
\end{equation}
This deceptively simple definition has myriad consequences which we
often take for granted.
\begin{itemize}
\item
For all numbers $a>0$, the number $a^0 = 1$.

This follows (via cancellation) from (\ref{eq:power-def}) via the fact
that
\[ a^b \cdot a^0 \ = \ a^{b+0} \ = \ a^b \ = \ a^b \cdot 1.  \]

\item
For all numbers $a >0$, the number $a^{1/2}$\index{$a^{1/2}$: the
  square root of number $a$}
is the {\it square root} of $a$,\index{square root}
i.e., $a^{1/2}$ is the (unique, via cancellation) number $b$ such that
$b^2 = a$.  Another common notation for The number $a^{1/2}$ is
$\sqrt{a}$.\index{$\sqrt{a}$: the square root of number $a$}

This follows from (\ref{eq:power-def}) via the fact that
\[ a \ = \ a^1 \ = \ a^{(1/2) + (1/2)} \ = \ a^{1/2} \cdot a^{1/2} \ = \
\left(a^{1/2}\right)^2. \]

\item
For all numbers $a>0$ and $b$, the number $a^{-b}$ is the {\it
  multiplicative inverse}\index{multiplicative inverse}
of $a^b$, meaning that $a^b \cdot a^{-b} = 1$

This follows from (\ref{eq:power-def}) via the fact that
\[ a^b \cdot a^{-b} \ = \ a^{(b + (-b))} \ = \ a^0 \ = \  1 \]
\end{itemize}
When the power $b$ is a positive integer, then definition
(\ref{eq:power-def}) can be cast in the following attractive inductive
form:
\begin{equation}
\label{eq:power-def-integer}
\begin{array}{llll}
\mbox{for all numbers $a>0$} & & & a^0 = 1 \\
 & & & \\
\mbox{for all numbers $a$ and integers $b$} & & & a^{b+1} = a \cdot
a^b.
\end{array}
\end{equation}
Summing up, we now know about powers that are integral or fractional,
positive, zero, or negative

\subsubsection{Binomial coefficients and Pascal's triangle}
\label{sec:binomial-coeff}
\index{arithmetic!basic operations!binomial coefficient}

We close our catalogue of arithmetic operations with a binary
operation on\footnote{In advanced contexts, one encounters binomial
  coefficients with non-integer arguments.}~$\N \times \N$.

\smallskip

Let $n$ and $k \leq n$ be nonnegative integers (i.e., elements of
$\N$).  The {\it binomial coefficient} denoted either as
$\displaystyle {n \choose k}$ or as $\Delta_{n,k}$, is the number
\index{binomial coefficients}
\begin{equation}
\label{eq:binom-coeff}
\Delta_{n,k} \ = \
{n \choose k} \ \eqdef \ \frac{n!}{k!(n-k)!} \ = \
\frac{n(n-1)(n-2) \cdots (n-k+1)}{k (k-1)(k-2) \cdots 1}
\end{equation}
Many of the secrets of these wonderful numbers---including the fact
that they are {\em integers}---can be deduced from the following
results.

\begin{prop}
\label{thm:manipulate-binom-coeff}
For all $n, k \in \N$ with $k \leq n$:

{\rm (a)} The symmetry rule:
\index{binomial coefficients!symmetry rule}
\begin{equation}
\label{eq:symmetry-binom-coeff}
{n \choose k} \ = \ {n \choose {n-k}}
\end{equation}

{\rm (b)} The addition rule:
\index{binomial coefficients!addition rule}
\begin{equation}
\label{eq:add-binom-coeff}
{n \choose k} \ + \ {n \choose {k+1}} \ = \ {{n+1} \choose {k+1}}
\end{equation}
\end{prop}

\begin{proof}
($a$)
We verify equation (\ref{eq:symmetry-binom-coeff}) by
(\ref{eq:binom-coeff}) plus the commutativity of multiplication (see
Section~\ref{sec:Arithmetic-Laws}),
\begin{eqnarray*}
{n \choose k} & = & \frac{n!}{k!(n-k)!} \\
              & = & \frac{n!}{(n-k)!k!} \\
              & = & {n \choose {n-k}}
\end{eqnarray*}

\noindent ($b$)
We verify equation (\ref{eq:add-binom-coeff}) by explicitly adding the
fractions exposed by (\ref{eq:binom-coeff}):
\begin{eqnarray*}
{n \choose k} \ + \ {n \choose {k+1}}
  & = &
\frac{n!}{k!(n-k)!} \ + \ \frac{n!}{(k+1)!(n-k-1)!} \\
  & = &
n! \cdot \frac{(k+1) + (n-k)} {(k+1)!(n-k)!} \\
  & = & 
\frac{(n+1)!}{(k+1)!(n-k)!} \\
  & = &
{{n+1} \choose {k+1}}
\end{eqnarray*}

We have thus established  both of the proposition's rules for binomial coefficients.
\qed
\end{proof}

\bigskip

Binomial coefficients are indispensable when studying myriad topics
related to {\em counting}, such as:
\begin{itemize}
\item
what are the relative likelihoods of various $5$-card deals from a
fair $52$-card deck?
\item
What is the likelihood of observing $15$ {\sc head}s and $25$ {\sc
  tail}s in $40$ flips of a fair coin?
\item
What are the comparative operation-count costs of Merge-Sort and
Quick-Sort when sorting $n$ keys; cf.~\cite{CLRS}?
\end{itemize}

We shall have a lot more to say about binomial coefficients throughout
the text.  Within the current chapter, we encounter binomial
coefficients in Section~\ref{sec:powers+polynolmials}; in
Chapter~\ref{ch:Summation}, they play a prominent role in evaluating
arithmetic summations (Section~\ref{sec:arithmetic-series}); in
Chapter~\ref{ch:Recurrences}, they provide an important example of
bilinear recurrences (Section~\ref{sec:bilinear-recurrences}); and in
Chapter~\ref{ch:prob-stat}, they will prove indispensable in analyzing
complex phenomena, by counting the way various situations (such as a
royal flush in poker) can occur.


\subsection{Rational arithmetic: A specialized computational exercise}
\label{sec:Rational-arithmetic}
\index{number!rational!arithmetic}

In Section~\ref{sec:rationals} we defined the rational numbers and
reviewed why they were needed to compensate for the general lack of
multiplicative inverses within the integers.  But we did not review
how to perform arithmetic on the elements of the set $\Q$.  We correct
this shortcoming now.  Of course, the reader will have encountered
rational arithmetic long ago---but we are now reviewing the topic in
order to provide the reader with a set of worthwhile exercise to
reinforce the mathematical thinking whose presentation is our main
goal.

\medskip

The rational numbers build their rules for arithmetic upon the
corresponding rules for integers.  For all $p/q$ and $r/s$ in $\Q$:
\[
\begin{array}{|llcl|}
\hline
\mbox{\small\sf Addition:} & 
{\displaystyle
{p \over q} + {r \over s} }
  & = &
{\displaystyle
 \frac{p \cdot s + r \cdot q}{q \cdot s} }  \\
 & & & \\
\mbox{\small\sf Subtraction:} &
{\displaystyle
{p \over q} + {r \over s} }
  & = & 
{\displaystyle
{p \over q} + {(-r) \over s} } \\
 & & & \\
\mbox{\small\sf Multiplication:} &
{\displaystyle
{p \over q} \cdot {r \over s} }
  & = & 
{\displaystyle
\frac{p \cdot r}{r \cdot s} } \\
  & & & \\
\mbox{\small\sf Division:} &
{\displaystyle
{p \over q} \div {r \over s} }
  & = &
{\displaystyle
{p \over q} \cdot {s \over r} } \\
\hline
\end{array}
\]

It is worth verifying that rational arithmetic as thus defined behaves
in the required manner; in particular that rational arithmetic:
\begin{itemize}
\item
works correctly when the argument rational numbers are, in fact,
integers, i.e., when $q = s = 1$ in the preceding table.
\item
treats the number $0$ appropriately, i.e., as an additive identity and
a multiplicative annihilator; cf., Sections~\ref{sec:arithmetic-tools}
and~\ref{sec:Arithmetic-Laws}.
\item
obeys the required laws; cf., Section~\ref{sec:Arithmetic-Laws}.

Verifying the distributivity of rational multiplication over rational
addition will be a particularly valuable exercise because of the
required amount of manipulation.
\end{itemize}


\section{The Laws of Arithmetic, with Applications}
\label{sec:Arithmetic-Laws}
\index{arithmetic!basic laws}

Following are the basic laws of arithmetic on the reals, rationals,
and reals---the ones that everyone should be able to employ cogently
in rigorous argumentation.

\subsection{The Commutative, Associative, and Distributive Laws} 
\index{commutative law!arithmetic}
\index{commutative law!addition}
\index{commutative law!multiplication}
\index{arithmetic!commutative law}

\noindent {\it i. The commutative law.}
For all numbers $x$ and $y$:
\[
\begin{array}{llc}
\mbox{\it for addition:}
  & & x+y \ = \ y+x \\
\mbox{\it for multiplication:}
  & & x \cdot y \ = \ y \cdot x
\end{array}
\]

\noindent {\it ii. The associative law.}
\index{associative law for arithmetic}
\index{arithmetic!associative law}
For all numbers $x$, $y$, and $z$,
\[ (x+y)+z \ = \ x+(y+z) \ \ \ \mbox{\bf and } \ \ 
x\cdot (y\cdot z) 
(x \cdot y) \cdot z \ = \ x\cdot (y\cdot z). \] 
This allows one, for instance, to write strings of additions or of
multiplications without using parentheses for grouping.


\noindent {\it iii. The distributive law.}
\index{distributive law for arithmetic}
\index{arithmetic!distributive law}
For all numbers $x$, $y$, and $z$,
\begin{equation}
\label{eq:distr-law}
x \cdot (y + z) \ = \ (x \cdot y) + (x \cdot z).
\end{equation}
One commonly articulates this law as, ``{\em Multiplication
  distributes over addition.}''


One of the most common uses of the distributive law reads equation
(\ref{eq:distr-law}) ``backwards,'' thereby deriving a formula for
{\em factoring} \index{arithmetic!factoring} complex expressions that
use both addition and multiplication.

Easily, addition does {\em not} distribute over multiplication; i.e.,
in general, $x + y \cdot z \ \neq \ (x+y) \cdot (x+z)$.  Hence, when
we see ``$x + y \cdot z$'', we know that the multiplication is
performed before the addition.  In other words, {\em Multiplication
  takes priority over addition.}  \index{arithmetic!priority of
  multiplication over addition} This priority permits us to write the
righthand side of (\ref{eq:distr-law}) without parentheses, as in
\[ x \cdot (y + z) \ = \ x \cdot y + x \cdot z. \]

Via multiple invocations of the preceding laws, we can derive a recipe
for multiplying complicated expressions.  We illustrate this via the
``simplest'' complicated expression, $(a+b) \cdot (c+d)$.

\begin{prop}
\label{prop:(a+b)(c+d)}
For all numbers $a, b, c, d$:
\begin{equation}
\label{eq:(a+b)(c+d)}
(a+b) \cdot (c+d) \ = \ a \cdot c + a \cdot d + b \cdot c + b \cdot d
\end{equation}
\end{prop}

\begin{proof}
Note first that because multiplication takes priority over addition,
the absence of parentheses in expressions such as
(\ref{prop:(a+b)(c+d)}) does not jeopardize unambiguity.  Our proof of
the proposition invokes the laws we have just enunciated multiple
times.
\[
\begin{array}{lclll}
(a+b) \cdot (c+d) & = & (a+b) \cdot c \ + \ (a+b) \cdot d
& & \mbox{distributive law} \\ 
  & = & c \cdot (a+b) \ + \ d \cdot (a+b)
& & \mbox{commutativity of multiplication} \ (2 \times) \\
  & = & c \cdot a + c \cdot b + d \cdot a + d \cdot b 
& & \mbox{distributive law} \ (2 \times) \\
  & = & a \cdot c + b \cdot c + a \cdot d + b \cdot d
& & \mbox{commutativity of multiplication} \ (4 \times) \\
  & = &  a \cdot c + a \cdot d + b \cdot c + b \cdot d
& & \mbox{commutativity of addition}
\end{array}
\]
\qed
\end{proof}


We close our short survey of the laws of arithmetic with the following
important two-part law.
\begin{itemize}
\item
{\it The law of inverses}.\index{inverse laws for
  arithmetic}\index{laws of arithmetic!inverse laws}
%
  \begin{itemize}
  \item
Every number $x$ has an {\em additive inverse},\index{additive inverse}
i.e., a number $y$ such that $x+y =0$.  This inverse is $x$'s {\it
  negative} $-x$.\index{additive inverse!negative as additive inverse}
  \item
Every {\em nonzero} number $x \neq 0$ has a {\em multiplicative
  inverse},\index{multiplicative inverse} i.e., a number $y$ such that
$x \cdot y = 1$.  This inverse is $x$'s {\it reciprocal},
$1/x$.\index{multiplicative inverse!reciprocal as multiplicative inverse}
  \end{itemize}
\end{itemize}

We close this section with another of our ``fun'' propositions.

\subsection{A Fun Result: A ``Trick'' for Squaring Certain Integers}

Sometimes only basic knowledge is needed to craft amusing
``tricks''---we know that they are not really tricks at all!---that
are really rigorous applications of principles that we have learned.
Here is an ``old chestnut'' example that may inspire you to design
your own. 

If someone presents you with a number that has a numeral that ends in
$5$, then there is a simple way to square the number mentally.  For
instance, if someone says

\hspace{.25in}``$n = 25$''

\noindent
then you can instantly respond

\hspace{.25in}``$n^2 = 625$''

\noindent
If the challenge is

\hspace{.25in}``$n = 75$''

\noindent
then your response is

\hspace{.25in}``$n^2 = 5625$''

\noindent
Let's make this ``game'' mathematical.

\begin{prop}
\label{thm:75x65=4925}
Let $n$ be any number that has a $2$-digit decimal numeral of the form

\hspace{.25in}$\delta \ 5$ \ \ \ \ $(\delta \in \{ 0,1,2,3,4,5,6,7,8,9\})$.

\noindent
Then the square of $n$ is the integer

\hspace{.25in}$25 \ + \ \delta \cdot (\delta +1)$. 
\end{prop}

\begin{proof}
We can rewrite the premise of the proposition in the form
\[ n \ = \ 10 \cdot \delta + 5 \]
It is now easy to invoke Proposition~\ref{prop:(a+b)(c+d)} and the
distributive law to compute that

\[ n^2 \ = \ 100 \cdot \delta \cdot (\delta+1) + 25. \]
To wit: 
\[
\begin{array}{lclll}
n^2 & = & (10 \cdot \delta + 5)^2 & & \mbox{Given} \\
    & = & 100 \cdot \delta^2 \ + \ 100 \cdot delta \ + \ 25
              & & \mbox{the proposition} \\
    & = & 100 \cdot (\delta^2 \ + \ \delta) \ + \ 25
              & & \mbox{factoring: distributive law} \\
    & = & 100 \cdot \delta \cdot (\delta + 1) \ + \ 25
              & & \mbox{factoring: distributive law} \\
\end{array}
\]
A parlor trick has become a mathematical demonstration!
\qed
\end{proof}


\section{Polynomials and Their Roots}
\label{sec:polynolmials}

\index{algebraic function} \index{function!algebraic}
A function is {\it algebraic}, i.e., is an {\it algebraic function},
if it is formed from {\it variables} and {\it numbers} using the
algebraic operations of addition/subtraction and
multiplication/division. 

\subsection{Constructing a polynomial}
\label{sec:make-a-poly}
\index{algebraic function!polynomial}

The most basic algebraic function is a {\it polynomial}. 
\index{algebraic function!polynomial}
\index{function!algebraic!polynomial} \index{polynomial}
A polynomial on the $k$ {\it variables} \index{polynomial!variables}
in the set $\{v_1, v_2, \ldots, v_k\}$ is any function that can be
written as a sum of {\it monomials}, \index{algebraic function!monomial}
\index{function!algebraic!monomial} \index{monomial}
each of the form
\[ a_{\langle c_1, i_1 \rangle, \langle c_2, i_2 \rangle, \ldots 
\langle c_k, i_k \rangle} 
v_1^{c_1} \cdot v_2^{c_2} \cdots v_k^{c_k}.
\]
In this expression: the subscripted symbols of the form $a_z$ denote
numbers that are called the {\it coefficients}
\index{polynomial!coefficients} \index{monomial!coefficient} of the
monomial or polynomial.  The symbols $c_i$ are {\em nonnegative
  integers} that are the {\it powers} \index{polynomial!powers}
\index{monomial!powers} to which the variables are raised.  Here, for
illustration, is a specific small polynomial, with integer
coefficients, on the variables $x_1$ and $x_2$.  (By convention a
variable does not appear when its power is $0$.)
\[ 3 x_1^7 \cdot x_2^{19} \ + \ x_1^3 \cdot x_2 \ + \ 45 x_2^{9} \ + \ 
7 x_1^{11} \cdot x_2^{5}.
\]
When $k=1$, the polynomial is said to be {\it univariate} (a Latinate
form of ``having a single variable''); \index{polynomial!univariate}
when $k=2$, the polynomial is said to be {\it bivariate} (a Latinate
form of ``having two variables'').  Much of our interest will be in
univariate polynomials; see Section~\ref{sec:univariate-polynomials}.

A problem of particular interest when discussing a polynomial $P(x_1,
x_2, \ldots, x_k)$ is to discover vectors of values of the variables
$\{x_i\}$ that cause $P$ to {\em vanish}, i.e., evaluate to $0$.  Each
such vector, $\langle r_1, r_2, \ldots, r_k)$ is called a {\it root}
\index{polynomial!root} of $P$.  As one simple example, the roots of
the polynomial
\[ P(x,y) \ \ = \ \ x^2 \ - \ 2xy \ + \ y^2 \]
are precisely the $2$-place vectors (i.e., ordered pairs)
$\langle i,i \rangle$, i.e., ordered pairs whose first and second
components are identical, $x=y$.

The problem of finding the roots of polynomials has garnered
tremendous attention for centuries, both for its practical
applications and its theoretical implications.  Historically, two of
the major problems regarding roots of polynomials are:
\begin{itemize}
\item
Find all roots of a given polynomial.

Most of the studies of this problem are found in algorithmic
settings---courses, books, software.  Yet, there are many valuable
mathematical lessons to be learned under the aegis of this problem.
We discuss this problem for univariate polynomials in
Section~\ref{sec:univariate-polynomials}.

\item
{\em Decide} whether a given multivariate polynomial has any
integer roots.

This problem is often found under the rubric of {\it Diophantine
  analysis}, so named in honor of the Greek mathematician Diophantus
of Alexandria, \index{Diophantus of Alexandria} who has been called
the father of algebra for his seminal studies of the process of
equation-solving.  By its very nature---seeking a ``YES''-``NO''
answer rather than an actual root-finding solution---this problem has
been studied largely in theoretical or mathematical settings.  While
most work on this subject uses advanced techniques, hence is beyond
the scope of the current text, there is one result in this domain
whose underlying message is so profound that we at least tell its
``story'' in Section~\ref{sec:Hilberts-Tenth}.
\end{itemize}

\subsection{Univariate Polynomials and Their Roots}
\label{sec:univariate-polynomials}
\index{polynomials!univariate}

We focus throughout on univariate polynomials with complex
coefficients.  We leave to our algorithmic siblings the practicalities
of actually computing the roots of a given univariate polynomial.  We
are concerned with using mathematics to elucidate the sturcture of
solutions.  We discuss two topics with strong lessons for the endeavor
of doing mathematics.  Section~\ref{sec:fund-thm-algebra} presents a
number of results about the $d$ complex roots that every degree-$d$
polynomial has---highlighting the {\it Fundamental Theorem of
  Algebra}, the storied result that verifies the existence of all
these roots.  At the other end of the spectrum,
Section~\ref{sec:poly-by-radical} discusses the quest for ``simple''
formulas for roots, in terms of algebraic operations and
\index{radical} {\it radicals}---square roots, cube roots, etc.  We
will note that such formulas are readily calculated for degree-$2$
({\it quadratic}) polynomials, \index{quadratic polynomials} arduously
calculated for degree-$3$ ({\it cubic}) \index{cubic polynomials}
polynomials, computable only in theory for degree-$4$ ({\it quartic})
\index{quartic polynomials} polynomials---and {\em nonexistent} for
polynomials of degree $5$ and higher.


\subsubsection{The $d$ roots of a degree-$d$ polynomial}
\label{sec:fund-thm-algebra}

When we discussed the history of our number system, in
Section~\ref{sec:numbers}, we remarked that each step in the
progression from the integers ($\Z$) to the rational numbers ($\Q$),
to the real numbers ($\R$), and finally to the complex numbers ($\C$)
was motivated by a perceived deficiency in the number system up to
that point.  We then commented that the complex numbers were the
culmination of this process, in that they were {\it algebraically
  complete}. \index{algebraically complete}  We promised at that time
to clarify the meaning of that term---and the time has come to do
that.  The ``algebraic completeness'' of the complex numbers resides
in the fact that:

{\em Polynomials with complex coefficients can always be solved.}

\noindent
The common way of stating this powerful property is via the storied
theorem known as {\it The Fundamental Theorem of Algebra}.

\index{The Fundamental Theorem of Algebra}
\begin{theorem}[The Fundamental Theorem of Algebra]
\label{thm:fund-thm-algebra}
Every degree-$d$ univariate polynomial with complex coefficients has
$d$ complex roots.
\end{theorem}

Despite its rather simple form, Theorem~\ref{thm:fund-thm-algebra}
resisted formal proof for literally centuries, resisting the proof
attempts of mathematical luminaries such as Fermat, 
\index{Fermat, Pierre de} Euler, \index{Euler, Leonhard} Lagrange,
\index{Lagrange, Joseph-Louis} and Laplace.
\index{Laplace, Pierre-Simon (marquis de)}
The theorem was finally proved in the early 19th century by the
amateur (!)~French mathematician Jean-Robert Argand.
\index{Argand, Jean-Robert}
Argand publicized his proof in 1806, \cite{Argand} but the proof
became well-known only when it appeared in the famous {\it Cours
  d'analyse} \cite{Cauchy21} of Augustin-Louis Cauchy,
\index{Cauchy, Augustin-Louis}
whose name we have encountered in earlier sections.

\medskip

A few cultural/historical comments are in order.
\begin{itemize}
\item
The many failed attempts at proving Theorem~\ref{thm:fund-thm-algebra}
should not be viewed as casting shadows over the luster of any of the
greats who failed.  In most of the cases, the failures pointed out new
mathematics that has yet to be developed.  Such is the trajectory of
mathematics and the sciences!
\item
Despite the word ``fundamental'' in the name of
Theorem~\ref{thm:fund-thm-algebra}, the result is no longer viewed as
{\em the} fundamental theorem of algebra.  The field of algebra has
grown in a variety of directions in the past two centuries, so the
{\em Theory of Equations}, which was largely coextensive with the
field of algebra into the 19th century is presently just one branch of
the field.
\item
Despite the word ``algebra'' in the name of
Theorem~\ref{thm:fund-thm-algebra}, there does not yet exist an {\em
  algebraic} proof of the theorem: Some input from some other area of
mathematics enters every known proof in some essential
way.\footnote{One of the authors reports learning a proof that
  borrowed results from topology during his undergraduate days.}
\item
The march toward Theorem~\ref{thm:fund-thm-algebra} took literally
millennia.  Among the luminaries who made major contributions along
the way were the following pioneers in the Theory of (Solving)
Equations:
  \begin{itemize}
  \item
{\it Diophantus of Alexandria}. \index{Diophantus of Alexandria} 
He authored a series of books known collectively as {\it Arithmetica},
\index{Diophantus of Alexandria!{\it Arithmetica}}
which expounded the basics of a theory of solving equations.  These
books earned Diophantus the name ``father of algebra''.
\index{Diophantus of Alexandria!father of algebra}
   \item
{\it Muhammad ibn Musa al-Khwarizmi}.
\index{Al-Khwarizmi!Muhammad ibn Musa al-Khwarizmi}
Known better as {\em Al-Khwarizmi}, this Persian mathematician and
scholar of the $9$th century lent his name to the modern term {\it
  algorithm}.\index{Al-Khwarizmi!eponym of ``algorithm''} His
extensive writings, especially the book \cite{Al-Khwarizmi},
introduced into Europe both Hindu-Arabic numerals
\index{Al-Khwarizmi!Hindu-Arabic numerals} and the elements of what
was known in his day of the field of algebra---particularly the
solution of equations.\footnote{The important role of the Middle East
  in the development of mathematics is testified to eloquently by the
  origin of the word ``algebra''.  According to the {\it Oxford
    English Dictionary}, this word comes from the Arabic ``{\it
    al-jabr}'', which literally means ``reunion of broken
  parts''. \index{algebra!etymology of the word}}
   \item
{\it Ren\'{e} Descartes}. \index{Descartes, Ren\'{e}}
The $17$th-century mathematican and philosopher Descartes is credited
with establishing the mathematical notation that we use to this day.
The reader should not minimize the importance of notation until trying
to perform arithmetic using Roman numerals.  At a more abstract level,
Descartes's invention of {\em Analytical Geometry}
\index{Descartes, Ren\'{e}!inventor of Analytical Geometry}
enabled the use of geometric concepts and techniques in algebra.  Both
of these contributions had incalculable value in the history leading
up to Theorem~\ref{thm:fund-thm-algebra}.
   \end{itemize}
\end{itemize}

\medskip

Even though the earliest proofs of Theorem~\ref{thm:fund-thm-algebra}
were not {\em constructive:} They did not provide a roadmap for
finding the roots of a given polynomial.  There currently do exist
proofs of the Theorem that are constructive, in the following sense.
Given a degree-$d$ polynomial $P(x)$, these proofs determine a disk
$D$ in two-dimensional space which contains all $d$ roots of $P(x)$.
How might such a disk emerge from a mathematical argument?  Here is a
{\em very simple illustration} which focuses on a {\em very special
  type of polynomial}.

Let us be given a degree-$d$ polynomial with real coefficients:
\[ P(x) \ \ = \ \ a_d x^d \ + \ a_{d-1} x^{d-1} \ + \ a_{d-2} x^{d-2}
\ + \cdots + \ a_1 x \ + \ a_0
\]
Since we are interested only studying the roots of $P(x)$, we lose no
generality by insisting that $P(x)$ is {\em monic},
\index{polynomial!monic} i.e., has leading coefficient $a_d = 1$.
(The reader should verify this assertion.)  Once we do this, we can
rewrite $P(x)$ in the following form
\[ P(x) \ \ = \ \ 
x^d \cdot \left( 1 \ + \ \frac{a_{d-1}}{x} \ + \ \frac{a_{d-2}}{x^2}
\ + \cdots + \ \frac{a_1}{x^{d-1}} \ + \ \frac{a_0}{x^d} \right)
\]
The important benefit of this rewriting is that it makes the following
inequality totally clear.
\[ P(x) \ \ > \ \ 0 \ \ \ \ \ \mbox{ for all } \ \ x >
d \cdot \left(|a_{d-1}| \ + \ |a_{d-2}|
\ + \cdots + \ |a_1| \ + \ |a_0| \right)
\]
This means that all real roots of $P(x)$, if there are any, lie within
the region $x \ \leq \ d \cdot \left(|a_{d-1}| \ + \ |a_{d-2}|
\ + \cdots + \ |a_1| \ + \ |a_0| \right)$.

This is just one simple example, but it does illustrate that one can
sometimes find bounded regions within which all of $P(x)$'s roots must
lie.  Many texts (and research papers) deal with the general problem
of computing the roots of polynomials; see, e.g., \cite{MacDuffee}.


\subsubsection{Solving polynomials by radicals}
\label{sec:poly-by-radical}

The problem of {\it solving} arbitrary degree-$d$ polynomials---i.e.,
of discovering their $d$ roots, as promised by
Theorem~\ref{thm:fund-thm-algebra}---is computationally very complex,
even for moderately low degrees.  (This assertion can be made
mathematically precise, but the required notions are beyond the scope
of this text.)  For univariate polynomials of low degree, there do
exist computationally feasible root-finding algorithms.  Indeed, for
polynomials of {\em very} low degree---specifically, degrees $2$, $3$
and $4$---there actually exist ``simple'' {\em formulas} that specify
the polynomial's roots.  The word ``simple'' is used in a technical
sense here: it refers to a formula that can be constructed using the
following algebraic operations: adding/subtracting two quantities,
multiplying/dividing two quantities, and raising a quantity to a
rational power.  Because the last of these operations is often
expressed by using a {\em radical sign}, \index{radical sign} rather
than an exponent---as when we write ``$\sqrt{x}$'' for
``$x^{1/2}$''---these formulas are often referred to as {\it solution
  by radicals}.  \index{polynomial!solution by radicals}
\index{solution by radicals}  

The remainder of this section is devoted to deriving the {\em
  quadratic} formula---the one that specifies the roots of any
degree-$2$ {\it (quadratic)} \index{polynomials!quadratic}
polynomial---and the {\em cubic} formula---the one that specifies the
roots of any degree-$3$ {\it (cubic)} polynomial.
\index{polynomials!cubic} We shall observe that the cubic
formula is so onerous calculationally that it is seldom actually
written out; it is instead specified {\em algorithmically}.  The {\em
  quartic} formula---the one that specifies the roots of any
degree-$4$ {\it (quartic)} \index{polynomials!quartic} polynomial---is
so complex that it is virtually never written out.  The courageous
reader can attack the quartic formula as an exercise, using the
conceptual techniques we derive here.

It is useless to try to solve polynomials of degrees $> 4$ by
radicals.  In the early 19th century, a (mathematically brilliant)
French teenager, Evariste Galois \index{Galois, Evariste}, proved that
one cannot solve the degree-$5$ {\it (quintic)}
\index{polynomials!quintic} polynomial by radicals, no matter how much
abstruse computation one is willing to do!  Galois achieved this
result by developing a mathematical theory, which has since been named
for him. \index{Galois theory} Tragically for the world of
mathematics, Galois was killed in a duel just two years after
announcing his theory via a memoir submitted to the Paris Academy of
Sciences.

\bigskip

\noindent
On to solving low-degree polynomials:

\medskip

\noindent {\it i. Solving quadratic polynomials by radicals}

\noindent
We derive the {\it quadratic formula}, which solves an arbitrary
quadratic polynomial \index{Polynomial!quadratic!generic} with real
coefficients: \index{polynomial!solving a quadratic polynomial} 
\begin{equation}
\label{eq:generic-quadratic-1}
P(x) \ = \  ax^2 \ + \ bx \ + \ c \ \ \mbox{  where  } \ b,c \in \R;
\ a \in \R \setminus \{0\}
\end{equation}
While the formula and its derivation are specialized to the structure
of quadratic polynomials, several aspects of the derivation can be
extrapolated to polynomials of higher degree.  The formula that we
derive is announced in the following proposition.  \index{quadratic
  formula}

\begin{prop}
\label{thm:quadratic-formula}
The two roots, $x_1$ and $x_2$, of the generic quadratic polynomial
(\ref{eq:generic-quadratic-1}) are:
\begin{eqnarray}
\nonumber
x_1 & = & \frac{-b \ + \ \sqrt{b^2 -4ac}}{2a} \\
\label{eq:generic-quadratic-4}
    &   & \\
\nonumber
x_2 & = & \frac{-b \ - \ \sqrt{b^2 -4ac}}{2a}.
\end{eqnarray}
\end{prop}


\begin{proof}
We find the roots of $P(x)$ by solving the polynomial equation $P(x) =
0$.  We simplify our task by dividing both sides of this equation by
$a$; easily, this does not impact the two solutions we seek.  We
thereby reduce the root-finding problem to the solution of the
equation
\begin{equation}
\label{eq:generic-quadratic-2}
x^2 \ + \ \frac{b}{a} x \ = \ - \frac{c}{a}.
\end{equation}

The technique of {\it completing the square}
\index{polynomial!quadratic!completing the square} gives us an easy
path to solving equation (\ref{eq:generic-quadratic-2}).  This
technique involves adding to both sides of the equation a
variable-free expression $E$ that turns the lefthand expression into a
perfect square.  In the current instance, the expression
\[ E \ = \ \frac{b^2}{4a^2} \]
does the job, because
\[
x^2 \ + \ \frac{b}{a} x \ + \ E \ \ = \ \
x^2 \ + \ \frac{b}{a} x \ + \ \frac{b^2}{4a^2}
   \ \ = \ \ \left( x \ + \ \frac{b}{2a} \right)^2
\]
We have thereby converted equation (\ref{eq:generic-quadratic-2}) to
the equation
\begin{equation}
\label{eq:generic-quadratic-3}
\left( x \ + \ \frac{b}{2a} \right)^2
  \ = \ \frac{b^2}{4a^2} - \frac{c}{a}
  \ = \ \frac{b^2 - 4ac}{4a^2}.
\end{equation}
Elementary calculation on equation (\ref{eq:generic-quadratic-3})
identifies $P(x)$'s two roots as the values $x_1$ and $x_2$ specified
in (\ref{eq:generic-quadratic-4}).  \qed
\end{proof}

Using a common shorthand, the expressions for $x_1$ and $x_2$ in
(\ref{eq:generic-quadratic-4}) are often abbreviated by using the
operator $\pm$, for ``plus or minus''.  \index{$\pm$: plus or minus}
The quadratic formula is then written:
\[
x \ = \  \frac{1}{2a} \left( -b \ \pm \ \sqrt{b^2 \ - \ 4ac} \right).
\]


\smallskip

The reader can verify that our proof essentially proceeds by
replacing $P(x)$'s variable $x$ with the variable
\[ u \ = \ x + \frac{b}{2a}. \]
This replacement streamlines the process of completing the square and
finding the solutions $x_1$ and $x_2$.  We presented the more
elementary proof to let the reader see the solution process proceed
step by step.  As we turn now to the clerically more complex solution
of the cubic polynomial, we get around some of the calculational
complexity by employing the variable-substitution strategem.

\bigskip

\noindent {\it ii. Solving cubic polynomials by radicals}

\noindent
We derive a formula that {\it solves} an arbitrary cubic polynomial
\index{Polynomial!cubic!generic} with real coefficients:
\index{polynomial!solving a cubic polynomial} 
\begin{equation}
\label{eq:generic-cubic-1}
P(x) \ = \  ax^3 \ + \ bx^2 \ + \ cx \ + \ d \ \ \mbox{  where  }
\ b,c, d  \in \R;\ a \in \R \setminus \{0\}
\end{equation}
Although the so-called {\em cubic formula} that we derive, and its
derivation, are specialized to the structure of degree-$3$
polynomials, the reader will recognize several aspects of the
derivation that are akin to our derivation of the quadratic formula.
Because the cubic formula is so complex in form, we present the
formula's proof/derivation {\em before} presenting the formula.

\begin{proof} {\it (Derivation of the cubic formula)}

\noindent {\it Step 1.} Convert the problem to solving a
cubic polynomial that is {\em monic}, \index{polynomial!monic}
i.e., has leading coefficient $1$.


\noindent
We convert the generic cubic polynomial $P(x)$ of
(\ref{eq:generic-cubic-1}) to the monic cubic polynomial that has the
same roots as $P(x)$.
\begin{eqnarray}
\label{eq:generic-cubic-2}
P^{(1)}(x) & = &  x^3 \ + \ Bx^2 \ + \ Cx \ + \ D \\
\nonumber
\mbox{where} & &
B \ = \ \frac{b}{a}; \ \ \
C \ = \ \frac{c}{a}; \ \ \
D \ = \ \frac{d}{a}
\end{eqnarray}
Monic polynomials lead to somewhat simpler calculation

It should be clear that the polynomials $P(x)$ and $P^{(1)}(x)$ have
the same roots.  {\Arny We should include this as a simple exercise.}

\medskip

\noindent {\it Step 2.} Convert $P^{(1)}(x)$ to {\em reduced form}.
\index{polynomial!cubic!monic!reduced form}

\noindent
When we make the transformation of variable
\begin{equation}
\label{eq:cubic-substitute-y-for-x} 
y \ = \ x \ + \ \frac{B}{3}
\end{equation}
in (\ref{eq:generic-cubic-2}), we convert polynomial $P^{(1)}(x)$ in
variable $x$ into the polynomial following in variable $y$:

\begin{eqnarray}
\nonumber
P^{(2)}(y) & = &  \left(y - \frac{B}{3} \right)^3
 \ + \ B \left(y - \frac{B}{3} \right)^2
 \ + \ C \left(y - \frac{B}{3} \right) \ + \ D \\
%\nonumber
%           & = &
%y^3 \ - \ B y^2 \ + \ \frac{B^2}{9} y \ - \ \frac{B^3}{27}
%\ + \ By^2 \ - \ \frac{2B^2}{3} y \ + \ \frac{B^3}{9} 
%\ + \ Cy \ - \ \frac{BC}{3}  \ + \ D \\
\label{eq:generic-cubic-3}
           & = &
y^3 \ + \
\left( \frac{B^2}{9} \ - \ \frac{2B^2}{3} \ + \ C  \right) y
\ + \ \left( \frac{2 B^3}{27}  \ - \ \frac{BC}{3}  \ + \ D \right)
\end{eqnarray}
Cubic polynomials that lack a quadratic term---i.e., a term involving
$y^2$---are said to be in {\it reduced form}.  For simplicity, we
rewrite $P^{(2)}(y)$, which clearly is in reduced form, as
\begin{eqnarray}
\label{eq:generic-cubic-4}
P^{(2)}(y) & = & y^3 \ + \ E y \ + \ F \\
\nonumber
\mbox{where} & & \\
\nonumber
E & = & \frac{B^2}{9} \ - \ \frac{2B^2}{3} \ + \ C \\
\nonumber
F & = & \frac{2 B^3}{27}  \ - \ \frac{BC}{3}  \ + \ D
\end{eqnarray}


\medskip

\noindent {\it Step 3.} Convert $P^{(2)}(y)$ to its {\em associated}
quadratic polynomial.

\noindent
We next apply a transformation attributed to the $16$th-century French
mathematician Fran\c{c}ois Vi\`{e}te \index{Vi\`{e}te, Fran\c{c}ois}
(often referred to by his Latinized name, Franciscus Vieta)
\index{Vieta, Franciscus}; see \cite{Hazewinkel}.  Vieta's
transformation converts $P^{(2)}(y)$ to a quadratic polynomial by means
of the variable-substitution
\begin{equation}
\label{eq:cubic-substitute-z-for-y}
y \ = \ z \ - \ \frac{E}{3z}
\end{equation}
in (\ref{eq:generic-cubic-4}).  We thereby obtain (after calculations
involving several cancellations) an expression
\begin{eqnarray}
\nonumber
P^{(3)}(z) & = & \left( z \ - \ \frac{E}{3z} \right)^3
\ + \ E \left(z \ - \ \frac{E}{3z} \right) \ + \ F \\
\label{eq:generic-cubic-5}
  & = &
z^3 \ - \ \frac{E^3}{27z^3}  \ + \ F
\end{eqnarray}
Clearly, $P^{(3)}(z)$ is not a polynomial in $z$, but it is a valuable
stepping stone because the function $P^{(3)}(z)$ vanishes---i.e.,
$P^{(3)}(z) = 0$---precisely when the following polynomial (in the
``variable'' $z^3$) vanishes.
\[ P^{(4)}(z^3) \ = \ (z^3)^2 \ + \ (z^3) F \ - \ \frac{E^3}{27}. \]
We wrote both instances of $z^3$ in the expression for $P^{(4)}(z^3)$
within parentheses to facilitate the view of $P^{(4)}(z^3)$ as a
(quadratic) polynomial in the ``variable'' $z^3$.  The quadratic
formula (\ref{eq:generic-quadratic-4}) provides us with two roots for
$P^{(4)}$, which we express as the following two values for $z^3$
(abbreviated via the shorthand operator $\pm$).
 \index{$\pm$: plus or minus}
\begin{equation}
\label{eq:solve-cubic-for-z3}
(z^3) \ = \
%\frac{1}{2} \left(-F \ \pm \ \sqrt{F^2 + 4 \frac{E^3}{27}} \right) \ = \ 
- \frac{F}{2} \ \pm \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\end{equation}

We can now derive all solutions for the variable $z$ in equation
(\ref{eq:solve-cubic-for-z3}) via back-substitution in transformation
(\ref{eq:cubic-substitute-z-for-y}).  But completing this derivation
requires a bit of background.

Theorem~\ref{thm:fund-thm-algebra} assures us that the polynomial
$P^{(2)}(z)$ has three roots.  In order to compute these roots, we
invoke a truly remarkable result that is known as \index{Euler's
  formula} {\it Euler's formula}, in honor of its discoverer, the
much-traveled $18$th-century mathematician Leonhard
Euler. \index{Euler, Leonhard} This result/formula exposes a
fundamental relationship among:
\begin{itemize}
\item
the imaginary unit \index{$i$: the imaginary unit} $i$;
\item
the ratio of the circumference of a circle to its radius,
$\pi \ = \ 3.141592653 \cdots$;
\index{$\pi$: the ratio of the circumference of a circle to its radius}
\item
the base of natural logarithms, Euler's constant $e \ = \ 2.718281828 \cdots$.
\index{Euler's constant}\index{$e$:the base of natural logarithms}
\end{itemize}

\begin{theorem}[Euler's formula]
\label{thm:Euler's-formula}
\[ e^{i \pi} \ = \ -1. \]
\end{theorem}

Back to equation (\ref{eq:solve-cubic-for-z3}):
Theorem~\ref{thm:fund-thm-algebra} tells us that within the complex
number system $\C$, the cubic polynomial
\[ u^3 \ - \ 1 \]
has three distinct roots.  These numbers are known as the {\em
  primitive $3$rd roots of unity} \index{primitive $3$rd roots of
  unity} and are denoted $\omega^0$, $\omega^1$, and $\omega^2$.
Using Theorem~\ref{thm:Euler's-formula}, we can provide explicit
values for these numbers, namely:
\[ \omega^0 \ = \ 1; \ \ \ \ \
\omega^1 \ = \ e^{2i \pi/3}; \ \ \ \ \
\omega^2 \ = \ e^{4i \pi/3}.
\]

\medskip
When we unite the abbreviated double equation
(\ref{eq:solve-cubic-for-z3}) for $z^3$ with Euler's formula
(Theorem~\ref{thm:Euler's-formula}), we discover {\em six} solutions
for the variable $z$, namely, {\small
\begin{equation}
\label{eq:cubic-solution-1}
\begin{array}{ccrrrrrccr}
z_1 & = &
{\displaystyle
\omega^0 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3} 
}
  & & & & &
z_2 & = &
{\displaystyle
\omega^0 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
} \\
z_3 & = &
{\displaystyle
\omega^1 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
  & & & & & 
z_4 & = &
{\displaystyle
\omega^1 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
} \\
z_5 & = &
{\displaystyle
\omega^2 \cdot
\left( -\frac{F}{2} \ + \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
  & & & & &
z_6 & = &
{\displaystyle
\omega^2 \cdot
\left( -\frac{F}{2} \ - \ \sqrt{\frac{F^2}{4} + \frac{E^3}{27}}
\right)^{1/3}
}
\end{array}
\end{equation}
}

The algorithmically interesting portion of the process of solving
cubics by radicals is now complete.  The remainder of the process
consists of ``reversing'' the two transformations,
(\ref{eq:cubic-substitute-z-for-y}) and
(\ref{eq:cubic-substitute-y-for-x}), that have taken us from the
problem of solving a polynomial in $x$ to the problem of solving a
polynomial in $z$.  The calculations that embody this reverse
transformation are onerous when solved symbolically, so we make do
with some exercises in which the reader will solve numerical
instances.  The most interesting and noteworthy feature of these
exercise will be the observation of ``collapsing'' of intermediate
expressions, whose impact is to leave us with with only {\em three}
solutions for $x$---which isthe number promised by
Theorem~\ref{thm:fund-thm-algebra}---rather than the six solutions
that the array (\ref{eq:cubic-solution-1}) of $z$-values would lead
one to expect.

While the promise of a visually appealing cubic analogue of the
quadratic formula (\ref{eq:generic-quadratic-4}) is appealing, the
actual cubic formula is so complex visually that it offers no
important insights.  The curious reader can find renditions of the
formulas on the web.  \qed
\end{proof}

{\Arny
I think that solving specific examples of cubics via radicals, although specialized, does
involve useful skills for manipulating polynomials.  Therefore, I
propose to stop the general case here and leave a few exercises.  WHAT DO YOU THINK?}



{\Arny Because your geometric solution below involves just a single
  special quadratic --- and one that seems not to have other special
  interest --- I propose that we include it in some form of ENRICHMENT
  section.  }

{\ignore {\Denis It is interesting here to stress how hard it was to solve such equations. Algebra raised only in the second half of the last millenium.
However, there are several nice examples of geometrical solutions.
One of the oldest comes from babylonians in the 18th century BC.
The numeral system was in base 60, and the problem was to determine the length of the side of a square which was part of a larger rectangle.
The following figure details the process.}}
{\Denis Add here a small introduction about solving polynomial of degree 2 from El Kwharizmi, or remove it and put it as an exercice.}
\begin{figure}[htb]
\begin{center}
       \includegraphics[scale=0.4]{FiguresArithmetic/tabletteMesopotamie}
\caption{Solving $x^2 + x = 45$.
The idea of the proof is to represent the left hand side by the square $x^2$ beside a rectangle $60 \times x$.
Then, split the right rectangle into two equal parts and move one part a the bottom of the left square.
The final figure shows the whole square whose surface is equal to $45$ plus the surface of the white square
whose surface is equal to $30 \times 30$.
In base $60$, this is $15$. 
$45+15 = 60$, thus, the big square is the unit square, its side is $60$.
Thus, the length of the initial square is equal to $60-30=30$.}
\label{fig:equationBabillon}
\end{center}
\end{figure}


\subsection{$\oplus$ Integer Roots of Polynomials: Hilbert's $10$th Problem}
\label{sec:Hilberts-Tenth}
\index{Hilbert's Tenth Problem}

This section is devoted to describing a true milestone in the history
of mathematics, of logic, and of computing.  Even though the
mathematical details necessary to fully describe the analyses that lead
to this blockbuster result go way beyond the scope of an introductory
text, this result has cultural lessons that make even its story
valuable.  Moreover, once one {\em really} understands the result, one
recognizes that the material in Sections~\ref{sec:Q-Z-R-cardinality}
and~\ref{sec:pairing}---both of which the reader {\em does} have
access to---supply essential mathematical underpinnings for the result.

In 1900, the influential German mathematician David Hilbert
\index{Hilbert, David} set forth $23$ problems to serve as a ``bucket
list''\footnote{The ``bucket'' is not yet empty.  A number of Hilbert's
  original $23$ are yet to be solved.}~for the world mathematics
community at the dawn of the twentieth century.\footnote{Hilbert's
  list was published in 1902 \cite{Hilbert02}, translated into English
  by Mary Frances Winston Newson. \index{Newson, Mary Frances Winston}
  Newson was the first American woman to receive a doctorate from the
  University of G\"{o}ttingen, which was probably the world's premier
  university for mathematical research in the late 19th and early 20th
  centuries.}  Of Hilbert's $23$ problems, the $10$th stands out
within the world of computing. \index{Hilbert's Tenth Problem} Stated
in modern terminology, with a modern perspective, the Problem can be
stated as follows.
\begin{description}
\item[{\bf Hilbert's Tenth Problem}]
{\it Develop an algorithm that will decide---via a ``YES'' or ``NO''
  answer---whether a given multivariate polynomial with integer
  coefficients has any integer roots.  }
\end{description}
This problem attracted the attentions of many of the best mathematical
minds of the $20$th century.  Building on the work of American
mathematicians Martin Davis \index{Davis, Martin} and Julia Robinson
\index{Robinson, Julia}, the Problem was finally resolved by the Russian
then-graduate student Yuri Matiyasevich \index{Matiyasevich, Yuri}.

\begin{theorem}[Hilbert's Tenth Problem, Resolved]
\label{thm:Hilberts-10th}
There is no algorithm that, when presented with an arbitrary
multivariate polynomial $P$ with integer coefficients, will correctly
determine whether $P$ has any integer roots.
\end{theorem}

The long history leading to Matiyasevich's proof of
Theorem~\ref{thm:Hilberts-10th} is documented in
\cite{Davis73,DavisH73,DavisMR76,Matiyasevich93}.  Although the
mathematics needed even to understand Hilbert's Tenth Problem and its
resolution are beyond an introductory text, a few words are in order
about a fundamental way in which 20th-century mathematics turned the
tables on all of mathematics to that point in history.

Reaching back just a bit, it is worth pondering that one of the main
movements in mathematics, beginning in the 19th century, had as its
goal to codify the notion of ``rigorous proof''.
\bigskip

\noindent \fbox{
\begin{minipage}{0.95\textwidth}
``Proofs'' predating the 19th century very often fell far below the
  standard that we now expect even of students.  Indeed, many of what
  we persist in calling ``proofs'' from early mathematicians were
  actually wrong---even though the theorems were totally correct.
\end{minipage}
}
\bigskip

\noindent
One of Hilbert's goals in 1900 was to have mathematicians rigorously
prove that the notions of ``Truth'' and ``Provability'' coincided.
This hope was, inn fact, the first of Hilbert's dreams to be dashed,
when, in the 1930s, Austrian expatriate Kurt G\"{o}del
\index{G\"{o}del, Kurt} proved his celebrated {\it Incompleteness
  Theorems}, \index{G\"{o}del's Incompleteness Theorems}
which---stated {\em very} informally!---asserted that in any
reasonable mathematical system, there would always be ``True''
statements that could not be proved \cite{Goedel31}.  Building upon
G\"{o}del's insights, English mathematician Alan Turing
\index{Turing, Alan} soon thereafter proved---again, stated {\em very}
informally!---that no matter how much ``smarts'' we try to build into
a digital computer, there would always be functions---even $0$-$1$
valued functions that the computer could not compute \cite{Turing36}.
Theorem~\ref{thm:Hilberts-10th} can be reworded to assert that the
integer-root-finding behavior called for in Hilbert's Tenth Problem is
precisely one of Turing's {\it uncomputable} \index{uncomputable
  function} functions!\footnote{The connections between G\"{o}del's
  results and Turing's, and the manner in which both authors' work
  connects to the work of German logician Georg Cantor 
\index{Cantor, Georg} on infinite sets of numbers,
\cite{Cantor74,Cantor78} are described in \cite{Rosenberg09} using
consistent terminology to discuss all three authors' work.}


\subsection{Bivariate Polynomials: The Binomial Theorem}
\label{sec:bivariate-polynomials}
\label{sec:Binomial-thm}
\index{polynomials!bivariate}
\index{The Binomial Theorem}

{\em Bivariate} polynomials are just polynomials each of whose
summands/terms has the form $a x^b y^c$: $x$ and $y$ are the polynomial's
variables; $a$ is the term's coefficient; $b$ and $c$ are the
respective powers of $x$ and $y$ in this term.

Probably the simplest bivariate polynomials are the ones in the
following family.
\begin{equation}
\label{eq:binomial-polys}
\mbox{For } \ n \in \N^+, \ \ \
P_n(x,y) \ \eqdef \ (x+y)^n.
\end{equation}
There are lessons to be learned from the structure of these
polynomials, so let us begin to expand them using the arithmetic
techniques we have learned earlier.
\begin{eqnarray*}
P_1(x,y) \ = \
(x+y)^1 & = & x+y  \\
P_2(x,y) \ = \
(x+y)^2 & = & (x+y) \cdot (x+y) \\
        & = & x^2 + 2xy + y^2 \\
P_3(x,y) \ = \
(x+y)^3 & = & (x+y) \cdot (x^2 + 2xy + y^2) \\
   & = & (x^3 + 2x^2y +  xy^2) + (x^2y + 2xy^2 + y^3) \\
   & = & x^3 + 3x^2y + 3xy^2 + y^3  
\end{eqnarray*}

Let us stop to review what we are seeing.  We have remarked before
that doing mathematics can sometimes involve a wonderfully exciting
(quite sophisticated) pattern-matching game.  So, let us pattern-match!
\begin{enumerate}
\item
The coefficients of the expanded $P_1(x,y)$ are $\langle 1,1 \rangle$.
\item
The coefficients of the expanded $P_2(x,y)$ are $\langle 1,2,1 \rangle$.
\item
The coefficients of the expanded $P_3(x,y)$ are $\langle 1,3,3,1 \rangle$.
\end{enumerate}
There is a pattern emerging here.  Can you spot it?  Where have we
seen a pattern of tuples that begins in the same manner?  As a rather
broad hint, look at Fig.~\ref{fig:pascal-triangle}!  Could the
coefficients of each $P_n$ possibly be the successive binomial
coefficients
\[ {n \choose 0}, \ {n \choose 1}, \ \ldots, \ {n \choose {n-1}}, \ {n
  \choose n}
\]
Let us use induction to explore this possibility by expanding a
generic $P_n$ with symbolic ``dummy'' coefficients and see what this
says about $P_{n+1}$.  To this end, let $a_{n,n-r}$ denote the
coefficient of $x^{n-r} y^r$ in the expansion of $P_n(x,y)$.  Using
our symbolic coefficients $a_{n,k}$, we have
\begin{eqnarray*}
P_n(x,y) & = &
 x^n \ + \ a_{n,n-1} x^{n-1} y \ + \cdots + \\
         &   &
a_{n,n-r} x^{n-r} y^r \ + \ a_{n,n-r-1} x^{n-r-1} y^{r+1}
\ + \ a_{n,n-r-2} x^{n-r-2} y^{r+2} \\
         &   &
+ \cdots + \ a_{n,1} x y^{n-1} \ + \ y^n
\end{eqnarray*}
Continuing with this symbolic evaluation, we have:
\begin{eqnarray}
\nonumber
x \cdot P_n(x,y) & = &
 x^{n+1} \ + \ a_{n,n-1} x^n y \ + \cdots + \\
\nonumber
         &   &
a_{n,n-r} x^{n-r+1} y^r \ + \ a_{n,n-r-1} x^{n-r} y^{r+1}
\ + \ a_{n,n-r-2} x^{n-r-1} y^{r+2} \\
\label{eq:xPk}
         &   &
+ \cdots + \ a_{n,1} x^2 y^{n-1} \ + \ x y^n
\end{eqnarray}
and
\begin{eqnarray}
\nonumber
y \cdot P_n(x,y) & = &
 x^n y \ + \ a_{n,n-1} x^n y^2 \ + \cdots + \\
\nonumber
         &   &
a_{n,n-r} x^{n-r} y^{r+1} \ + \ a_{n,n-r-1} x^{n-r-1} y^{r+2}
\ + \ a_{n,n-r-2} x^{n-r-2} y^{r+3} \\
\label{eq:yPk}
         &   &
+ \cdots  + \ a_{n,1} x^2 y^n \ + \ y^{n+1}
\end{eqnarray}
Because
\[ P_{n+1}(x+y) \ = \ (x+y) \cdot P_n(x,y)
                \ = \ x \cdot P_n(x,y) \ + \ y \cdot P_n(x,y),
\]
the symbolic coefficient $a_{n-r+1,r}$ of $x^{n-r+1} y^r$ in
$P_{n+1}(x+y)$ is the sum of the following symbolic coefficients in
$P_n(x,y)$:
\begin{center}
$\bullet$
the coefficient $a_{n,n-r}$ of $x^{n-r}y^r$ \ \ \ \ \
and \ \ \ \ \
$\bullet$
the coefficient $a_{n,n-r+1}$ of $x^{n-r+1}y^{r-1}$
\end{center}
By induction, then, for all $n,r \in \N$ with $r \leq n$,
\[ a_{n,r} + a_{n,r+1} \ = \ a_{n+1,r+1} \]
Combining this equation with the observed initial conditions
\[ a_{1,0} \ = \ a_{1,1} \ = \ 1 \]
we see that each coefficient $a_{n,r}$ is actually the binomial
coefficient $\displaystyle {n \choose r}$.

The preceding observation is attributed to the renowned English
mathematician/physicist Isaac Newton \index{Newton, Isaac} and is
enshrined in Newton's famous {\it Binomial Theorem}.  In fact, the
calculations preceding the observation constitute a proof of this
seminal result.

\ignore{**********
so that, finally,
\begin{eqnarray*}
             & = &
 x^{k+1} \ + \ (k+1) x^k y \ + \ \cdots \ + \
   (a_{k,k-r-1} + a_{k,k-r}) x^{k-r} y^{r+1} \\
             &   & \ \ \ + \
   (a_{k,k-r-2} + a_{k,k-r-1}) x^{k-r-1} y^{r+2}
 \ + \ \cdots \ + \ (k+1) xy^k \ + \ y^{k+1} \\
      & = &
 x^{k+1} \ + \ (k+1) x^k y \ + \ \cdots \ + \
 a_{k+1,k-r} x^{k-r} y^{r+1} \\
            &    & \ \ \ + \  a_{k+1,k-r-1} x^{k-r-1} y^{r+2}
 \ + \ \cdots \ + \ (k+1) xy^k \ + \ y^{k+1}
\end{eqnarray*}
*******}

\begin{theorem}[The Binomial Theorem]
\label{thm:Binomial-theorem}
For all $n \in \N$,
\begin{equation}
\label{eq:binomial-theorem}
(x+y)^n \ \ = \ \
\sum_{i=0}^n \ \ {n \choose i} x^{n-i} y^i.
\end{equation}
\end{theorem}
\index{The Binomial Theorem!binomial coefficients}
\index{binomial coefficients!The Binomial Theorem}
\index{polynomials!bivariate!The Binomial Theorem}



\section{Exponential and Logarithmic Functions}
\label{sec:exponential+logarithm}

This section introduces the fundamentals of two extremely important
classes of functions which are functional inverses of each other, in
the following sense.  Functions $f$ and $g$ are {\it functional
  inverses}\index{functional inverse} of each other if for all
arguments $x$
\begin{equation}
\label{eq:functional-inverse}
f(g(x)) \ = \ x.
\end{equation}

\subsection{Basic definitions}

\subsubsection{Exponential functions}.\index{Exponential functions}

A function $f$ is {\it exponential} if there is a positive number $b$
such that, for all $x$,
\begin{equation}
\label{eq:exponential-defn}
f(x) \ = \ b^x.
\end{equation}
The number $b$ is the {\it base}\index{base of exponential}
%
of $f(x)$.  The basic arithmetic properties of exponential functions
are derivable from (\ref{eq:power-def}), so we leave these details to
the reader and turn immediately to the functional inverses of
exponential functions..

\subsubsection{Logarithmic functions}.\index{Logarithmic functions}
\label{sec:logarithmic-fns}

Given an integer $b >1$ (mnemonic for ``base''), the {\em base-$b$
  logarithm}\index{base-$b$ logarithm}
%
of a real number $a > 0$ is denoted $\log_b a$ and defined by the
equation\index{$\log_b a$: the base-$b$ logarithm of number $a$}
\begin{equation}
\label{eq:logarithm-defn}
a \ = \ b^{\log_b a}.
\end{equation}
Logarithms are partial functions: $\log_b a$ is not defined for
non-positive arguments.

The base $b = 2$ is so prominent in the contexts of computation theory
and information theory that we commonly invoke one of two special
notations for $\log_2 a$: (1) we often elide the base-$2$ subscript
and write $\log a$;\index{$\log(a)$: base-$2$ logarithm of number $a$}
(2) we employ the specialized notation $\ln a$\index{$\ln(a)$:
  base-$2$ logarithm of number $a$}.  Notationally:
\[ \log_2 a \ \eqdef \ \log a \ \eqdef \ \ln a \]

We leave to the reader the easy verification, from
(\ref{eq:logarithm-defn}), that the {\it base-$b$ logarithmic
  function}, defined by
\begin{equation}
\label{eq:log-function-defn}
f(x) \ = \ \log_b x
\end{equation}
is the functional inverse of the base-$b$ exponential function.

\subsection{Fun facts about exponentials and logarithms}

Definition (\ref{eq:logarithm-defn}) exposes and---even more
importantly---explains myriad facts about logarithms that we often
take for granted.

\begin{prop}
For any base $b >1$, for all numbers $x >0$, $y>0$,
\[ \log_b (x \cdot y) \ = \ \log_b x \ + \ \log_b y \]
\end{prop}

\begin{proof}
Definition (\ref{eq:logarithm-defn}) tells us that $x = b^{\log_b x}$
and $y = b^{\log_b y}$.  Therefore,
\[ x \cdot y \ = \ b^{\log_b x} \cdot b^{\log_b y} \ = \
b^{\log_b x \ + \ \log_b y}, \]
by the laws of powers.  Taking base-$b$ logarithms of the first and
last terms in the chain yields the claimed equation.
\qed
\end{proof}



Many students believe that the following result is a {\em convention}
rather than a consequence of the basic definitions.  {\em The logarithm
  of $1$ to any base is $0$.}

\begin{prop}
For any base $b >1$,
\[ \log_b 1 \ = \ 0 \]
\end{prop}

\begin{proof}
We note the following chain of equalities.
\[  b^{\log_b x} \ = \ b^{\log_b (x \cdot 1)} 
\ = \ b^{(\log_b x) + (\log_b 1)} 
\ = \ b^{\log_b x} \cdot b^{\log_b 1}
\]
Hence, $b^{\log_b 1} \ = \ 1$.  If $\log_b 1$ did not equal $0$, then
$b^{\log_b 1}$ would exceed $1$.  \qed
\end{proof}

\begin{prop}
For all bases $b > 1$ and all numbers $x, y$,
\[ x^{\log_b y} \ = \ y^{\log_b x} \]
\end{prop}

\begin{proof}
We invoke (\ref{eq:logarithm-defn}) twice to remark that
\[ \left[x^{\log_b y} \ = \ b^{(\log_b x) \cdot (\log_b y)}\right]
\ \ \mbox{ and } \ \ 
\left[y^{\log_b x}\ = \ b^{(\log_b y) \cdot (\log_b x)}\right] \]
The commutativity of addition completes the verification.  \qed
\end{proof}

\begin{prop}
For any base $b >1$,
\[ \log_b (1/x) \ = \ - \log_b x \]
\end{prop}

\begin{proof}
This follows from the fact that $\log_b 1 =0$, coupled with the
product law for logarithms.
\[ \log_b x + \log_b (1/x) \ = \ \log_b (x \cdot (1/x))
\  = \ \log_b 1 \ = \ 0 
\]
\qed
\end{proof}

\begin{prop}
For any bases $a, b >1$,
\begin{equation}
\label{eq:log-exp-0}
\log_b x \ = \ \left(\log_b a \right) \cdot \left( \log_a x \right).
\end{equation}
\end{prop}

\begin{proof}
We begin by noting that, by definition,
Note that
\begin{equation}
\label{eq:log-exp-1}
 x \ = \ b^{\log_b x} \ = \ a^{\log_a x} .
\end{equation}
Let us take the base-$b$ logarithm of the second and third expressions
in (\ref{eq:log-exp-1}) and then invoke the product law for logarithms.
From the second expression in (\ref{eq:log-exp-1}), we find that
\begin{equation}
\label{eq:log-exp-2}
 \log_b \left(b^{\log_b x} \right) \ = \ \log_b x .
\end{equation}
From the third expression in (\ref{eq:log-exp-1}), we find that
\begin{equation}
\label{eq:log-exp-3}
 \log_b \left( a^{\log_a x} \right) \ = \
\left(\log_b a \right) \cdot \left( \log_a x \right).
\end{equation}
We know from (\ref{eq:log-exp-1}) that the righthand expressions in
(\ref{eq:log-exp-2}) and (\ref{eq:log-exp-3}) are equal, whence
(\ref{eq:log-exp-0}).   \qed
\end{proof}

If we set $x = b$ in (\ref{eq:log-exp-0}), then we find the following
marvelous equation.

\begin{prop}
For any integers $a, b >1$,
\begin{equation}
\left(\log_b a \right) \cdot \left( \log_a b \right) \ = \ 1 \ \ \ \ \
\mbox{ or, equivalently, } \ \ \ \ \
\log_b a \ = \ \frac{1}{\log_a b} .
\end{equation}
\end{prop}


\subsection{Exponentials and logarithms within information theory}
\label{sec:count-strings}

The student should recognize and be able to reason about the following
facts.  If one has an alphabet of $a$ letters/symbols and must provide
distinct string-label ``names'' for $n$ items, then at least one
string-name must have length no shorter than $\lceil \log_a n \rceil$.

\begin{prop}
\label{thm:bound-stringnames-lgth-k}
Say that one must assign distinct labels to $n$ items, via strings
over an alphabet of $a$ symbols.  Then at least one string-label must
have length no shorter than $\lceil \log_a n \rceil$.
\end{prop}

\begin{proof}
Let $\Sigma$ be an alphabet of $a$ symbols.  For each integer $k \geq
0$, let $\Sigma^{(k)}$ denote the set of all length-$k$ strings over
$\Sigma$.  The bound of Proposition~\ref{thm:bound-stringnames-lgth-k}
follows by counting the number of strings of various lengths over
$\Sigma$, because each such string can label at most one item.  Let
us, therefore, inductively evaluate the cardinality $|\Sigma^{(k)}|$
of each set $\Sigma^{(k)}$.
\begin{itemize}
\item
$|\Sigma^{(0)}| =1$

This is because the null-string $\varepsilon$ \index{$\varepsilon$:
  the null string, of length $0$} \index{null string $\varepsilon$}
is the unique string in $\Sigma^{(0)}$; i.e., $\Sigma^{(0)} = \{
\varepsilon \}$.

\item
$|\Sigma^{(k+1)}| = |\Sigma| \cdot |\Sigma^{(k)}|$.

This reckoning follows from the following recipe for creating all
strings over $\Sigma$ of length $k+1$ from all strings of length $k$.
\[
\Sigma^{(k+1)} \ = \ \{ \sigma x \ | \ \sigma \in \Sigma \ \ \mbox{
  and } \ \ x \in \Sigma^{(k)} \}
\]
In other words, every length-$(k+1)$ string over $\Sigma$ is obtained
by taking a length-$k$ string $x$ over $\Sigma$ and {\em prepending}
\index{prepend a symbol to a string} to it a symbol from $\Sigma$,
i.e., adding $x$ an additional leftmost symbol.

This recipe is correct because
  \begin{itemize}
  \item
Each string in $\Sigma^{(k+1)}$, as constructed, has length $k+1$.

This is because the recipe adds a single symbol to a length-$k$
string.
  \item
For each string $x \in \Sigma^{(k)}$, there are $|\Sigma|$ distinct
strings in $\Sigma^{(k+1)}$, as constructed.

This is because each string in $\Sigma^{(k+1)}$ begins with a distinct
symbol from $\Sigma$.

  \item
$\Sigma^{(k+1)}$, as constructed, contains all strings of length $k+1$
over $\Sigma$.

This is because for each $\sigma \in \Sigma$ and each $x \in
\Sigma^{(k)}$, the string $\sigma x$ is in $\Sigma^{(k+1)}$, as
constructed.
  \end{itemize}
\end{itemize}
We thus have the following recurrence.
\begin{eqnarray*}
|\Sigma^{(0)}| & = & 1 \\
|\Sigma^{(k+1)}| & = & |\Sigma| \cdot |\Sigma^{(k)}| \ \ \ \ 
\mbox{ for } \ k \geq 0
\end{eqnarray*}
Using the Master Theorem of Section~\ref{sec:masterTheorem}, we thus
find explicitly that:

\noindent
For each $\ell \in \N$,
\[ |\Sigma^{(\ell)}| \ \ = \ \ \frac{|\Sigma|^{\ell+1} \ - \ |\Sigma|}
{|\Sigma| -1} \ \ \leq \ \ c \cdot |\Sigma|^{\ell}
\]
for some constant $c$.  In order for this quantity to reach the value
$n$, we must have
\[ \ell \ > \ d \cdot \log_{|\Sigma|} n   \]
for some small constant $d$.  \qed
\end{proof}

The following result can be considered anther way of looking at
Proposition~\ref{thm:bound-stringnames-lgth-k}.

\begin{prop}
\label{thm:Num-strings-lgth-k}
The number of distinct strings of length $k$ over an alphabet of $a$
symbols is $a^k$.
\end{prop}

\begin{proof}
As in Proposition~\ref{thm:bound-stringnames-lgth-k}, let us focus on
the generic $a$-letter alphabet $\Sigma \ = \ \{\sigma_1, \sigma_2,
\ldots, \sigma_a\}$.  We argue by induction on string-length $k$.

\noindent
{\it Bases.}
The induction we develop can start either with strings of length $k=0$
or with strings of length $k=1$.  Some people countenance the {\it
  null string} \index{word!the null word} \index{string!the null string}
$\varepsilon$,  \index{word!$\varepsilon$: the null word}
\index{string!$\varepsilon$: the null string} which contains no
symbols, hence has length $0$ as a legitimate string; others insist
that the status ``string'' can be enjoyed only by non-null strings.
This is purely a matter of taste.

At any rate, everyone agrees that, if one accepts $\varepsilon$ as a
legitimate string over alphabet $\Sigma$, then there is only $a^0 = 1$
such word; this is the case $k=0$ of the proposition.  And, everyone
agrees that, if you insist on non-null tsrings, then there are $a^1 =
a$ such strings over $\Sigma$, one for each symbol $\sigma \in
\Sigma$; this is the case $k=1$ of the proposition.  No matter which
side of the fence you choose to stand on, we have verified our
induction's base case.

\noindent
{\it The inductive hypothesis.}
Say that for all string-lengths $k$ up through $n$, there are $a^k$
distinct words of length $k$ over $\Sigma$.

\noindent
{\it Extending the induction.}
Take each length-$n$ string $x$ over $\Sigma$, and {\em append}
\index{append a symbol to a string} to it, in turn, each of $\Sigma$'s
$a$ symbols, i.e., add each symbol to $x$ as an additional rightmost
symbol.  One thereby creates $a$ new strings each obviously distinct
$x$, namely, $x \sigma_1$, $x \sigma_2$, \ldots, $x \sigma_a$.  We
have thus created $a^{n+1}$ distinct length-$(n+1)$ words over
$\Sigma$ from $\Sigma$'s $a^n$ distinct length-$n$ words.

The induction is extended, which completes the proof.  \qed
\end{proof}

\section{$\oplus$ Pointers to Advanced Topics}

**HERE

If the intended curriculum will approach more sophisticated
application areas such as robotics or data science or information
retrieval or data mining (of course, at levels consistent with the
students' preparation), then one would do well to insist on
familiarity with notions such as:

\bigskip

\noindent
{\bf\em Measures of distance in tuple-spaces.}

including the following
norms/metrics: $L_1$ (Manhattan, or, rook's-move distance), $L_2$
(Euclidean distance); $L_\infty$ (King's-move distance).

\bigskip

\noindent
{\bf\em Edit-distance: a measure of closeness in {\em string spaces}}.
From the earliest days of digital computing, the challenge of
employing computers as ``editorial assistants'' was viewed as a prime
domain within which to achieve a significant practical payoff.  An
early problem within this domain was to determine of ``how different''
two strings, $x$ and $y$, over an alphabet $\Sigma$ are from one
another.  The quotation marks in the preceding sentence acknowledges
that it is actually not obvious how to measure the ``distance''
between $x$ and $y$ in a way that matters.  The intersecting need of
three separate developing communities finally supplied the definition
that has generally been accepted.  Roughly contemoraneously, during,
say, the decade from 1955--1965:
  \begin{itemize}
  \item
Computational linguists attempted to enlist computers in the
processing of natural language.  The goal of automatic translation
from Russian to English became the ``holy grail'' of this community.
Reminiscences by Anthony G.~Oettinger, \index{Oettinger, Anthony G.}
a pioneer in this effort, appear in \cite{Hutchins00}.
   \item
As the potential power of computers became appreciated, more
sophisticated process-specification languages began to emerge.  From
the earliest days of this development, programming languages became
more ``language-like''.  One of the earliest entrants in this arena
was the business-oriented language COBOL, which was inspired by early
work of the computing giant Grace Murray Hopper 
\index{Hopper, Grace Murray} and developed under the leadership of
Jean E.~Sammet; \index{Sammet, Jean E.} see \cite{Sammet78}.  A second
early entrant was the string-processing language COMIT developed under
the leadership of Victor Yngve \index{Yngve, Victor} with an eye
toward applications such as language processing; see \cite{Yngve}.
Finally, the ever-evolving FORTRAN language was developed in the
mid-1950s by a small team at IBM, headed by John Backus;
\index{Backus, John} see \cite{Backus-etal57}.  FORTRAN was the first
programming language that aimed to enable scientists to specify the
problems they wished to compute in a manner that was at least
reminiscent of the mathematical notation they would use to communicate
with one another.
   \item
As it became clear that FORTRAN was a step toward satisfying the
programming needs of an enormous market, a veritable army of systems
programmers began to develop increasingly sophisticated processors for
programming languages.  The era of ``smart compilers'' was dawning.
   \end{itemize}

A common feature in the preceding developments was that people were
typing more as they used computers.  Inevitably, therefore, they were
making more typing mistakes.  A very specific computing challenge
arose, to make much more concrete the desire to understand how close a
string $x$ was to a string $y$.  Could one develop an algorithm that
would rewrite one of these strings as the other while ``editing'',
or, rewriting, as few symbols as possible?  While such an algorithm
would not completely solve the ``mistyping'' problem---consider, e.g.,
that the strings

\hspace*{.25in} ``SORTH'' \ \ and \ \ ``NOUTH''

\noindent
are both ``edit-distance $1$'' from both

\hspace*{.25in} ``SOUTH'' \ \ and \ \ ``NORTH''

\noindent
hence cannot be ``corrected'' automatically---being able to correct a
mistyped string to its edit-nearest legitimate string would probably
be very useful in practice.  Happily, although this algorithmic
problem was more difficult than many had imagined, it did admit the
efficient elegant solution that appears in \cite{WagnerF74}.






\ignore{**************
\section{Useful Nonalgebraic Notions}
\label{sec:extra-functions}

\subsection{Nonalgebraic Notions Involving Numbers}
*************}

\ignore{************

This section is devoted to showing that,
even when a number system lacks intrinsic desirable ordering
properties, we can sometimes endow the system with ``inherited
access'' to such properties by devising a {\em bijection} that {\it
  encodes}

%
the system's numbers as nonnegative integers.  We focus here on two
important number systems, the rational and complex numbers, $\Q$ and
$\C$, whose ordering properties are much weaker than those of the
nonnegative integers $\N$: Neither $\Q$ nor $\C$ is {\em
  well-ordered}; $\C$ is not even {\em totally} ordered.
\[ \approx \approx \approx \approx \approx \approx \approx \approx \approx \approx \]
{\em Well-ordering} is an especially welcome property because it
enables algorithms that are structured as a linear recursion that
``counts down'' from an argument $n$.  Well-ordering guarantees that
there is a ``bottom'' that will terminate the downward recursion.
\[ \approx \approx \approx \approx \approx \approx \approx \approx \approx \approx \]

\noindent
Two simplifications facilitate our quest for bijective encodings
\[ \varepsilon_1: \Q \leftrightarrow \N \ \ \ \ \ \mbox{ and }
\ \ \ \ \ \varepsilon_2: \C \leftrightarrow \N
\]
\begin{itemize}
\item
We abstract both $\Q$ and $\C$ as the set $\N \times \N$.  Since both
$\Q$ and the 
\index{numbers!complex integers}
%
{\it complex integers}---complex numbers $a+bi$ where $a,b \in
\N$---can be encoded as ordered pairs of nonnegative integers, this is
a natural abstraction.

Of course, using this abstraction to encode $\Q$ into $\N$ ignores
common (integer) divisors of a fraction's numerator and denominator,
whose elimination preserve the fraction's value.  A very simple
algorithm would compensate for this.

\item
We slightly change our agenda and focus on bijections between $\N^+
\times \N^+$ and $\N^+$, where $\N^+$ is the set of {\em positive}
integers.
\index{number!integer!positive integer}
\index{$\N^+$:number!integer!positive integer}

Focusing on {\em positive} integers rather than {\em nonnegative}
integers somewhat simplifies certain mathematical expessions.
\end{itemize}
The structures of the domain $\N \times \N$ and the range $\N$ of the
bijective encodings of interest have led to the name {\it pairing
  functions}
\index{pairing function}\index{pairing function as encoding}
for these bijections.

\smallskip
**************}


